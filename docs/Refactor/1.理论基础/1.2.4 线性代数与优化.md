# 1.2.4 线性代数与优化 / Linear Algebra and Optimization

## 1. 向量空间 / Vector Spaces

### 1.1 向量空间定义 / Vector Space Definition

**向量空间定义：**
- $Vector_{Space} = \{V, +, \cdot\} | \text{满足向量空间公理}$
- $V = \text{非空集合，向量集合}$
- $+ : V \times V \rightarrow V \text{向量加法}$
- $\cdot : \mathbb{R} \times V \rightarrow V \text{标量乘法}$

**向量空间公理 / Vector Space Axioms：**
- $Associativity: (u + v) + w = u + (v + w)$
- $Commutativity: u + v = v + u$
- $Identity: \exists 0 \in V, v + 0 = v$
- $Inverse: \forall v \in V, \exists (-v), v + (-v) = 0$
- $Distributivity: a(u + v) = au + av$
- $Scalar\ Associativity: (ab)v = a(bv)$
- $Unit: 1 \cdot v = v$

**常见向量空间 / Common Vector Spaces：**
- $\mathbb{R}^n = \text{实数n维向量空间}$
- $\mathbb{C}^n = \text{复数n维向量空间}$
- $P_n = \text{n次多项式空间}$
- $M_{m,n} = \text{m×n矩阵空间}$

### 1.2 线性变换 / Linear Transformations

**线性变换定义：**
- $T: V \rightarrow W \text{是线性变换，如果}$
- $T(u + v) = T(u) + T(v)$
- $T(av) = aT(v)$

**线性变换性质 / Linear Transformation Properties：**
- $Kernel: Ker(T) = \{v \in V | T(v) = 0\}$
- $Image: Im(T) = \{T(v) | v \in V\}$
- $Rank: rank(T) = dim(Im(T))$
- $Nullity: nullity(T) = dim(Ker(T))$

**线性变换矩阵表示 / Matrix Representation：**
- $T(v) = Av \text{其中A是变换矩阵}$
- $A = [T(e_1) \ T(e_2) \ ... \ T(e_n)]$

## 2. 矩阵运算 / Matrix Operations

### 2.1 矩阵基础 / Matrix Basics

**矩阵定义：**
- $Matrix = A = [a_{ij}]_{m \times n}$
- $a_{ij} = \text{第i行第j列的元素}$
- $m = \text{行数}, n = \text{列数}$

**矩阵类型 / Matrix Types：**
- $Square_{Matrix}: m = n$
- $Identity_{Matrix}: I = [\delta_{ij}]$
- $Diagonal_{Matrix}: D = [d_i \delta_{ij}]$
- $Symmetric_{Matrix}: A^T = A$
- $Orthogonal_{Matrix}: A^T A = I$

### 2.2 矩阵运算 / Matrix Operations

**基本运算 / Basic Operations：**
- $Addition: (A + B)_{ij} = a_{ij} + b_{ij}$
- $Scalar_{Multiplication}: (cA)_{ij} = ca_{ij}$
- $Matrix_{Multiplication}: (AB)_{ij} = \sum_k a_{ik} b_{kj}$

**矩阵性质 / Matrix Properties：**
- $Associativity: (AB)C = A(BC)$
- $Distributivity: A(B + C) = AB + AC$
- $Transpose: (A^T)_{ij} = a_{ji}$
- $Inverse: AA^{-1} = A^{-1}A = I$

### 2.3 行列式与逆矩阵 / Determinants and Inverses

**行列式定义：**
- $det(A) = \sum_{\sigma} sgn(\sigma) \prod_i a_{i,\sigma(i)}$
- $sgn(\sigma) = \text{排列的符号}$

**行列式性质 / Determinant Properties：**
- $det(AB) = det(A)det(B)$
- $det(A^T) = det(A)$
- $det(cA) = c^n det(A)$
- $det(A^{-1}) = \frac{1}{det(A)}$

**逆矩阵计算 / Inverse Matrix Calculation：**
- $A^{-1} = \frac{1}{det(A)} adj(A)$
- $adj(A) = \text{伴随矩阵}$

## 3. 特征值分解 / Eigenvalue Decomposition

### 3.1 特征值与特征向量 / Eigenvalues and Eigenvectors

**特征值定义：**
- $Av = \lambda v \text{其中} \lambda \text{是特征值}, v \text{是特征向量}$
- $Characteristic_{Polynomial}: det(A - \lambda I) = 0$

**特征值性质 / Eigenvalue Properties：**
- $Trace(A) = \sum_i \lambda_i$
- $det(A) = \prod_i \lambda_i$
- $Eigenvalues_{of} A^k = \lambda_i^k$

**特征向量性质 / Eigenvector Properties：**
- $Eigenvectors_{are} linearly_{independent}$
- $Geometric_{multiplicity} \leq Algebraic_{multiplicity}$

### 3.2 对角化 / Diagonalization

**对角化定义：**
- $A = PDP^{-1} \text{其中D是对角矩阵}$
- $D = diag(\lambda_1, \lambda_2, ..., \lambda_n)$
- $P = [v_1 \ v_2 \ ... \ v_n] \text{特征向量矩阵}$

**对角化条件 / Diagonalization Conditions：**
- $A \text{有n个线性无关的特征向量}$
- $A \text{的特征值代数重数等于几何重数}$

**应用 / Applications：**
- $Matrix_{Powers}: A^k = PD^kP^{-1}$
- $Matrix_{Exponential}: e^A = Pe^DP^{-1}$

### 3.3 奇异值分解 / Singular Value Decomposition

**SVD定义：**
- $A = U\Sigma V^T$
- $U = \text{左奇异向量矩阵}$
- $\Sigma = \text{奇异值对角矩阵}$
- $V = \text{右奇异向量矩阵}$

**SVD性质 / SVD Properties：**
- $U^TU = I, V^TV = I$
- $\Sigma = diag(\sigma_1, \sigma_2, ..., \sigma_r)$
- $\sigma_i = \sqrt{\lambda_i(A^TA)}$

## 4. 优化理论 / Optimization Theory

### 4.1 优化问题 / Optimization Problems

**优化问题定义：**
- $min_{x \in \Omega} f(x) \text{或} max_{x \in \Omega} f(x)$
- $f(x) = \text{目标函数}$
- $\Omega = \text{可行域}$
- $x = \text{决策变量}$

**优化问题类型 / Optimization Problem Types：**
- $Linear_{Programming}: f(x) = c^Tx, \Omega = \{x | Ax \leq b\}$
- $Quadratic_{Programming}: f(x) = \frac{1}{2}x^TQx + c^Tx$
- $Convex_{Programming}: f(x) \text{凸函数}, \Omega \text{凸集}$
- $Nonlinear_{Programming}: f(x) \text{非线性函数}$

### 4.2 凸优化 / Convex Optimization

**凸集定义：**
- $C \text{是凸集，如果} \forall x,y \in C, \lambda x + (1-\lambda)y \in C$
- $\lambda \in [0,1]$

**凸函数定义：**
- $f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y)$
- $\forall x,y \in dom(f), \lambda \in [0,1]$

**凸优化性质 / Convex Optimization Properties：**
- $Local_{optimum} = Global_{optimum}$
- $Kuhn_{Tucker} conditions \text{是充分必要条件}$

### 4.3 梯度下降 / Gradient Descent

**梯度下降算法：**
- $x_{k+1} = x_k - \alpha_k \nabla f(x_k)$
- $\alpha_k = \text{步长}$
- $\nabla f(x_k) = \text{梯度}$

**收敛条件 / Convergence Conditions：**
- $Lipschitz_{continuity}: \|\nabla f(x) - \nabla f(y)\| \leq L\|x-y\|$
- $Strong_{convexity}: f(y) \geq f(x) + \nabla f(x)^T(y-x) + \frac{\mu}{2}\|y-x\|^2$

**步长选择 / Step Size Selection：**
- $Fixed_{step}: \alpha_k = \alpha$
- $Line_{search}: \alpha_k = argmin_{\alpha} f(x_k - \alpha \nabla f(x_k))$
- $Adaptive_{step}: \alpha_k = \frac{1}{L}$

## 5. 工程实现 / Engineering Implementation

```rust
use ndarray::{Array1, Array2, ArrayView1, ArrayView2};
use nalgebra::{DMatrix, DVector, Matrix3, Vector3};
use std::collections::HashMap;
use std::f64::consts::PI;

// 向量空间实现
#[derive(Debug, Clone)]
pub struct VectorSpace {
    pub dimension: usize,
    pub field: FieldType,
    pub basis: Vec<Vector>,
}

#[derive(Debug, Clone, PartialEq)]
pub enum FieldType {
    Real,
    Complex,
}

#[derive(Debug, Clone)]
pub struct Vector {
    pub components: Vec<f64>,
    pub dimension: usize,
}

impl Vector {
    pub fn new(components: Vec<f64>) -> Self {
        let dimension = components.len();
        Vector { components, dimension }
    }
    
    pub fn zero(dimension: usize) -> Self {
        Vector {
            components: vec![0.0; dimension],
            dimension,
        }
    }
    
    pub fn add(&self, other: &Vector) -> Result<Vector, String> {
        if self.dimension != other.dimension {
            return Err("Vector dimensions must match".to_string());
        }
        
        let components: Vec<f64> = self.components.iter()
            .zip(other.components.iter())
            .map(|(a, b)| a + b)
            .collect();
            
        Ok(Vector { components, dimension: self.dimension })
    }
    
    pub fn scalar_multiply(&self, scalar: f64) -> Vector {
        let components: Vec<f64> = self.components.iter()
            .map(|&x| x * scalar)
            .collect();
            
        Vector { components, dimension: self.dimension }
    }
    
    pub fn dot_product(&self, other: &Vector) -> Result<f64, String> {
        if self.dimension != other.dimension {
            return Err("Vector dimensions must match".to_string());
        }
        
        let result: f64 = self.components.iter()
            .zip(other.components.iter())
            .map(|(a, b)| a * b)
            .sum();
            
        Ok(result)
    }
    
    pub fn norm(&self) -> f64 {
        self.dot_product(self).unwrap().sqrt()
    }
    
    pub fn normalize(&self) -> Vector {
        let norm = self.norm();
        if norm == 0.0 {
            return self.clone();
        }
        self.scalar_multiply(1.0 / norm)
    }
}

// 矩阵实现
#[derive(Debug, Clone)]
pub struct Matrix {
    pub data: Vec<Vec<f64>>,
    pub rows: usize,
    pub cols: usize,
}

impl Matrix {
    pub fn new(data: Vec<Vec<f64>>) -> Result<Self, String> {
        if data.is_empty() || data[0].is_empty() {
            return Err("Matrix cannot be empty".to_string());
        }
        
        let rows = data.len();
        let cols = data[0].len();
        
        // 检查所有行长度一致
        for row in &data {
            if row.len() != cols {
                return Err("All rows must have the same length".to_string());
            }
        }
        
        Ok(Matrix { data, rows, cols })
    }
    
    pub fn identity(size: usize) -> Self {
        let mut data = vec![vec![0.0; size]; size];
        for i in 0..size {
            data[i][i] = 1.0;
        }
        Matrix { data, rows: size, cols: size }
    }
    
    pub fn zero(rows: usize, cols: usize) -> Self {
        let data = vec![vec![0.0; cols]; rows];
        Matrix { data, rows, cols }
    }
    
    pub fn get(&self, row: usize, col: usize) -> Result<f64, String> {
        if row >= self.rows || col >= self.cols {
            return Err("Index out of bounds".to_string());
        }
        Ok(self.data[row][col])
    }
    
    pub fn set(&mut self, row: usize, col: usize, value: f64) -> Result<(), String> {
        if row >= self.rows || col >= self.cols {
            return Err("Index out of bounds".to_string());
        }
        self.data[row][col] = value;
        Ok(())
    }
    
    pub fn add(&self, other: &Matrix) -> Result<Matrix, String> {
        if self.rows != other.rows || self.cols != other.cols {
            return Err("Matrix dimensions must match".to_string());
        }
        
        let mut data = vec![vec![0.0; self.cols]; self.rows];
        for i in 0..self.rows {
            for j in 0..self.cols {
                data[i][j] = self.data[i][j] + other.data[i][j];
            }
        }
        
        Ok(Matrix { data, rows: self.rows, cols: self.cols })
    }
    
    pub fn multiply(&self, other: &Matrix) -> Result<Matrix, String> {
        if self.cols != other.rows {
            return Err("Matrix dimensions incompatible for multiplication".to_string());
        }
        
        let mut data = vec![vec![0.0; other.cols]; self.rows];
        for i in 0..self.rows {
            for j in 0..other.cols {
                for k in 0..self.cols {
                    data[i][j] += self.data[i][k] * other.data[k][j];
                }
            }
        }
        
        Ok(Matrix { data, rows: self.rows, cols: other.cols })
    }
    
    pub fn transpose(&self) -> Matrix {
        let mut data = vec![vec![0.0; self.rows]; self.cols];
        for i in 0..self.rows {
            for j in 0..self.cols {
                data[j][i] = self.data[i][j];
            }
        }
        Matrix { data, rows: self.cols, cols: self.rows }
    }
    
    pub fn determinant(&self) -> Result<f64, String> {
        if self.rows != self.cols {
            return Err("Determinant only defined for square matrices".to_string());
        }
        
        if self.rows == 1 {
            return Ok(self.data[0][0]);
        }
        
        if self.rows == 2 {
            return Ok(self.data[0][0] * self.data[1][1] - self.data[0][1] * self.data[1][0]);
        }
        
        // 递归计算行列式
        let mut det = 0.0;
        for j in 0..self.cols {
            let minor = self.minor(0, j)?;
            det += self.data[0][j] * minor.determinant()? * if j % 2 == 0 { 1.0 } else { -1.0 };
        }
        
        Ok(det)
    }
    
    fn minor(&self, row: usize, col: usize) -> Result<Matrix, String> {
        if row >= self.rows || col >= self.cols {
            return Err("Index out of bounds".to_string());
        }
        
        let mut data = Vec::new();
        for i in 0..self.rows {
            if i != row {
                let mut new_row = Vec::new();
                for j in 0..self.cols {
                    if j != col {
                        new_row.push(self.data[i][j]);
                    }
                }
                data.push(new_row);
            }
        }
        
        Matrix::new(data)
    }
    
    pub fn inverse(&self) -> Result<Matrix, String> {
        if self.rows != self.cols {
            return Err("Inverse only defined for square matrices".to_string());
        }
        
        let det = self.determinant()?;
        if det == 0.0 {
            return Err("Matrix is not invertible".to_string());
        }
        
        let adjoint = self.adjoint()?;
        let mut inverse_data = vec![vec![0.0; self.cols]; self.rows];
        
        for i in 0..self.rows {
            for j in 0..self.cols {
                inverse_data[i][j] = adjoint.data[i][j] / det;
            }
        }
        
        Matrix::new(inverse_data)
    }
    
    fn adjoint(&self) -> Result<Matrix, String> {
        let mut adjoint_data = vec![vec![0.0; self.cols]; self.rows];
        
        for i in 0..self.rows {
            for j in 0..self.cols {
                let minor = self.minor(i, j)?;
                let cofactor = minor.determinant()? * if (i + j) % 2 == 0 { 1.0 } else { -1.0 };
                adjoint_data[j][i] = cofactor; // 注意转置
            }
        }
        
        Matrix::new(adjoint_data)
    }
}

// 特征值分解实现
#[derive(Debug, Clone)]
pub struct EigenDecomposition {
    pub eigenvalues: Vec<f64>,
    pub eigenvectors: Matrix,
    pub diagonalizable: bool,
}

impl Matrix {
    pub fn eigen_decomposition(&self) -> Result<EigenDecomposition, String> {
        if self.rows != self.cols {
            return Err("Eigen decomposition only for square matrices".to_string());
        }
        
        // 简化的幂迭代方法
        let mut eigenvalues = Vec::new();
        let mut eigenvectors = Matrix::zero(self.rows, self.rows);
        
        // 使用幂迭代找到主要特征值
        for i in 0..self.rows {
            let (eigenvalue, eigenvector) = self.power_iteration(i)?;
            eigenvalues.push(eigenvalue);
            
            // 将特征向量存储为列
            for j in 0..self.rows {
                eigenvectors.set(j, i, eigenvector.components[j])?;
            }
        }
        
        Ok(EigenDecomposition {
            eigenvalues,
            eigenvectors,
            diagonalizable: true,
        })
    }
    
    fn power_iteration(&self, iteration: usize) -> Result<(f64, Vector), String> {
        let mut x = Vector::new(vec![1.0; self.rows]);
        
        for _ in 0..100 { // 最大迭代次数
            // 计算 Ax
            let mut new_x = Vector::zero(self.rows);
            for i in 0..self.rows {
                for j in 0..self.cols {
                    new_x.components[i] += self.data[i][j] * x.components[j];
                }
            }
            
            // 归一化
            let norm = new_x.norm();
            if norm == 0.0 {
                return Err("Power iteration failed".to_string());
            }
            
            for i in 0..self.rows {
                new_x.components[i] /= norm;
            }
            
            // 计算特征值估计
            let eigenvalue = self.rayleigh_quotient(&new_x)?;
            
            // 检查收敛
            let diff = (new_x.norm() - x.norm()).abs();
            if diff < 1e-10 {
                return Ok((eigenvalue, new_x));
            }
            
            x = new_x;
        }
        
        Err("Power iteration did not converge".to_string())
    }
    
    fn rayleigh_quotient(&self, x: &Vector) -> Result<f64, String> {
        let mut numerator = 0.0;
        let mut denominator = 0.0;
        
        for i in 0..self.rows {
            for j in 0..self.cols {
                numerator += x.components[i] * self.data[i][j] * x.components[j];
            }
            denominator += x.components[i] * x.components[i];
        }
        
        if denominator == 0.0 {
            return Err("Denominator is zero".to_string());
        }
        
        Ok(numerator / denominator)
    }
}

// 优化算法实现
#[derive(Debug, Clone)]
pub struct OptimizationProblem {
    pub objective_function: Box<dyn Fn(&Vector) -> f64 + Send + Sync>,
    pub constraint_functions: Vec<Box<dyn Fn(&Vector) -> f64 + Send + Sync>>,
    pub domain: OptimizationDomain,
}

#[derive(Debug, Clone)]
pub enum OptimizationDomain {
    Unconstrained,
    Box { lower: Vector, upper: Vector },
    Linear { A: Matrix, b: Vector },
}

#[derive(Debug, Clone)]
pub struct OptimizationResult {
    pub optimal_point: Vector,
    pub optimal_value: f64,
    pub iterations: usize,
    pub converged: bool,
}

impl OptimizationProblem {
    pub fn new(
        objective_function: Box<dyn Fn(&Vector) -> f64 + Send + Sync>,
        domain: OptimizationDomain,
    ) -> Self {
        OptimizationProblem {
            objective_function,
            constraint_functions: Vec::new(),
            domain,
        }
    }
    
    pub fn add_constraint(&mut self, constraint: Box<dyn Fn(&Vector) -> f64 + Send + Sync>) {
        self.constraint_functions.push(constraint);
    }
    
    pub fn gradient_descent(&self, initial_point: Vector, learning_rate: f64, max_iterations: usize) -> OptimizationResult {
        let mut x = initial_point;
        let mut iterations = 0;
        
        for _ in 0..max_iterations {
            let gradient = self.numerical_gradient(&x);
            let step = gradient.scalar_multiply(-learning_rate);
            
            let new_x = x.add(&step).unwrap();
            
            // 检查收敛
            let improvement = (self.objective_function)(&x) - (self.objective_function)(&new_x);
            if improvement.abs() < 1e-10 {
                break;
            }
            
            x = new_x;
            iterations += 1;
        }
        
        OptimizationResult {
            optimal_point: x.clone(),
            optimal_value: (self.objective_function)(&x),
            iterations,
            converged: iterations < max_iterations,
        }
    }
    
    fn numerical_gradient(&self, x: &Vector) -> Vector {
        let h = 1e-6;
        let mut gradient = Vector::zero(x.dimension);
        
        for i in 0..x.dimension {
            let mut x_plus = x.clone();
            let mut x_minus = x.clone();
            
            x_plus.components[i] += h;
            x_minus.components[i] -= h;
            
            let f_plus = (self.objective_function)(&x_plus);
            let f_minus = (self.objective_function)(&x_minus);
            
            gradient.components[i] = (f_plus - f_minus) / (2.0 * h);
        }
        
        gradient
    }
}

// 线性规划实现
#[derive(Debug, Clone)]
pub struct LinearProgramming {
    pub objective_coefficients: Vector,
    pub constraint_matrix: Matrix,
    pub constraint_vector: Vector,
    pub variable_bounds: Option<(Vector, Vector)>, // (lower, upper)
}

impl LinearProgramming {
    pub fn new(
        objective_coefficients: Vector,
        constraint_matrix: Matrix,
        constraint_vector: Vector,
    ) -> Self {
        LinearProgramming {
            objective_coefficients,
            constraint_matrix,
            constraint_vector,
            variable_bounds: None,
        }
    }
    
    pub fn set_bounds(&mut self, lower: Vector, upper: Vector) {
        self.variable_bounds = Some((lower, upper));
    }
    
    pub fn solve_simplex(&self) -> Result<OptimizationResult, String> {
        // 简化的单纯形法实现
        let n_vars = self.objective_coefficients.dimension;
        let n_constraints = self.constraint_matrix.rows;
        
        // 构造标准形式
        let mut tableau = self.construct_tableau()?;
        
        // 单纯形迭代
        for iteration in 0..1000 {
            let pivot_col = self.find_pivot_column(&tableau)?;
            if pivot_col.is_none() {
                break; // 最优解
            }
            
            let pivot_row = self.find_pivot_row(&tableau, pivot_col.unwrap())?;
            if pivot_row.is_none() {
                return Err("Problem is unbounded".to_string());
            }
            
            self.pivot(&mut tableau, pivot_row.unwrap(), pivot_col.unwrap())?;
        }
        
        // 提取解
        let solution = self.extract_solution(&tableau)?;
        
        Ok(OptimizationResult {
            optimal_point: solution,
            optimal_value: self.objective_coefficients.dot_product(&solution)?,
            iterations: 0,
            converged: true,
        })
    }
    
    fn construct_tableau(&self) -> Result<Matrix, String> {
        let n_vars = self.objective_coefficients.dimension;
        let n_constraints = self.constraint_matrix.rows;
        let n_slack = n_constraints;
        
        let mut tableau = Matrix::zero(n_constraints + 1, n_vars + n_slack + 1);
        
        // 约束行
        for i in 0..n_constraints {
            for j in 0..n_vars {
                tableau.set(i, j, self.constraint_matrix.data[i][j])?;
            }
            // 松弛变量
            tableau.set(i, n_vars + i, 1.0)?;
            // 右端常数
            tableau.set(i, n_vars + n_slack, self.constraint_vector.components[i])?;
        }
        
        // 目标函数行
        for j in 0..n_vars {
            tableau.set(n_constraints, j, -self.objective_coefficients.components[j])?;
        }
        
        Ok(tableau)
    }
    
    fn find_pivot_column(&self, tableau: &Matrix) -> Result<Option<usize>, String> {
        let last_row = tableau.rows - 1;
        let mut max_coeff = f64::NEG_INFINITY;
        let mut pivot_col = None;
        
        for j in 0..tableau.cols - 1 {
            let coeff = tableau.get(last_row, j)?;
            if coeff > max_coeff {
                max_coeff = coeff;
                pivot_col = Some(j);
            }
        }
        
        Ok(if max_coeff > 0.0 { pivot_col } else { None })
    }
    
    fn find_pivot_row(&self, tableau: &Matrix, pivot_col: usize) -> Result<Option<usize>, String> {
        let mut min_ratio = f64::INFINITY;
        let mut pivot_row = None;
        
        for i in 0..tableau.rows - 1 {
            let pivot_element = tableau.get(i, pivot_col)?;
            if pivot_element > 0.0 {
                let rhs = tableau.get(i, tableau.cols - 1)?;
                let ratio = rhs / pivot_element;
                if ratio < min_ratio {
                    min_ratio = ratio;
                    pivot_row = Some(i);
                }
            }
        }
        
        Ok(pivot_row)
    }
    
    fn pivot(&self, tableau: &mut Matrix, pivot_row: usize, pivot_col: usize) -> Result<(), String> {
        let pivot_element = tableau.get(pivot_row, pivot_col)?;
        
        // 归一化主元行
        for j in 0..tableau.cols {
            let value = tableau.get(pivot_row, j)?;
            tableau.set(pivot_row, j, value / pivot_element)?;
        }
        
        // 消元其他行
        for i in 0..tableau.rows {
            if i != pivot_row {
                let factor = tableau.get(i, pivot_col)?;
                for j in 0..tableau.cols {
                    let current = tableau.get(i, j)?;
                    let pivot_row_value = tableau.get(pivot_row, j)?;
                    tableau.set(i, j, current - factor * pivot_row_value)?;
                }
            }
        }
        
        Ok(())
    }
    
    fn extract_solution(&self, tableau: &Matrix) -> Result<Vector, String> {
        let n_vars = self.objective_coefficients.dimension;
        let mut solution = Vector::zero(n_vars);
        
        // 从最终表格中提取基本变量的值
        for i in 0..tableau.rows - 1 {
            let mut basic_col = None;
            for j in 0..n_vars {
                if tableau.get(i, j)? == 1.0 {
                    let mut is_basic = true;
                    for k in 0..tableau.rows {
                        if k != i && tableau.get(k, j)? != 0.0 {
                            is_basic = false;
                            break;
                        }
                    }
                    if is_basic {
                        basic_col = Some(j);
                        break;
                    }
                }
            }
            
            if let Some(col) = basic_col {
                solution.components[col] = tableau.get(i, tableau.cols - 1)?;
            }
        }
        
        Ok(solution)
    }
}
```

## 6. 批判性分析 / Critical Analysis

### 6.1 理论局限性 / Theoretical Limitations
- **计算复杂度 Computational Complexity**：大规模矩阵运算的计算成本。
- **数值稳定性 Numerical Stability**：浮点运算的精度问题。
- **收敛性 Convergence**：优化算法的收敛保证。

### 6.2 工程挑战 / Engineering Challenges
- **内存管理 Memory Management**：大规模矩阵的内存使用。
- **并行计算 Parallel Computing**：矩阵运算的并行化。
- **数值精度 Numerical Precision**：高精度计算的需求。

## 7. 工程论证 / Engineering Arguments

- **机器学习 Machine Learning**：线性代数在神经网络中的应用。
- **信号处理 Signal Processing**：傅里叶变换的矩阵表示。
- **图像处理 Image Processing**：图像变换的矩阵运算。
- **控制系统 Control Systems**：状态空间模型的矩阵表示。

---
> 本文件为线性代数与优化的系统化重构，采用严格的形式化定义、数学表达、工程实现，确保内容的学术规范性和工程实用性。
> This file provides systematic refactoring of linear algebra and optimization, using strict formal definitions, mathematical expressions, and engineering implementations, ensuring academic standards and engineering practicality. 