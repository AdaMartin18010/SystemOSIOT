# 13.1.1 神经形态计算基础 / Neuromorphic Computing Fundamentals

## 1. 神经形态计算基础 / Foundations of Neuromorphic Computing

### 1.1 神经形态计算定义 / Definition of Neuromorphic Computing

**神经形态计算定义：**

- $Neuromorphic_{Computing} = \{Hardware\ systems | Brain_{inspired}\ architecture\}$  
  神经形态计算：受大脑启发的硬件系统架构。
- $Neuron_{Model} = \{Biological\ neuron | Mathematical\ abstraction\}$  
  神经元模型：生物神经元的数学抽象。
- $Synapse_{Model} = \{Synaptic\ plasticity | Weight\ adaptation\}$  
  突触模型：突触可塑性的权重适应。

**神经形态计算特征 / Neuromorphic Computing Characteristics：**

- **事件驱动 Event-driven**：$Event_{driven} = \{Spike_{based} computation, Asynchronous\ processing\}$
- **并行处理 Parallel Processing**：$Parallel = \{Massive\ parallelism, Distributed\ computation\}$
- **低功耗 Low Power**：$Low_{Power} = \{Efficient\ energy\ usage, Biological\ efficiency\}$
- **自适应 Adaptive**：$Adaptive = \{Plasticity, Learning, Self_{organization}\}$

### 1.2 神经形态计算层次 / Neuromorphic Computing Levels

**硬件层 Hardware Layer：**

- **神经元电路 Neuron Circuits**：$Neuron_{Circuits} = \{Analog, Digital, Mixed_{signal}\}$
- **突触电路 Synapse Circuits**：$Synapse_{Circuits} = \{Memristor, Capacitor, Transistor_{based}\}$
- **互连网络 Interconnection Network**：$Interconnection = \{Routing, Arbitration, Communication\}$

**算法层 Algorithm Layer：**

- **脉冲神经网络 Spiking Neural Networks**：$SNN = \{Temporal\ coding, Rate\ coding, Phase\ coding\}$
- **学习算法 Learning Algorithms**：$Learning = \{STDP, Hebbian, BCM\ learning\}$
- **网络拓扑 Network Topology**：$Topology = \{Feedforward, Recurrent, Reservoir\}$

**应用层 Application Layer：**

- **感知处理 Sensory Processing**：$Sensory = \{Vision, Audition, Olfaction\}$
- **认知任务 Cognitive Tasks**：$Cognitive = \{Memory, Attention, Decision_{making}\}$
- **控制应用 Control Applications**：$Control = \{Robotics, Autonomous\ systems, Adaptive\ control\}$

## 2. 神经元模型 / Neuron Models

### 2.1 生物神经元模型 Biological Neuron Models

**Hodgkin-Huxley模型 Hodgkin-Huxley Model：**

- $Membrane_{Potential} = C_m \frac{dV}{dt} = I_{ext} - I_{Na} - I_K - I_L$
- $Sodium_{Current} = I_{Na} = g_{Na} m^3 h (V - E_{Na})$
- $Potassium_{Current} = I_K = g_K n^4 (V - E_K)$
- $Leak_{Current} = I_L = g_L (V - E_L)$

**门控变量动力学 Gating Variable Dynamics：**

- $\frac{dm}{dt} = \alpha_m(V)(1-m) - \beta_m(V)m$
- $\frac{dh}{dt} = \alpha_h(V)(1-h) - \beta_h(V)h$
- $\frac{dn}{dt} = \alpha_n(V)(1-n) - \beta_n(V)n$

**速率函数 Rate Functions：**

- $\alpha_m(V) = \frac{0.1(V+40)}{1-e^{-(V+40)/10}}$
- $\beta_m(V) = 4e^{-(V+65)/18}$
- $\alpha_h(V) = 0.07e^{-(V+65)/20}$
- $\beta_h(V) = \frac{1}{1+e^{-(V+35)/10}}$

### 2.2 简化神经元模型 Simplified Neuron Models

**Leaky Integrate-and-Fire模型 LIF Model：**

- $\tau_m \frac{dV}{dt} = -(V - V_{rest}) + R I_{ext}(t)$
- $Spike_{Condition} = V(t) \geq V_{threshold} \Rightarrow V(t) = V_{reset}$
- $Refractory_{Period} = V(t) = V_{reset}\ for\ t \in [t_{spike}, t_{spike} + \tau_{ref}]$

**Izhikevich模型 Izhikevich Model：**

- $\frac{dV}{dt} = 0.04V^2 + 5V + 140 - u + I$
- $\frac{du}{dt} = a(bV - u)$
- $Reset_{Condition} = if\ V \geq 30\ then\ V = c,\ u = u + d$

**Adaptive Exponential模型 Adaptive Exponential Model：**

- $\tau_m \frac{dV}{dt} = -(V - E_L) + \Delta_T e^{(V - V_T)/\Delta_T} + R I_{ext}$
- $\tau_w \frac{dw}{dt} = a(V - E_L) - w$
- $Spike_{Condition} = V \geq V_{threshold} \Rightarrow V = V_{reset},\ w = w + b$

### 2.3 脉冲编码模型 Spike Coding Models

**速率编码 Rate Coding：**

- $Firing_{Rate} = r = \frac{N_{spikes}}{T_{window}}$
- $Rate_{Response} = r = f(I_{input})$
- $Information_{Rate} = I = \sum_{i} p_i \log_2 \frac{p_i}{q_i}$

**时间编码 Temporal Coding：**

- $Spike_{Timing} = t_i = \{t_1, t_2, ..., t_N\}$
- $Phase_{Coding} = \phi = 2\pi \frac{t - t_{ref}}{T_{cycle}}$
- $Latency_{Coding} = \tau = t_{spike} - t_{stimulus}$

**群体编码 Population Coding：**

- $Population_{Response} = \vec{r} = [r_1, r_2, ..., r_N]$
- $Tuning_{Curve} = r_i = f_i(\theta) = r_{max} e^{-\frac{(\theta - \theta_i)^2}{2\sigma^2}}$
- $Population_{Decoding} = \hat{\theta} = \arg\max_{\theta} P(\theta|\vec{r})$

## 3. 突触模型 / Synapse Models

### 3.1 突触可塑性 Synaptic Plasticity

**短期可塑性 Short-term Plasticity：**

- $Depression_{Model} = \frac{dR}{dt} = \frac{1-R}{\tau_{rec}} - U R \delta(t-t_{spike})$
- $Facilitation_{Model} = \frac{dU}{dt} = \frac{U_{rest}-U}{\tau_{facil}} + U_{sp} (1-U) \delta(t-t_{spike})$
- $Synaptic_{Current} = I_{syn} = A R U \delta(t-t_{spike})$

**长期可塑性 Long-term Plasticity：**

- $STDP_{Rule} = \Delta w = \begin{cases} A_+ e^{-\Delta t/\tau_+} & \text{if } \Delta t > 0 \\ -A_- e^{\Delta t/\tau_-} & \text{if } \Delta t < 0 \end{cases}$
- $Hebbian_{Rule} = \Delta w = \eta x_i y_j$
- $BCM_{Rule} = \Delta w = \eta y_j (y_j - \theta_M) x_i$

### 3.2 突触动力学 Synaptic Dynamics

**AMPA受体 AMPA Receptors：**

- $\frac{dg_{AMPA}}{dt} = -\frac{g_{AMPA}}{\tau_{AMPA}} + \sum_k \alpha_{AMPA} \delta(t-t_k)$
- $I_{AMPA} = g_{AMPA} (V - E_{AMPA})$
- $\tau_{AMPA} = 2-5\ ms$

**NMDA受体 NMDA Receptors：**

- $\frac{dg_{NMDA}}{dt} = -\frac{g_{NMDA}}{\tau_{NMDA}} + \sum_k \alpha_{NMDA} \delta(t-t_k)$
- $I_{NMDA} = g_{NMDA} B(V) (V - E_{NMDA})$
- $B(V) = \frac{1}{1 + [Mg^{2+}] e^{-\gamma V}/3.57}$

**GABA受体 GABA Receptors：**

- $\frac{dg_{GABA}}{dt} = -\frac{g_{GABA}}{\tau_{GABA}} + \sum_k \alpha_{GABA} \delta(t-t_k)$
- $I_{GABA} = g_{GABA} (V - E_{GABA})$
- $\tau_{GABA} = 5-10\ ms$

### 3.3 学习规则 Learning Rules

**STDP学习规则 STDP Learning Rule：**

- $Weight_{Update} = \Delta w_{ij} = \eta \sum_{t_i} \sum_{t_j} W(\Delta t)$
- $STDP_{Kernel} = W(\Delta t) = \begin{cases} A_+ e^{-\Delta t/\tau_+} & \text{if } \Delta t > 0 \\ -A_- e^{\Delta t/\tau_-} & \text{if } \Delta t < 0 \end{cases}$
- $Causal_{Window} = \tau_+ = 10-20\ ms$
- $Anti_{causal}_{Window} = \tau_- = 20-40\ ms$

**BCM学习规则 BCM Learning Rule：**

- $\frac{dw_i}{dt} = \eta y x_i (y - \theta_M)$
- $Threshold_{Update} = \frac{d\theta_M}{dt} = \frac{1}{\tau_\theta} (y^2 - \theta_M)$
- $Sliding_{Threshold} = \theta_M = \langle y^2 \rangle$

**Oja学习规则 Oja Learning Rule：**

- $\frac{dw_i}{dt} = \eta y (x_i - y w_i)$
- $Weight_{Normalization} = \sum_i w_i^2 = 1$
- $Principal_{Component} = \vec{w} \propto \vec{v}_1$

## 4. 网络架构 / Network Architectures

### 4.1 前馈网络 Feedforward Networks

**多层感知机 Multi-layer Perceptron：**

- $Hidden_{Layer} = h_j = f(\sum_i w_{ij} x_i + b_j)$
- $Output_{Layer} = y_k = f(\sum_j w_{jk} h_j + b_k)$
- $Activation_{Function} = f(x) = \frac{1}{1 + e^{-x}}$ (Sigmoid)

**卷积神经网络 Convolutional Neural Network：**

- $Convolution_{Layer} = (I * K)_{ij} = \sum_m \sum_n I_{i-m,j-n} K_{mn}$
- $Pooling_{Layer} = P_{ij} = \max_{(m,n) \in W} I_{i+m,j+n}$
- $Feature_{Map} = F_l = \sigma(\sum_k I_k * W_{kl} + b_l)$

### 4.2 循环网络 Recurrent Networks

**Hopfield网络 Hopfield Network：**

- $Energy_{Function} = E = -\frac{1}{2} \sum_{i,j} w_{ij} s_i s_j$
- $State_{Update} = s_i(t+1) = \text{sgn}(\sum_j w_{ij} s_j(t))$
- $Stability_{Condition} = w_{ij} = w_{ji},\ w_{ii} = 0$

**Elman网络 Elman Network：**

- $Hidden_{State} = h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$
- $Output_{State} = y_t = f(W_{hy} h_t + b_y)$
- $Context_{Layer} = c_t = h_{t-1}$

**LSTM网络 LSTM Network：**

- $Input_{Gate} = i_t = \sigma(W_{xi} x_t + W_{hi} h_{t-1} + b_i)$
- $Forget_{Gate} = f_t = \sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f)$
- $Output_{Gate} = o_t = \sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o)$
- $Cell_{State} = c_t = f_t \odot c_{t-1} + i_t \odot \tanh(W_{xc} x_t + W_{hc} h_{t-1} + b_c)$
- $Hidden_{State} = h_t = o_t \odot \tanh(c_t)$

### 4.3 储备池网络 Reservoir Networks

**液体状态机 Liquid State Machine：**

- $Reservoir_{State} = x(t) = f(W_{in} u(t) + W_{res} x(t-\tau) + W_{fb} y(t-\tau))$
- $Readout_{Layer} = y(t) = W_{out} x(t)$
- $Separation_{Property} = \frac{||x_1(t) - x_2(t)||}{||u_1(t) - u_2(t)||} > 1$

**回声状态网络 Echo State Network：**

- $Reservoir_{Dynamics} = \frac{dx}{dt} = -\alpha x + \tanh(W_{in} u + W_{res} x + W_{fb} y)$
- $Output_{Mapping} = y = W_{out} x$
- $Echo_{State}_{Property} = \rho(W_{res}) < 1$

## 5. 工程实现 / Engineering Implementation

```rust
use std::collections::{HashMap, VecDeque};
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use serde::{Deserialize, Serialize};
use tokio::sync::mpsc;
use ndarray::{Array1, Array2, ArrayView1, ArrayView2};
use ndarray_linalg::{Eig, QR, SVD};

// 神经形态计算系统类型
#[derive(Debug, Clone, PartialEq)]
pub enum NeuromorphicSystemType {
    SpikingNeuralNetwork,
    ReservoirComputing,
    LiquidStateMachine,
    EchoStateNetwork,
    BrainInspiredChip,
    MemristorArray,
}

// 神经元类型
#[derive(Debug, Clone, PartialEq)]
pub enum NeuronType {
    LIF,
    Izhikevich,
    HodgkinHuxley,
    AdaptiveExponential,
    HodgkinHuxleySimplified,
}

// 神经形态计算系统
#[derive(Debug, Clone)]
pub struct NeuromorphicComputingSystem {
    pub id: String,
    pub name: String,
    pub system_type: NeuromorphicSystemType,
    pub neurons: Vec<Neuron>,
    pub synapses: Vec<Synapse>,
    pub networks: Vec<Network>,
    pub learning_rules: Vec<LearningRule>,
    pub hardware_platform: HardwarePlatform,
    pub simulation_engine: SimulationEngine,
    pub configuration: NeuromorphicConfiguration,
    pub state: Arc<Mutex<NeuromorphicState>>,
}

#[derive(Debug, Clone)]
pub struct Neuron {
    pub id: String,
    pub name: String,
    pub neuron_type: NeuronType,
    pub parameters: NeuronParameters,
    pub membrane_potential: f64,
    pub spike_history: Vec<Spike>,
    pub refractory_period: Duration,
    pub last_spike_time: Option<Instant>,
}

#[derive(Debug, Clone)]
pub struct NeuronParameters {
    pub membrane_capacitance: f64,
    pub membrane_resistance: f64,
    pub resting_potential: f64,
    pub threshold_potential: f64,
    pub reset_potential: f64,
    pub refractory_period: Duration,
    pub adaptation_current: f64,
    pub adaptation_time_constant: f64,
}

#[derive(Debug, Clone)]
pub struct Spike {
    pub timestamp: Instant,
    pub neuron_id: String,
    pub amplitude: f64,
    pub metadata: HashMap<String, String>,
}

#[derive(Debug, Clone)]
pub struct Synapse {
    pub id: String,
    pub name: String,
    pub presynaptic_neuron: String,
    pub postsynaptic_neuron: String,
    pub weight: f64,
    pub delay: Duration,
    pub plasticity: SynapticPlasticity,
    pub neurotransmitter_type: NeurotransmitterType,
}

#[derive(Debug, Clone)]
pub struct SynapticPlasticity {
    pub plasticity_type: PlasticityType,
    pub parameters: HashMap<String, f64>,
    pub learning_rate: f64,
    pub weight_bounds: (f64, f64),
}

#[derive(Debug, Clone)]
pub enum PlasticityType {
    STDP,
    Hebbian,
    BCM,
    Oja,
    None,
}

#[derive(Debug, Clone)]
pub enum NeurotransmitterType {
    AMPA,
    NMDA,
    GABA,
    Acetylcholine,
    Dopamine,
    Serotonin,
}

#[derive(Debug, Clone)]
pub struct Network {
    pub id: String,
    pub name: String,
    pub network_type: NetworkType,
    pub neurons: Vec<String>,
    pub synapses: Vec<String>,
    pub topology: NetworkTopology,
    pub dynamics: NetworkDynamics,
}

#[derive(Debug, Clone)]
pub enum NetworkType {
    Feedforward,
    Recurrent,
    Reservoir,
    Competitive,
    Cooperative,
}

#[derive(Debug, Clone)]
pub struct NetworkTopology {
    pub topology_type: TopologyType,
    pub connectivity_matrix: Array2<f64>,
    pub connection_probability: f64,
    pub max_connections: usize,
}

#[derive(Debug, Clone)]
pub enum TopologyType {
    Random,
    SmallWorld,
    ScaleFree,
    Regular,
    Hierarchical,
}

#[derive(Debug, Clone)]
pub struct NetworkDynamics {
    pub dynamics_type: DynamicsType,
    pub parameters: HashMap<String, f64>,
    pub stability_conditions: Vec<StabilityCondition>,
}

#[derive(Debug, Clone)]
pub enum DynamicsType {
    Synchronous,
    Asynchronous,
    EventDriven,
    Continuous,
}

#[derive(Debug, Clone)]
pub struct StabilityCondition {
    pub condition_type: ConditionType,
    pub threshold: f64,
    pub description: String,
}

#[derive(Debug, Clone)]
pub enum ConditionType {
    Lyapunov,
    Eigenvalue,
    Energy,
    Entropy,
}

#[derive(Debug, Clone)]
pub struct LearningRule {
    pub id: String,
    pub name: String,
    pub rule_type: LearningRuleType,
    pub parameters: HashMap<String, f64>,
    pub implementation: Box<dyn Fn(&Synapse, &Spike, &Spike) -> f64 + Send + Sync>,
}

#[derive(Debug, Clone)]
pub enum LearningRuleType {
    STDP,
    Hebbian,
    BCM,
    Oja,
    Custom,
}

#[derive(Debug, Clone)]
pub struct HardwarePlatform {
    pub id: String,
    pub name: String,
    pub platform_type: PlatformType,
    pub specifications: HardwareSpecifications,
    pub constraints: HardwareConstraints,
}

#[derive(Debug, Clone)]
pub enum PlatformType {
    FPGA,
    ASIC,
    GPU,
    CPU,
    Memristor,
    Photonic,
}

#[derive(Debug, Clone)]
pub struct HardwareSpecifications {
    pub num_neurons: usize,
    pub num_synapses: usize,
    pub clock_frequency: f64,
    pub memory_capacity: usize,
    pub power_consumption: f64,
    pub area: f64,
}

#[derive(Debug, Clone)]
pub struct HardwareConstraints {
    pub max_neurons: usize,
    pub max_synapses: usize,
    pub max_frequency: f64,
    pub max_power: f64,
    pub max_area: f64,
}

#[derive(Debug, Clone)]
pub struct SimulationEngine {
    pub id: String,
    pub name: String,
    pub engine_type: EngineType,
    pub time_step: Duration,
    pub simulation_time: Duration,
    pub integrator: Integrator,
    pub observables: Vec<Observable>,
}

#[derive(Debug, Clone)]
pub enum EngineType {
    EventDriven,
    TimeDriven,
    Hybrid,
    Continuous,
    Discrete,
}

#[derive(Debug, Clone)]
pub struct Integrator {
    pub integrator_type: IntegratorType,
    pub order: u32,
    pub tolerance: f64,
    pub max_steps: usize,
}

#[derive(Debug, Clone)]
pub enum IntegratorType {
    Euler,
    RungeKutta,
    Adaptive,
    Symplectic,
    Stochastic,
}

#[derive(Debug, Clone)]
pub struct Observable {
    pub id: String,
    pub name: String,
    pub observable_type: ObservableType,
    pub measurement_interval: Duration,
    pub data: Vec<Measurement>,
}

#[derive(Debug, Clone)]
pub enum ObservableType {
    MembranePotential,
    SpikeRate,
    SynapticWeight,
    NetworkActivity,
    EnergyConsumption,
}

#[derive(Debug, Clone)]
pub struct Measurement {
    pub timestamp: Instant,
    pub value: f64,
    pub uncertainty: f64,
    pub metadata: HashMap<String, String>,
}

#[derive(Debug, Clone)]
pub struct NeuromorphicConfiguration {
    pub simulation_parameters: HashMap<String, f64>,
    pub learning_parameters: HashMap<String, f64>,
    pub hardware_parameters: HashMap<String, f64>,
    pub network_parameters: HashMap<String, f64>,
}

#[derive(Debug, Clone)]
pub struct NeuromorphicState {
    pub current_time: Duration,
    pub active_neurons: Vec<String>,
    pub spike_queue: VecDeque<Spike>,
    pub network_activity: f64,
    pub energy_consumption: f64,
    pub learning_progress: HashMap<String, f64>,
    pub performance_metrics: HashMap<String, f64>,
}

impl NeuromorphicComputingSystem {
    pub fn new(id: String, name: String, system_type: NeuromorphicSystemType) -> Self {
        NeuromorphicComputingSystem {
            id,
            name,
            system_type,
            neurons: Vec::new(),
            synapses: Vec::new(),
            networks: Vec::new(),
            learning_rules: Vec::new(),
            hardware_platform: HardwarePlatform {
                id: "platform1".to_string(),
                name: "Neuromorphic Platform".to_string(),
                platform_type: PlatformType::FPGA,
                specifications: HardwareSpecifications {
                    num_neurons: 1000,
                    num_synapses: 10000,
                    clock_frequency: 100e6, // 100 MHz
                    memory_capacity: 1024 * 1024, // 1 MB
                    power_consumption: 1.0, // 1 W
                    area: 100.0, // 100 mm²
                },
                constraints: HardwareConstraints {
                    max_neurons: 10000,
                    max_synapses: 100000,
                    max_frequency: 200e6, // 200 MHz
                    max_power: 5.0, // 5 W
                    max_area: 500.0, // 500 mm²
                },
            },
            simulation_engine: SimulationEngine {
                id: "engine1".to_string(),
                name: "Neuromorphic Simulator".to_string(),
                engine_type: EngineType::EventDriven,
                time_step: Duration::from_micros(1), // 1 μs
                simulation_time: Duration::from_secs(1), // 1 s
                integrator: Integrator {
                    integrator_type: IntegratorType::Euler,
                    order: 1,
                    tolerance: 1e-6,
                    max_steps: 1000000,
                },
                observables: Vec::new(),
            },
            configuration: NeuromorphicConfiguration {
                simulation_parameters: HashMap::new(),
                learning_parameters: HashMap::new(),
                hardware_parameters: HashMap::new(),
                network_parameters: HashMap::new(),
            },
            state: Arc::new(Mutex::new(NeuromorphicState {
                current_time: Duration::from_secs(0),
                active_neurons: Vec::new(),
                spike_queue: VecDeque::new(),
                network_activity: 0.0,
                energy_consumption: 0.0,
                learning_progress: HashMap::new(),
                performance_metrics: HashMap::new(),
            })),
        }
    }
    
    pub fn add_neuron(&mut self, neuron: Neuron) {
        self.neurons.push(neuron);
    }
    
    pub fn add_synapse(&mut self, synapse: Synapse) {
        self.synapses.push(synapse);
    }
    
    pub fn add_network(&mut self, network: Network) {
        self.networks.push(network);
    }
    
    pub fn add_learning_rule(&mut self, rule: LearningRule) {
        self.learning_rules.push(rule);
    }
    
    pub async fn simulate(&mut self) -> Result<SimulationResult, String> {
        let mut result = SimulationResult {
            system_id: self.id.clone(),
            simulation_time: self.simulation_engine.simulation_time,
            spike_data: Vec::new(),
            membrane_potentials: HashMap::new(),
            synaptic_weights: HashMap::new(),
            performance_metrics: HashMap::new(),
        };
        
        let mut current_time = Duration::from_secs(0);
        let time_step = self.simulation_engine.time_step;
        
        while current_time < self.simulation_engine.simulation_time {
            // 更新神经元状态
            self.update_neurons(current_time, &mut result);
            
            // 处理突触传递
            self.process_synapses(current_time, &mut result);
            
            // 应用学习规则
            self.apply_learning_rules(current_time, &mut result);
            
            // 记录观测值
            self.record_observables(current_time, &mut result);
            
            current_time += time_step;
        }
        
        Ok(result)
    }
    
    fn update_neurons(&mut self, current_time: Duration, result: &mut SimulationResult) {
        for neuron in &mut self.neurons {
            // 检查是否在不应期
            if let Some(last_spike) = neuron.last_spike_time {
                if current_time.as_millis() - last_spike.duration_since(UNIX_EPOCH).as_millis() 
                   < neuron.refractory_period.as_millis() {
                    continue;
                }
            }
            
            // 更新膜电位
            let membrane_potential = self.update_membrane_potential(neuron, current_time);
            neuron.membrane_potential = membrane_potential;
            
            // 检查是否产生脉冲
            if membrane_potential >= neuron.parameters.threshold_potential {
                let spike = Spike {
                    timestamp: Instant::now(),
                    neuron_id: neuron.id.clone(),
                    amplitude: 1.0,
                    metadata: HashMap::new(),
                };
                
                neuron.spike_history.push(spike.clone());
                neuron.last_spike_time = Some(Instant::now());
                neuron.membrane_potential = neuron.parameters.reset_potential;
                
                result.spike_data.push(spike);
            }
            
            // 记录膜电位
            result.membrane_potentials.insert(neuron.id.clone(), membrane_potential);
        }
    }
    
    fn update_membrane_potential(&self, neuron: &Neuron, current_time: Duration) -> f64 {
        match neuron.neuron_type {
            NeuronType::LIF => {
                // LIF模型
                let tau = neuron.parameters.membrane_capacitance * neuron.parameters.membrane_resistance;
                let input_current = self.calculate_input_current(neuron);
                
                let delta_v = (-(neuron.membrane_potential - neuron.parameters.resting_potential) 
                              + neuron.parameters.membrane_resistance * input_current) / tau;
                
                neuron.membrane_potential + delta_v * self.simulation_engine.time_step.as_secs_f64()
            },
            NeuronType::Izhikevich => {
                // Izhikevich模型
                let a = 0.02;
                let b = 0.2;
                let c = -65.0;
                let d = 2.0;
                
                let v = neuron.membrane_potential;
                let u = neuron.adaptation_current;
                let input_current = self.calculate_input_current(neuron);
                
                let dv = 0.04 * v * v + 5.0 * v + 140.0 - u + input_current;
                let du = a * (b * v - u);
                
                (v + dv * self.simulation_engine.time_step.as_secs_f64(),
                 u + du * self.simulation_engine.time_step.as_secs_f64())
            },
            _ => neuron.membrane_potential,
        }
    }
    
    fn calculate_input_current(&self, neuron: &Neuron) -> f64 {
        let mut total_current = 0.0;
        
        for synapse in &self.synapses {
            if synapse.postsynaptic_neuron == neuron.id {
                // 简化的突触电流计算
                total_current += synapse.weight;
            }
        }
        
        total_current
    }
    
    fn process_synapses(&mut self, current_time: Duration, result: &mut SimulationResult) {
        for synapse in &mut self.synapses {
            // 检查是否有脉冲到达
            if let Some(presynaptic_neuron) = self.neurons.iter().find(|n| n.id == synapse.presynaptic_neuron) {
                if let Some(last_spike) = presynaptic_neuron.spike_history.last() {
                    let spike_time = last_spike.timestamp.duration_since(UNIX_EPOCH);
                    let delay_time = spike_time + synapse.delay;
                    
                    if current_time >= delay_time && current_time < delay_time + self.simulation_engine.time_step {
                        // 突触激活
                        self.activate_synapse(synapse, result);
                    }
                }
            }
        }
    }
    
    fn activate_synapse(&mut self, synapse: &mut Synapse, result: &mut SimulationResult) {
        // 简化的突触激活
        let postsynaptic_neuron = self.neurons.iter_mut()
            .find(|n| n.id == synapse.postsynaptic_neuron)
            .unwrap();
        
        // 增加膜电位
        postsynaptic_neuron.membrane_potential += synapse.weight;
        
        // 记录突触权重
        result.synaptic_weights.insert(synapse.id.clone(), synapse.weight);
    }
    
    fn apply_learning_rules(&mut self, current_time: Duration, result: &mut SimulationResult) {
        for rule in &self.learning_rules {
            for synapse in &mut self.synapses {
                // 查找相关的脉冲
                let presynaptic_spikes: Vec<&Spike> = result.spike_data.iter()
                    .filter(|s| s.neuron_id == synapse.presynaptic_neuron)
                    .collect();
                
                let postsynaptic_spikes: Vec<&Spike> = result.spike_data.iter()
                    .filter(|s| s.neuron_id == synapse.postsynaptic_neuron)
                    .collect();
                
                // 应用学习规则
                for pre_spike in &presynaptic_spikes {
                    for post_spike in &postsynaptic_spikes {
                        let weight_change = (rule.implementation)(synapse, pre_spike, post_spike);
                        synapse.weight += weight_change;
                        
                        // 限制权重范围
                        if let Some(bounds) = synapse.plasticity.weight_bounds {
                            synapse.weight = synapse.weight.max(bounds.0).min(bounds.1);
                        }
                    }
                }
            }
        }
    }
    
    fn record_observables(&self, current_time: Duration, result: &mut SimulationResult) {
        // 记录网络活动
        let active_neurons = self.neurons.iter()
            .filter(|n| n.membrane_potential > n.parameters.threshold_potential * 0.8)
            .count();
        
        let network_activity = active_neurons as f64 / self.neurons.len() as f64;
        result.performance_metrics.insert("network_activity".to_string(), network_activity);
        
        // 记录能量消耗
        let energy_consumption = self.calculate_energy_consumption();
        result.performance_metrics.insert("energy_consumption".to_string(), energy_consumption);
    }
    
    fn calculate_energy_consumption(&self) -> f64 {
        // 简化的能量消耗计算
        let spike_count: usize = self.neurons.iter()
            .map(|n| n.spike_history.len())
            .sum();
        
        let energy_per_spike = 1e-12; // 1 pJ per spike
        spike_count as f64 * energy_per_spike
    }
    
    pub fn get_network_statistics(&self) -> NetworkStatistics {
        let total_neurons = self.neurons.len();
        let total_synapses = self.synapses.len();
        let total_spikes: usize = self.neurons.iter()
            .map(|n| n.spike_history.len())
            .sum();
        
        let average_firing_rate = if total_neurons > 0 {
            total_spikes as f64 / total_neurons as f64
        } else {
            0.0
        };
        
        NetworkStatistics {
            total_neurons,
            total_synapses,
            total_spikes,
            average_firing_rate,
            network_connectivity: total_synapses as f64 / (total_neurons * total_neurons) as f64,
        }
    }
}

#[derive(Debug, Clone)]
pub struct SimulationResult {
    pub system_id: String,
    pub simulation_time: Duration,
    pub spike_data: Vec<Spike>,
    pub membrane_potentials: HashMap<String, f64>,
    pub synaptic_weights: HashMap<String, f64>,
    pub performance_metrics: HashMap<String, f64>,
}

#[derive(Debug, Clone)]
pub struct NetworkStatistics {
    pub total_neurons: usize,
    pub total_synapses: usize,
    pub total_spikes: usize,
    pub average_firing_rate: f64,
    pub network_connectivity: f64,
}

// LIF神经元实现
pub struct LIFNeuron {
    pub id: String,
    pub membrane_potential: f64,
    pub resting_potential: f64,
    pub threshold_potential: f64,
    pub reset_potential: f64,
    pub membrane_time_constant: f64,
    pub refractory_period: Duration,
    pub last_spike_time: Option<Instant>,
    pub input_current: f64,
}

impl LIFNeuron {
    pub fn new(id: String) -> Self {
        LIFNeuron {
            id,
            membrane_potential: -65.0,
            resting_potential: -65.0,
            threshold_potential: -55.0,
            reset_potential: -65.0,
            membrane_time_constant: 20.0, // ms
            refractory_period: Duration::from_millis(2),
            last_spike_time: None,
            input_current: 0.0,
        }
    }
    
    pub fn update(&mut self, dt: f64) -> Option<Spike> {
        // 检查不应期
        if let Some(last_spike) = self.last_spike_time {
            if Instant::now().duration_since(last_spike) < self.refractory_period {
                return None;
            }
        }
        
        // 更新膜电位
        let dv = (-(self.membrane_potential - self.resting_potential) 
                 + self.input_current) / self.membrane_time_constant;
        self.membrane_potential += dv * dt;
        
        // 检查是否产生脉冲
        if self.membrane_potential >= self.threshold_potential {
            self.membrane_potential = self.reset_potential;
            self.last_spike_time = Some(Instant::now());
            
            Some(Spike {
                timestamp: Instant::now(),
                neuron_id: self.id.clone(),
                amplitude: 1.0,
                metadata: HashMap::new(),
            })
        } else {
            None
        }
    }
    
    pub fn add_input_current(&mut self, current: f64) {
        self.input_current += current;
    }
    
    pub fn reset_input_current(&mut self) {
        self.input_current = 0.0;
    }
}

// STDP学习规则实现
pub struct STDPLearningRule {
    pub learning_rate: f64,
    pub tau_plus: f64,
    pub tau_minus: f64,
    pub a_plus: f64,
    pub a_minus: f64,
}

impl STDPLearningRule {
    pub fn new() -> Self {
        STDPLearningRule {
            learning_rate: 0.01,
            tau_plus: 20.0, // ms
            tau_minus: 20.0, // ms
            a_plus: 0.1,
            a_minus: 0.1,
        }
    }
    
    pub fn calculate_weight_change(&self, delta_t: f64) -> f64 {
        if delta_t > 0.0 {
            // 前脉冲在突触后脉冲之前
            self.a_plus * (-delta_t / self.tau_plus).exp()
        } else {
            // 前脉冲在突触后脉冲之后
            -self.a_minus * (delta_t / self.tau_minus).exp()
        }
    }
}

// 储备池网络实现
pub struct ReservoirNetwork {
    pub id: String,
    pub input_size: usize,
    pub reservoir_size: usize,
    pub output_size: usize,
    pub input_weights: Array2<f64>,
    pub reservoir_weights: Array2<f64>,
    pub output_weights: Array2<f64>,
    pub reservoir_states: Array1<f64>,
    pub spectral_radius: f64,
    pub connectivity: f64,
}

impl ReservoirNetwork {
    pub fn new(id: String, input_size: usize, reservoir_size: usize, output_size: usize) -> Self {
        let mut rng = rand::thread_rng();
        
        // 初始化权重矩阵
        let input_weights = Array2::random((reservoir_size, input_size), rand::distributions::Uniform::new(-1.0, 1.0));
        let reservoir_weights = Array2::random((reservoir_size, reservoir_size), rand::distributions::Uniform::new(-1.0, 1.0));
        let output_weights = Array2::zeros((output_size, reservoir_size));
        
        // 设置储备池权重
        let mut reservoir_weights = reservoir_weights;
        let spectral_radius = 0.9;
        let eigenvalues = reservoir_weights.eigvals().unwrap();
        let max_eigenvalue = eigenvalues.iter().map(|x| x.norm()).fold(0.0, f64::max);
        reservoir_weights = reservoir_weights * (spectral_radius / max_eigenvalue);
        
        // 稀疏化连接
        let connectivity = 0.1;
        for i in 0..reservoir_size {
            for j in 0..reservoir_size {
                if rand::random::<f64>() > connectivity {
                    reservoir_weights[[i, j]] = 0.0;
                }
            }
        }
        
        ReservoirNetwork {
            id,
            input_size,
            reservoir_size,
            output_size,
            input_weights,
            reservoir_weights,
            output_weights,
            reservoir_states: Array1::zeros(reservoir_size),
            spectral_radius,
            connectivity,
        }
    }
    
    pub fn forward(&mut self, input: &Array1<f64>) -> Array1<f64> {
        // 更新储备池状态
        let input_contribution = self.input_weights.dot(input);
        let reservoir_contribution = self.reservoir_weights.dot(&self.reservoir_states);
        
        self.reservoir_states = (input_contribution + reservoir_contribution).mapv(|x| x.tanh());
        
        // 计算输出
        self.output_weights.dot(&self.reservoir_states)
    }
    
    pub fn train(&mut self, inputs: &Array2<f64>, targets: &Array2<f64>) {
        // 收集储备池状态
        let mut reservoir_states = Array2::zeros((inputs.nrows(), self.reservoir_size));
        
        for (i, input) in inputs.outer_iter().enumerate() {
            let mut network = self.clone();
            let _ = network.forward(&input.to_owned());
            reservoir_states.row_mut(i).assign(&network.reservoir_states);
        }
        
        // 训练输出权重（伪逆）
        let reservoir_pinv = reservoir_states.pinv(1e-10).unwrap();
        self.output_weights = targets.t().dot(&reservoir_pinv).t();
    }
    
    pub fn predict(&self, input: &Array1<f64>) -> Array1<f64> {
        let mut network = self.clone();
        network.forward(input)
    }
}
```

## 5. 批判性分析 / Critical Analysis

### 5.1 理论局限性 / Theoretical Limitations

- **生物真实性 Biological Realism**：简化模型与真实神经元的差异。
- **可扩展性 Scalability**：大规模网络的实现挑战。
- **学习效率 Learning Efficiency**：生物学习机制的完全模拟困难。

### 5.2 工程挑战 / Engineering Challenges

- **硬件实现 Hardware Implementation**：专用神经形态芯片的设计复杂性。
- **编程模型 Programming Model**：事件驱动编程的复杂性。
- **性能优化 Performance Optimization**：大规模并行计算的优化。

## 6. 工程论证 / Engineering Arguments

- **边缘计算**：如IoT设备，需低功耗、实时响应的神经形态处理器。
- **机器人控制**：如自主机器人，需自适应、鲁棒的神经形态控制系统。
- **脑机接口**：如神经假体，需生物兼容的神经形态接口。

---
> 本文件为神经形态计算基础的系统化重构，采用严格的形式化定义、数学表达、工程实现，确保内容的学术规范性和工程实用性。
> This file provides systematic refactoring of neuromorphic computing fundamentals, using strict formal definitions, mathematical expressions, and engineering implementations, ensuring academic standards and engineering practicality.
