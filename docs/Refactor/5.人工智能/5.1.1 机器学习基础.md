# 5.1.1 机器学习基础 / Machine Learning Fundamentals

## 1. 机器学习基础 / Foundations of Machine Learning

### 1.1 机器学习定义 / Definition of Machine Learning

**机器学习定义：**

- $ML = \{Algorithm | Learn\ patterns\ from\ data\ without\ explicit\ programming\}$  
  机器学习：从数据中学习模式而无需显式编程的算法。
- $Learning_{Process} = \{Data_{Input} \rightarrow Pattern_{Extraction} \rightarrow Model_{Output}\}$  
  学习过程：数据输入→模式提取→模型输出。
- $Model = \{Function | f: X \rightarrow Y\}$  
  模型：从输入空间到输出空间的函数。

**学习目标 / Learning Objectives：**

- **预测 Prediction**：$Prediction = \{Output | Future_{value}\ based\ on\ historical_{data}\}$
- **分类 Classification**：$Classification = \{Output | Discrete_{categories}\ for\ input_{data}\}$
- **回归 Regression**：$Regression = \{Output | Continuous_{values}\ for\ input_{data}\}$
- **聚类 Clustering**：$Clustering = \{Output | Group_{similar}\ data_{points}\}$

### 1.2 学习范式 / Learning Paradigms

**监督学习 Supervised Learning：**

- $Supervised_{Learning} = \{Learning | Training_{data} = \{(x_i, y_i)\}_{i=1}^n\}$
- $Loss_{Function} = \{Function | L(f(x), y) \rightarrow \mathbb{R}\}$
- $Objective = \min_{f} \sum_{i=1}^n L(f(x_i), y_i)$

**无监督学习 Unsupervised Learning：**

- $Unsupervised_{Learning} = \{Learning | Training_{data} = \{x_i\}_{i=1}^n\}$
- $Objective = \max_{f} \sum_{i=1}^n Similarity(f(x_i), f(x_j))$

**强化学习 Reinforcement Learning：**

- $RL = \{Learning | Agent_{interacts}\ with\ environment\}$
- $State_{Space} = \{S | Environment_{states}\}$
- $Action_{Space} = \{A | Possible_{actions}\}$
- $Reward_{Function} = \{R | S \times A \rightarrow \mathbb{R}\}$

**半监督学习 Semi-supervised Learning：**

- $Semi_{supervised} = \{Learning | Labeled + Unlabeled_{data}\}$
- $Data_{Set} = \{L = \{(x_i, y_i)\}, U = \{x_j\}\}$

## 2. 核心算法 / Core Algorithms

### 2.1 线性模型 Linear Models

**线性回归 Linear Regression：**

- $f(x) = w^T x + b$
- $Loss = \frac{1}{2n} \sum_{i=1}^n (f(x_i) - y_i)^2$
- $Gradient = \frac{1}{n} \sum_{i=1}^n (f(x_i) - y_i) x_i$

**逻辑回归 Logistic Regression：**

- $f(x) = \frac{1}{1 + e^{-(w^T x + b)}}$
- $Loss = -\frac{1}{n} \sum_{i=1}^n [y_i \log(f(x_i)) + (1-y_i) \log(1-f(x_i))]$

**支持向量机 Support Vector Machine：**

- $f(x) = w^T \phi(x) + b$
- $Objective = \min_{w,b} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i$
- $Constraints: y_i(w^T \phi(x_i) + b) \geq 1 - \xi_i$

### 2.2 决策树 Decision Trees

**决策树定义：**

- $Tree = \{Node | Split_{condition}, Children_{nodes}\}$
- $Split_{Criterion} = \{Gini, Entropy, Information_{Gain}\}$

**信息增益 Information Gain：**

- $IG(S, A) = H(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} H(S_v)$
- $H(S) = -\sum_{c \in C} p(c) \log_2 p(c)$

**随机森林 Random Forest：**

- $RF = \{Ensemble | Multiple_{decision_{trees}}\}$
- $Prediction = \frac{1}{T} \sum_{t=1}^T f_t(x)$

### 2.3 神经网络 Neural Networks

**前馈神经网络 Feedforward Neural Network：**

- $f(x) = W_L \sigma(W_{L-1} \sigma(...\sigma(W_1 x + b_1)...) + b_L)$
- $Activation_{Function} = \{\sigma | ReLU, Sigmoid, Tanh\}$

**反向传播 Backpropagation：**

- $\frac{\partial L}{\partial w_{ij}^{(l)}} = \frac{\partial L}{\partial z_i^{(l)}} \frac{\partial z_i^{(l)}}{\partial w_{ij}^{(l)}}$
- $z_i^{(l)} = \sum_j w_{ij}^{(l)} a_j^{(l-1)} + b_i^{(l)}$

**卷积神经网络 Convolutional Neural Network：**

- $Convolution = \{Filter | Feature_{extraction}\}$
- $Pooling = \{Downsampling | Max, Average\}$
- $Architecture = \{Conv \rightarrow Pool \rightarrow Conv \rightarrow Pool \rightarrow FC\}$

### 2.4 聚类算法 Clustering Algorithms

**K-means聚类：**

- $Objective = \min_{\{S_1,...,S_k\}} \sum_{i=1}^k \sum_{x \in S_i} \|x - \mu_i\|^2$
- $Update: \mu_i = \frac{1}{|S_i|} \sum_{x \in S_i} x$

**层次聚类 Hierarchical Clustering：**

- $Distance_{Matrix} = \{d_{ij} | Distance_{between_{clusters}}\}$
- $Linkage = \{Single, Complete, Average, Ward\}$

**DBSCAN：**

- $Core_{Point} = \{p | |N_\epsilon(p)| \geq MinPts\}$
- $Border_{Point} = \{p | p \notin Core_{Point} \land p \in N_\epsilon(q)\}$

## 3. 模型评估 / Model Evaluation

### 3.1 评估指标 Evaluation Metrics

**分类指标 Classification Metrics：**

- **准确率 Accuracy**：$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$
- **精确率 Precision**：$Precision = \frac{TP}{TP + FP}$
- **召回率 Recall**：$Recall = \frac{TP}{TP + FN}$
- **F1分数 F1-Score**：$F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$

**回归指标 Regression Metrics：**

- **均方误差 MSE**：$MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$
- **均方根误差 RMSE**：$RMSE = \sqrt{MSE}$
- **平均绝对误差 MAE**：$MAE = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|$
- **决定系数 R²**：$R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}$

### 3.2 交叉验证 Cross-Validation

**K折交叉验证 K-Fold Cross-Validation：**

- $CV_{Score} = \frac{1}{K} \sum_{k=1}^K Score_k$
- $Data_{Split} = \{D_1, D_2, ..., D_K | D_i \cap D_j = \emptyset\}$

**留一法交叉验证 Leave-One-Out：**

- $LOO = \{K = n | Each_{sample}\ as\ test_{set}\}$

## 4. 工程实现 / Engineering Implementation

```rust
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use std::time::Instant;
use serde::{Deserialize, Serialize};
use rand::Rng;

// 数据类型
#[derive(Debug, Clone, PartialEq)]
pub enum DataType {
    Numerical,
    Categorical,
    Text,
    Image,
}

// 数据集
#[derive(Debug, Clone)]
pub struct Dataset {
    features: Vec<Vec<f64>>,
    labels: Vec<f64>,
    feature_names: Vec<String>,
    data_type: DataType,
}

// 机器学习模型特征
#[derive(Debug, Clone)]
pub struct ModelFeatures {
    feature_importance: HashMap<String, f64>,
    feature_correlation: HashMap<(String, String), f64>,
    feature_statistics: FeatureStatistics,
}

#[derive(Debug, Clone)]
pub struct FeatureStatistics {
    mean: f64,
    std: f64,
    min: f64,
    max: f64,
    missing_count: usize,
}

// 线性回归模型
pub struct LinearRegression {
    weights: Vec<f64>,
    bias: f64,
    learning_rate: f64,
    max_iterations: usize,
    training_history: Vec<TrainingStep>,
}

#[derive(Debug, Clone)]
pub struct TrainingStep {
    iteration: usize,
    loss: f64,
    accuracy: f64,
    timestamp: Instant,
}

impl LinearRegression {
    pub fn new(feature_count: usize, learning_rate: f64) -> Self {
        let mut rng = rand::thread_rng();
        let weights = (0..feature_count)
            .map(|_| rng.gen_range(-0.1..0.1))
            .collect();
        
        LinearRegression {
            weights,
            bias: rng.gen_range(-0.1..0.1),
            learning_rate,
            max_iterations: 1000,
            training_history: Vec::new(),
        }
    }
    
    pub fn predict(&self, features: &[f64]) -> f64 {
        let mut prediction = self.bias;
        for (i, &feature) in features.iter().enumerate() {
            prediction += self.weights[i] * feature;
        }
        prediction
    }
    
    pub fn train(&mut self, dataset: &Dataset) -> Vec<TrainingStep> {
        let mut history = Vec::new();
        
        for iteration in 0..self.max_iterations {
            let mut total_loss = 0.0;
            let mut predictions = Vec::new();
            
            // 前向传播
            for (i, features) in dataset.features.iter().enumerate() {
                let prediction = self.predict(features);
                predictions.push(prediction);
                
                let loss = (prediction - dataset.labels[i]).powi(2);
                total_loss += loss;
            }
            
            // 计算平均损失
            let avg_loss = total_loss / dataset.features.len() as f64;
            
            // 反向传播
            self.backward_propagate(&dataset.features, &dataset.labels, &predictions);
            
            // 记录训练历史
            let step = TrainingStep {
                iteration,
                loss: avg_loss,
                accuracy: self.calculate_accuracy(&dataset),
                timestamp: Instant::now(),
            };
            history.push(step);
            
            // 早停条件
            if avg_loss < 0.001 {
                break;
            }
        }
        
        self.training_history = history.clone();
        history
    }
    
    fn backward_propagate(&mut self, features: &[Vec<f64>], labels: &[f64], predictions: &[f64]) {
        let n = features.len() as f64;
        
        // 计算梯度
        let mut weight_gradients = vec![0.0; self.weights.len()];
        let mut bias_gradient = 0.0;
        
        for (i, feature_vector) in features.iter().enumerate() {
            let error = predictions[i] - labels[i];
            
            // 权重梯度
            for (j, &feature) in feature_vector.iter().enumerate() {
                weight_gradients[j] += error * feature;
            }
            
            // 偏置梯度
            bias_gradient += error;
        }
        
        // 更新参数
        for (i, gradient) in weight_gradients.iter().enumerate() {
            self.weights[i] -= self.learning_rate * gradient / n;
        }
        self.bias -= self.learning_rate * bias_gradient / n;
    }
    
    fn calculate_accuracy(&self, dataset: &Dataset) -> f64 {
        let mut correct_predictions = 0;
        let mut total_predictions = 0;
        
        for (i, features) in dataset.features.iter().enumerate() {
            let prediction = self.predict(features);
            let actual = dataset.labels[i];
            
            // 简化的准确率计算（对于回归问题使用阈值）
            if (prediction - actual).abs() < 0.1 {
                correct_predictions += 1;
            }
            total_predictions += 1;
        }
        
        correct_predictions as f64 / total_predictions as f64
    }
    
    pub fn get_weights(&self) -> &[f64] {
        &self.weights
    }
    
    pub fn get_bias(&self) -> f64 {
        self.bias
    }
}

// 逻辑回归模型
pub struct LogisticRegression {
    weights: Vec<f64>,
    bias: f64,
    learning_rate: f64,
    max_iterations: usize,
    training_history: Vec<TrainingStep>,
}

impl LogisticRegression {
    pub fn new(feature_count: usize, learning_rate: f64) -> Self {
        let mut rng = rand::thread_rng();
        let weights = (0..feature_count)
            .map(|_| rng.gen_range(-0.1..0.1))
            .collect();
        
        LogisticRegression {
            weights,
            bias: rng.gen_range(-0.1..0.1),
            learning_rate,
            max_iterations: 1000,
            training_history: Vec::new(),
        }
    }
    
    pub fn sigmoid(&self, z: f64) -> f64 {
        1.0 / (1.0 + (-z).exp())
    }
    
    pub fn predict(&self, features: &[f64]) -> f64 {
        let mut z = self.bias;
        for (i, &feature) in features.iter().enumerate() {
            z += self.weights[i] * feature;
        }
        self.sigmoid(z)
    }
    
    pub fn train(&mut self, dataset: &Dataset) -> Vec<TrainingStep> {
        let mut history = Vec::new();
        
        for iteration in 0..self.max_iterations {
            let mut total_loss = 0.0;
            let mut predictions = Vec::new();
            
            // 前向传播
            for (i, features) in dataset.features.iter().enumerate() {
                let prediction = self.predict(features);
                predictions.push(prediction);
                
                // 交叉熵损失
                let loss = if dataset.labels[i] == 1.0 {
                    -prediction.ln()
                } else {
                    -(1.0 - prediction).ln()
                };
                total_loss += loss;
            }
            
            // 计算平均损失
            let avg_loss = total_loss / dataset.features.len() as f64;
            
            // 反向传播
            self.backward_propagate(&dataset.features, &dataset.labels, &predictions);
            
            // 记录训练历史
            let step = TrainingStep {
                iteration,
                loss: avg_loss,
                accuracy: self.calculate_accuracy(&dataset),
                timestamp: Instant::now(),
            };
            history.push(step);
            
            // 早停条件
            if avg_loss < 0.1 {
                break;
            }
        }
        
        self.training_history = history.clone();
        history
    }
    
    fn backward_propagate(&mut self, features: &[Vec<f64>], labels: &[f64], predictions: &[f64]) {
        let n = features.len() as f64;
        
        // 计算梯度
        let mut weight_gradients = vec![0.0; self.weights.len()];
        let mut bias_gradient = 0.0;
        
        for (i, feature_vector) in features.iter().enumerate() {
            let error = predictions[i] - labels[i];
            
            // 权重梯度
            for (j, &feature) in feature_vector.iter().enumerate() {
                weight_gradients[j] += error * feature;
            }
            
            // 偏置梯度
            bias_gradient += error;
        }
        
        // 更新参数
        for (i, gradient) in weight_gradients.iter().enumerate() {
            self.weights[i] -= self.learning_rate * gradient / n;
        }
        self.bias -= self.learning_rate * bias_gradient / n;
    }
    
    fn calculate_accuracy(&self, dataset: &Dataset) -> f64 {
        let mut correct_predictions = 0;
        let mut total_predictions = 0;
        
        for (i, features) in dataset.features.iter().enumerate() {
            let prediction = self.predict(features);
            let actual = dataset.labels[i];
            
            // 二分类准确率
            let predicted_class = if prediction >= 0.5 { 1.0 } else { 0.0 };
            if predicted_class == actual {
                correct_predictions += 1;
            }
            total_predictions += 1;
        }
        
        correct_predictions as f64 / total_predictions as f64
    }
}

// 决策树模型
pub struct DecisionTree {
    root: Option<Box<TreeNode>>,
    max_depth: usize,
    min_samples_split: usize,
    min_samples_leaf: usize,
}

#[derive(Debug, Clone)]
pub struct TreeNode {
    feature_index: Option<usize>,
    threshold: Option<f64>,
    left_child: Option<Box<TreeNode>>,
    right_child: Option<Box<TreeNode>>,
    is_leaf: bool,
    prediction: Option<f64>,
    samples_count: usize,
}

impl DecisionTree {
    pub fn new(max_depth: usize, min_samples_split: usize, min_samples_leaf: usize) -> Self {
        DecisionTree {
            root: None,
            max_depth,
            min_samples_split,
            min_samples_leaf,
        }
    }
    
    pub fn fit(&mut self, dataset: &Dataset) {
        self.root = Some(self.build_tree(&dataset.features, &dataset.labels, 0));
    }
    
    fn build_tree(&self, features: &[Vec<f64>], labels: &[f64], depth: usize) -> Box<TreeNode> {
        let samples_count = features.len();
        
        // 停止条件
        if depth >= self.max_depth || samples_count < self.min_samples_split {
            return Box::new(TreeNode {
                feature_index: None,
                threshold: None,
                left_child: None,
                right_child: None,
                is_leaf: true,
                prediction: Some(self.calculate_leaf_prediction(labels)),
                samples_count,
            });
        }
        
        // 寻找最佳分割
        let (best_feature, best_threshold, best_gain) = self.find_best_split(features, labels);
        
        if best_gain <= 0.0 {
            return Box::new(TreeNode {
                feature_index: None,
                threshold: None,
                left_child: None,
                right_child: None,
                is_leaf: true,
                prediction: Some(self.calculate_leaf_prediction(labels)),
                samples_count,
            });
        }
        
        // 分割数据
        let (left_features, left_labels, right_features, right_labels) = 
            self.split_data(features, labels, best_feature, best_threshold);
        
        // 检查最小样本数
        if left_features.len() < self.min_samples_leaf || right_features.len() < self.min_samples_leaf {
            return Box::new(TreeNode {
                feature_index: None,
                threshold: None,
                left_child: None,
                right_child: None,
                is_leaf: true,
                prediction: Some(self.calculate_leaf_prediction(labels)),
                samples_count,
            });
        }
        
        // 递归构建子树
        let left_child = self.build_tree(&left_features, &left_labels, depth + 1);
        let right_child = self.build_tree(&right_features, &right_labels, depth + 1);
        
        Box::new(TreeNode {
            feature_index: Some(best_feature),
            threshold: Some(best_threshold),
            left_child: Some(left_child),
            right_child: Some(right_child),
            is_leaf: false,
            prediction: None,
            samples_count,
        })
    }
    
    fn find_best_split(&self, features: &[Vec<f64>], labels: &[f64]) -> (usize, f64, f64) {
        let mut best_feature = 0;
        let mut best_threshold = 0.0;
        let mut best_gain = 0.0;
        
        let parent_entropy = self.calculate_entropy(labels);
        
        for feature_index in 0..features[0].len() {
            let mut feature_values: Vec<f64> = features.iter()
                .map(|f| f[feature_index])
                .collect();
            feature_values.sort_by(|a, b| a.partial_cmp(b).unwrap());
            
            for i in 0..feature_values.len() - 1 {
                let threshold = (feature_values[i] + feature_values[i + 1]) / 2.0;
                let (left_labels, right_labels) = self.split_labels_by_threshold(
                    features, labels, feature_index, threshold
                );
                
                let gain = self.calculate_information_gain(parent_entropy, &left_labels, &right_labels);
                
                if gain > best_gain {
                    best_gain = gain;
                    best_feature = feature_index;
                    best_threshold = threshold;
                }
            }
        }
        
        (best_feature, best_threshold, best_gain)
    }
    
    fn calculate_entropy(&self, labels: &[f64]) -> f64 {
        let mut class_counts = HashMap::new();
        for &label in labels {
            *class_counts.entry(label).or_insert(0) += 1;
        }
        
        let total = labels.len() as f64;
        let mut entropy = 0.0;
        
        for count in class_counts.values() {
            let probability = *count as f64 / total;
            if probability > 0.0 {
                entropy -= probability * probability.ln();
            }
        }
        
        entropy
    }
    
    fn calculate_information_gain(&self, parent_entropy: f64, left_labels: &[f64], right_labels: &[f64]) -> f64 {
        let total_samples = left_labels.len() + right_labels.len();
        let left_entropy = self.calculate_entropy(left_labels);
        let right_entropy = self.calculate_entropy(right_labels);
        
        let left_weight = left_labels.len() as f64 / total_samples as f64;
        let right_weight = right_labels.len() as f64 / total_samples as f64;
        
        parent_entropy - (left_weight * left_entropy + right_weight * right_entropy)
    }
    
    fn split_labels_by_threshold(&self, features: &[Vec<f64>], labels: &[f64], feature_index: usize, threshold: f64) -> (Vec<f64>, Vec<f64>) {
        let mut left_labels = Vec::new();
        let mut right_labels = Vec::new();
        
        for (i, feature_vector) in features.iter().enumerate() {
            if feature_vector[feature_index] <= threshold {
                left_labels.push(labels[i]);
            } else {
                right_labels.push(labels[i]);
            }
        }
        
        (left_labels, right_labels)
    }
    
    fn split_data(&self, features: &[Vec<f64>], labels: &[f64], feature_index: usize, threshold: f64) -> (Vec<Vec<f64>>, Vec<f64>, Vec<Vec<f64>>, Vec<f64>) {
        let mut left_features = Vec::new();
        let mut left_labels = Vec::new();
        let mut right_features = Vec::new();
        let mut right_labels = Vec::new();
        
        for (i, feature_vector) in features.iter().enumerate() {
            if feature_vector[feature_index] <= threshold {
                left_features.push(feature_vector.clone());
                left_labels.push(labels[i]);
            } else {
                right_features.push(feature_vector.clone());
                right_labels.push(labels[i]);
            }
        }
        
        (left_features, left_labels, right_features, right_labels)
    }
    
    fn calculate_leaf_prediction(&self, labels: &[f64]) -> f64 {
        // 对于回归问题，返回平均值
        labels.iter().sum::<f64>() / labels.len() as f64
    }
    
    pub fn predict(&self, features: &[f64]) -> f64 {
        if let Some(ref root) = self.root {
            self.predict_recursive(root, features)
        } else {
            0.0
        }
    }
    
    fn predict_recursive(&self, node: &TreeNode, features: &[f64]) -> f64 {
        if node.is_leaf {
            return node.prediction.unwrap_or(0.0);
        }
        
        if let (Some(feature_index), Some(threshold)) = (node.feature_index, node.threshold) {
            if features[feature_index] <= threshold {
                if let Some(ref left_child) = node.left_child {
                    return self.predict_recursive(left_child, features);
                }
            } else {
                if let Some(ref right_child) = node.right_child {
                    return self.predict_recursive(right_child, features);
                }
            }
        }
        
        0.0
    }
}

// 模型评估器
pub struct ModelEvaluator {
    metrics: HashMap<String, f64>,
    confusion_matrix: Option<ConfusionMatrix>,
}

#[derive(Debug, Clone)]
pub struct ConfusionMatrix {
    true_positives: usize,
    true_negatives: usize,
    false_positives: usize,
    false_negatives: usize,
}

impl ModelEvaluator {
    pub fn new() -> Self {
        ModelEvaluator {
            metrics: HashMap::new(),
            confusion_matrix: None,
        }
    }
    
    pub fn evaluate_classification(&mut self, predictions: &[f64], actuals: &[f64]) -> HashMap<String, f64> {
        let mut confusion_matrix = ConfusionMatrix {
            true_positives: 0,
            true_negatives: 0,
            false_positives: 0,
            false_negatives: 0,
        };
        
        for (prediction, actual) in predictions.iter().zip(actuals.iter()) {
            let pred_class = if *prediction >= 0.5 { 1.0 } else { 0.0 };
            
            match (pred_class, *actual) {
                (1.0, 1.0) => confusion_matrix.true_positives += 1,
                (0.0, 0.0) => confusion_matrix.true_negatives += 1,
                (1.0, 0.0) => confusion_matrix.false_positives += 1,
                (0.0, 1.0) => confusion_matrix.false_negatives += 1,
                _ => {}
            }
        }
        
        self.confusion_matrix = Some(confusion_matrix.clone());
        
        let total = predictions.len() as f64;
        let tp = confusion_matrix.true_positives as f64;
        let tn = confusion_matrix.true_negatives as f64;
        let fp = confusion_matrix.false_positives as f64;
        let fn = confusion_matrix.false_negatives as f64;
        
        let accuracy = (tp + tn) / total;
        let precision = if tp + fp > 0.0 { tp / (tp + fp) } else { 0.0 };
        let recall = if tp + fn > 0.0 { tp / (tp + fn) } else { 0.0 };
        let f1_score = if precision + recall > 0.0 {
            2.0 * precision * recall / (precision + recall)
        } else {
            0.0
        };
        
        let mut metrics = HashMap::new();
        metrics.insert("accuracy".to_string(), accuracy);
        metrics.insert("precision".to_string(), precision);
        metrics.insert("recall".to_string(), recall);
        metrics.insert("f1_score".to_string(), f1_score);
        
        self.metrics = metrics.clone();
        metrics
    }
    
    pub fn evaluate_regression(&mut self, predictions: &[f64], actuals: &[f64]) -> HashMap<String, f64> {
        let n = predictions.len() as f64;
        
        let mut mse = 0.0;
        let mut mae = 0.0;
        let mut total_squared_error = 0.0;
        let mut total_absolute_error = 0.0;
        
        for (prediction, actual) in predictions.iter().zip(actuals.iter()) {
            let squared_error = (prediction - actual).powi(2);
            let absolute_error = (prediction - actual).abs();
            
            total_squared_error += squared_error;
            total_absolute_error += absolute_error;
        }
        
        mse = total_squared_error / n;
        mae = total_absolute_error / n;
        let rmse = mse.sqrt();
        
        // 计算R²
        let mean_actual = actuals.iter().sum::<f64>() / n;
        let mut total_variance = 0.0;
        for actual in actuals {
            total_variance += (actual - mean_actual).powi(2);
        }
        
        let r_squared = if total_variance > 0.0 {
            1.0 - total_squared_error / total_variance
        } else {
            0.0
        };
        
        let mut metrics = HashMap::new();
        metrics.insert("mse".to_string(), mse);
        metrics.insert("rmse".to_string(), rmse);
        metrics.insert("mae".to_string(), mae);
        metrics.insert("r_squared".to_string(), r_squared);
        
        self.metrics = metrics.clone();
        metrics
    }
    
    pub fn get_metrics(&self) -> &HashMap<String, f64> {
        &self.metrics
    }
    
    pub fn get_confusion_matrix(&self) -> Option<&ConfusionMatrix> {
        self.confusion_matrix.as_ref()
    }
}

// 交叉验证器
pub struct CrossValidator {
    k_folds: usize,
    metrics: Vec<HashMap<String, f64>>,
}

impl CrossValidator {
    pub fn new(k_folds: usize) -> Self {
        CrossValidator {
            k_folds,
            metrics: Vec::new(),
        }
    }
    
    pub fn cross_validate<F>(&mut self, dataset: &Dataset, model_factory: F) -> HashMap<String, f64>
    where
        F: Fn() -> Box<dyn MLModel>,
    {
        let fold_size = dataset.features.len() / self.k_folds;
        let mut fold_metrics = Vec::new();
        
        for fold in 0..self.k_folds {
            let start_idx = fold * fold_size;
            let end_idx = if fold == self.k_folds - 1 {
                dataset.features.len()
            } else {
                start_idx + fold_size
            };
            
            // 分割训练集和测试集
            let mut train_features = Vec::new();
            let mut train_labels = Vec::new();
            let mut test_features = Vec::new();
            let mut test_labels = Vec::new();
            
            for i in 0..dataset.features.len() {
                if i >= start_idx && i < end_idx {
                    test_features.push(dataset.features[i].clone());
                    test_labels.push(dataset.labels[i]);
                } else {
                    train_features.push(dataset.features[i].clone());
                    train_labels.push(dataset.labels[i]);
                }
            }
            
            let train_dataset = Dataset {
                features: train_features,
                labels: train_labels,
                feature_names: dataset.feature_names.clone(),
                data_type: dataset.data_type.clone(),
            };
            
            let test_dataset = Dataset {
                features: test_features,
                labels: test_labels,
                feature_names: dataset.feature_names.clone(),
                data_type: dataset.data_type.clone(),
            };
            
            // 训练模型
            let mut model = model_factory();
            model.fit(&train_dataset);
            
            // 预测
            let mut predictions = Vec::new();
            for features in &test_dataset.features {
                predictions.push(model.predict(features));
            }
            
            // 评估
            let mut evaluator = ModelEvaluator::new();
            let metrics = if dataset.data_type == DataType::Categorical {
                evaluator.evaluate_classification(&predictions, &test_dataset.labels)
            } else {
                evaluator.evaluate_regression(&predictions, &test_dataset.labels)
            };
            
            fold_metrics.push(metrics);
        }
        
        // 计算平均指标
        self.calculate_average_metrics(&fold_metrics)
    }
    
    fn calculate_average_metrics(&self, fold_metrics: &[HashMap<String, f64>]) -> HashMap<String, f64> {
        let mut average_metrics = HashMap::new();
        
        if fold_metrics.is_empty() {
            return average_metrics;
        }
        
        let metric_names: Vec<String> = fold_metrics[0].keys().cloned().collect();
        
        for metric_name in metric_names {
            let sum: f64 = fold_metrics.iter()
                .filter_map(|metrics| metrics.get(&metric_name))
                .sum();
            let average = sum / fold_metrics.len() as f64;
            average_metrics.insert(metric_name, average);
        }
        
        average_metrics
    }
}

// 机器学习模型特征
pub trait MLModel {
    fn fit(&mut self, dataset: &Dataset);
    fn predict(&self, features: &[f64]) -> f64;
}

impl MLModel for LinearRegression {
    fn fit(&mut self, dataset: &Dataset) {
        self.train(dataset);
    }
    
    fn predict(&self, features: &[f64]) -> f64 {
        self.predict(features)
    }
}

impl MLModel for LogisticRegression {
    fn fit(&mut self, dataset: &Dataset) {
        self.train(dataset);
    }
    
    fn predict(&self, features: &[f64]) -> f64 {
        self.predict(features)
    }
}

impl MLModel for DecisionTree {
    fn fit(&mut self, dataset: &Dataset) {
        self.fit(dataset);
    }
    
    fn predict(&self, features: &[f64]) -> f64 {
        self.predict(features)
    }
}
```

## 5. 批判性分析 / Critical Analysis

### 5.1 理论局限性 / Theoretical Limitations

- **过拟合风险 Overfitting Risk**：模型在训练集上表现良好但泛化能力差。
- **数据偏差 Data Bias**：训练数据中的偏差导致模型偏见。
- **可解释性不足 Interpretability Issues**：复杂模型缺乏可解释性。

### 5.2 工程挑战 / Engineering Challenges

- **数据质量 Data Quality**：高质量训练数据的获取和预处理。
- **计算资源 Computational Resources**：大规模模型训练的计算需求。
- **模型部署 Model Deployment**：模型在生产环境中的部署和维护。

## 6. 工程论证 / Engineering Arguments

- **推荐系统**：如电商推荐，需处理大规模用户行为数据。
- **金融风控**：如信用评估，需高精度分类和风险评估。
- **医疗诊断**：如疾病预测，需高可靠性模型和可解释性。

---
> 本文件为机器学习基础的系统化重构，采用严格的形式化定义、数学表达、工程实现，确保内容的学术规范性和工程实用性。
> This file provides systematic refactoring of machine learning fundamentals, using strict formal definitions, mathematical expressions, and engineering implementations, ensuring academic standards and engineering practicality.
