# 6.1.1 负载均衡 / Load Balancing


<!-- TOC START -->

- [6.1.1 负载均衡 / Load Balancing](#611-负载均衡-load-balancing)
  - [目录](#目录)
  - [1. 负载均衡理论 / Load Balancing Theory](#1-负载均衡理论-load-balancing-theory)
    - [1.1 负载均衡定义 / Load Balancing Definition](#11-负载均衡定义-load-balancing-definition)
    - [1.2 负载均衡算法分类 / Load Balancing Algorithm Classification](#12-负载均衡算法分类-load-balancing-algorithm-classification)
  - [2. 负载均衡算法实现 / Load Balancing Algorithm Implementation](#2-负载均衡算法实现-load-balancing-algorithm-implementation)
    - [2.1 轮询算法实现 / Round Robin Implementation](#21-轮询算法实现-round-robin-implementation)
    - [2.2 最少连接算法实现 / Least Connections Implementation](#22-最少连接算法实现-least-connections-implementation)
    - [2.3 响应时间算法实现 / Response Time Implementation](#23-响应时间算法实现-response-time-implementation)
  - [3. 健康检查实现 / Health Check Implementation](#3-健康检查实现-health-check-implementation)
    - [3.1 健康检查机制 / Health Check Mechanism](#31-健康检查机制-health-check-mechanism)
    - [3.2 会话保持 / Session Persistence](#32-会话保持-session-persistence)
  - [4. 负载均衡器实现 / Load Balancer Implementation](#4-负载均衡器实现-load-balancer-implementation)
    - [4.1 完整负载均衡器 / Complete Load Balancer](#41-完整负载均衡器-complete-load-balancer)
  - [5. 批判性分析 / Critical Analysis](#5-批判性分析-critical-analysis)
    - [5.1 负载均衡的优势 / Advantages of Load Balancing](#51-负载均衡的优势-advantages-of-load-balancing)
    - [5.2 局限性分析 / Limitation Analysis](#52-局限性分析-limitation-analysis)
    - [5.3 工程权衡 / Engineering Trade-offs](#53-工程权衡-engineering-trade-offs)

<!-- TOC END -->

## 目录

- [6.1.1 负载均衡 / Load Balancing](#611-负载均衡--load-balancing)
  - [目录](#目录)
  - [1. 负载均衡理论 / Load Balancing Theory](#1-负载均衡理论--load-balancing-theory)
    - [1.1 负载均衡定义 / Load Balancing Definition](#11-负载均衡定义--load-balancing-definition)
    - [1.2 负载均衡算法分类 / Load Balancing Algorithm Classification](#12-负载均衡算法分类--load-balancing-algorithm-classification)
  - [2. 负载均衡算法实现 / Load Balancing Algorithm Implementation](#2-负载均衡算法实现--load-balancing-algorithm-implementation)
    - [2.1 轮询算法实现 / Round Robin Implementation](#21-轮询算法实现--round-robin-implementation)
    - [2.2 最少连接算法实现 / Least Connections Implementation](#22-最少连接算法实现--least-connections-implementation)
    - [2.3 响应时间算法实现 / Response Time Implementation](#23-响应时间算法实现--response-time-implementation)
  - [3. 健康检查实现 / Health Check Implementation](#3-健康检查实现--health-check-implementation)
    - [3.1 健康检查机制 / Health Check Mechanism](#31-健康检查机制--health-check-mechanism)
    - [3.2 会话保持 / Session Persistence](#32-会话保持--session-persistence)
  - [4. 负载均衡器实现 / Load Balancer Implementation](#4-负载均衡器实现--load-balancer-implementation)
    - [4.1 完整负载均衡器 / Complete Load Balancer](#41-完整负载均衡器--complete-load-balancer)
  - [5. 批判性分析 / Critical Analysis](#5-批判性分析--critical-analysis)
    - [5.1 负载均衡的优势 / Advantages of Load Balancing](#51-负载均衡的优势--advantages-of-load-balancing)
    - [5.2 局限性分析 / Limitation Analysis](#52-局限性分析--limitation-analysis)
    - [5.3 工程权衡 / Engineering Trade-offs](#53-工程权衡--engineering-trade-offs)

## 1. 负载均衡理论 / Load Balancing Theory

### 1.1 负载均衡定义 / Load Balancing Definition

**负载均衡形式化定义：**

- $Load_{Balancer} = (Servers, Algorithm, Health_{Checker}, Metrics)$  
  Load balancer consists of servers, algorithm, health checker, and metrics
- $Servers = \{Server_1, Server_2, ..., Server_n\}$：服务器集合  
  Set of servers
- $Algorithm: Request \times Server_{State} \rightarrow Server$：负载均衡算法  
  Load balancing algorithm: maps request and server state to server
- $Health_{Checker}: Server \rightarrow Health_{Status}$：健康检查函数  
  Health checker: maps server to health status

**负载均衡目标：**

- **负载分布**：$Load_{Distribution} = \{\forall Server_i, Load_i \approx \frac{Total_{Load}}{n}\}$  
  Load distribution: load on each server approximately equal to total load divided by number of servers
- **响应时间最小化**：$Response_{Time} = \min\{\sum_{i=1}^{n} Response_{Time}_i\}$  
  Response time minimization: minimize sum of response times
- **吞吐量最大化**：$Throughput = \max\{\sum_{i=1}^{n} Throughput_i\}$  
  Throughput maximization: maximize sum of throughputs

### 1.2 负载均衡算法分类 / Load Balancing Algorithm Classification

**静态算法：**

- **轮询算法**：$Round_{Robin} = \{Server_i = Server_{(i-1) \bmod n + 1}\}$  
  Round robin: server selection based on modulo operation
- **加权轮询**：$Weighted_{Round}_{Robin} = \{Server_i \propto Weight_i\}$  
  Weighted round robin: server selection proportional to weight
- **哈希算法**：$Hash_{Based} = \{Server_i = Hash(Request) \bmod n\}$  
  Hash-based: server selection based on request hash

**动态算法：**

- **最少连接**：$Least_{Connections} = \{Server_i = \arg\min\{Connections_j\}\}$  
  Least connections: select server with minimum connections
- **加权最少连接**：$Weighted_{Least}_{Connections} = \{Server_i = \arg\min\{\frac{Connections_j}{Weight_j}\}\}$  
  Weighted least connections: select server with minimum connections per weight
- **响应时间**：$Response_{Time}_{Based} = \{Server_i = \arg\min\{Response_{Time}_j\}\}$  
  Response time based: select server with minimum response time

## 2. 负载均衡算法实现 / Load Balancing Algorithm Implementation

### 2.1 轮询算法实现 / Round Robin Implementation

**基础轮询算法：**

```rust
use std::sync::{Arc, Mutex};
use std::collections::VecDeque;

#[derive(Debug, Clone)]
pub struct Server {
    id: String,
    address: String,
    port: u16,
    weight: u32,
    health_status: HealthStatus,
    current_connections: u32,
    response_time: f64,
}

#[derive(Debug, Clone, PartialEq)]
pub enum HealthStatus {
    Healthy,
    Unhealthy,
    Unknown,
}

pub struct RoundRobinBalancer {
    servers: Arc<Mutex<Vec<Server>>>,
    current_index: Arc<Mutex<usize>>,
}

impl RoundRobinBalancer {
    pub fn new() -> Self {
        RoundRobinBalancer {
            servers: Arc::new(Mutex::new(Vec::new())),
            current_index: Arc::new(Mutex::new(0)),
        }
    }
    
    pub fn add_server(&self, server: Server) {
        let mut servers = self.servers.lock().unwrap();
        servers.push(server);
    }
    
    pub fn remove_server(&self, server_id: &str) {
        let mut servers = self.servers.lock().unwrap();
        servers.retain(|server| server.id != server_id);
    }
    
    pub fn get_next_server(&self) -> Option<Server> {
        let servers = self.servers.lock().unwrap();
        if servers.is_empty() {
            return None;
        }
        
        let mut current_index = self.current_index.lock().unwrap();
        let server = servers[*current_index].clone();
        
        *current_index = (*current_index + 1) % servers.len();
        
        Some(server)
    }
    
    pub fn get_next_healthy_server(&self) -> Option<Server> {
        let servers = self.servers.lock().unwrap();
        if servers.is_empty() {
            return None;
        }
        
        let mut current_index = self.current_index.lock().unwrap();
        let start_index = *current_index;
        
        loop {
            let server = &servers[*current_index];
            if server.health_status == HealthStatus::Healthy {
                let selected_server = server.clone();
                *current_index = (*current_index + 1) % servers.len();
                return Some(selected_server);
            }
            
            *current_index = (*current_index + 1) % servers.len();
            
            // 如果已经检查了所有服务器，返回None
            if *current_index == start_index {
                return None;
            }
        }
    }
}

// 加权轮询算法
pub struct WeightedRoundRobinBalancer {
    servers: Arc<Mutex<Vec<Server>>>,
    current_weight: Arc<Mutex<i32>>,
    max_weight: Arc<Mutex<i32>>,
    gcd: Arc<Mutex<i32>>,
}

impl WeightedRoundRobinBalancer {
    pub fn new() -> Self {
        WeightedRoundRobinBalancer {
            servers: Arc::new(Mutex::new(Vec::new())),
            current_weight: Arc::new(Mutex::new(0)),
            max_weight: Arc::new(Mutex::new(0)),
            gcd: Arc::new(Mutex::new(0)),
        }
    }
    
    pub fn add_server(&self, server: Server) {
        let mut servers = self.servers.lock().unwrap();
        servers.push(server);
        
        // 更新最大权重和GCD
        self.update_weights();
    }
    
    fn update_weights(&self) {
        let servers = self.servers.lock().unwrap();
        let weights: Vec<i32> = servers.iter().map(|s| s.weight as i32).collect();
        
        let max_weight = weights.iter().max().unwrap_or(&1);
        let gcd = self.calculate_gcd(&weights);
        
        *self.max_weight.lock().unwrap() = *max_weight;
        *self.gcd.lock().unwrap() = gcd;
    }
    
    fn calculate_gcd(&self, numbers: &[i32]) -> i32 {
        if numbers.is_empty() {
            return 1;
        }
        
        let mut gcd = numbers[0];
        for &num in &numbers[1..] {
            gcd = self.gcd(gcd, num);
        }
        
        gcd
    }
    
    fn gcd(&self, mut a: i32, mut b: i32) -> i32 {
        while b != 0 {
            let temp = b;
            b = a % b;
            a = temp;
        }
        a
    }
    
    pub fn get_next_server(&self) -> Option<Server> {
        let servers = self.servers.lock().unwrap();
        if servers.is_empty() {
            return None;
        }
        
        let mut current_weight = self.current_weight.lock().unwrap();
        let max_weight = *self.max_weight.lock().unwrap();
        let gcd = *self.gcd.lock().unwrap();
        
        loop {
            *current_weight -= gcd;
            if *current_weight <= 0 {
                *current_weight = max_weight;
            }
            
            // 找到权重大于等于当前权重的服务器
            for server in servers.iter() {
                if server.weight as i32 >= *current_weight && server.health_status == HealthStatus::Healthy {
                    return Some(server.clone());
                }
            }
        }
    }
}
```

### 2.2 最少连接算法实现 / Least Connections Implementation

**最少连接算法：**

```rust
use std::collections::BinaryHeap;
use std::cmp::Ordering;

#[derive(Debug, Clone)]
pub struct ServerWithConnections {
    server: Server,
    connection_count: u32,
}

impl PartialEq for ServerWithConnections {
    fn eq(&self, other: &Self) -> bool {
        self.connection_count == other.connection_count
    }
}

impl Eq for ServerWithConnections {}

impl PartialOrd for ServerWithConnections {
    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
        Some(self.cmp(other))
    }
}

impl Ord for ServerWithConnections {
    fn cmp(&self, other: &Self) -> Ordering {
        // 最小堆，所以反转比较
        other.connection_count.cmp(&self.connection_count)
    }
}

pub struct LeastConnectionsBalancer {
    servers: Arc<Mutex<BinaryHeap<ServerWithConnections>>>,
    server_map: Arc<Mutex<HashMap<String, Server>>>,
}

impl LeastConnectionsBalancer {
    pub fn new() -> Self {
        LeastConnectionsBalancer {
            servers: Arc::new(Mutex::new(BinaryHeap::new())),
            server_map: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    pub fn add_server(&self, server: Server) {
        let server_with_connections = ServerWithConnections {
            server: server.clone(),
            connection_count: 0,
        };
        
        {
            let mut servers = self.servers.lock().unwrap();
            servers.push(server_with_connections);
        }
        
        {
            let mut server_map = self.server_map.lock().unwrap();
            server_map.insert(server.id.clone(), server);
        }
    }
    
    pub fn get_next_server(&self) -> Option<Server> {
        let mut servers = self.servers.lock().unwrap();
        
        // 找到健康的服务器
        let mut healthy_servers = Vec::new();
        let mut selected_server = None;
        
        while let Some(server_with_connections) = servers.pop() {
            if server_with_connections.server.health_status == HealthStatus::Healthy {
                selected_server = Some(server_with_connections);
                break;
            } else {
                healthy_servers.push(server_with_connections);
            }
        }
        
        // 将其他服务器放回堆中
        for server in healthy_servers {
            servers.push(server);
        }
        
        if let Some(selected) = selected_server {
            // 增加连接数并重新放入堆中
            let mut updated_server = selected.clone();
            updated_server.connection_count += 1;
            servers.push(updated_server);
            
            Some(selected.server)
        } else {
            None
        }
    }
    
    pub fn release_connection(&self, server_id: &str) {
        let mut servers = self.servers.lock().unwrap();
        let mut server_map = self.server_map.lock().unwrap();
        
        if let Some(server) = server_map.get_mut(server_id) {
            server.current_connections = server.current_connections.saturating_sub(1);
        }
        
        // 重新构建堆
        let mut new_heap = BinaryHeap::new();
        while let Some(server_with_connections) = servers.pop() {
            if server_with_connections.server.id == server_id {
                let mut updated = server_with_connections.clone();
                updated.connection_count = updated.connection_count.saturating_sub(1);
                new_heap.push(updated);
            } else {
                new_heap.push(server_with_connections);
            }
        }
        
        *servers = new_heap;
    }
}

// 加权最少连接算法
pub struct WeightedLeastConnectionsBalancer {
    servers: Arc<Mutex<Vec<Server>>>,
}

impl WeightedLeastConnectionsBalancer {
    pub fn new() -> Self {
        WeightedLeastConnectionsBalancer {
            servers: Arc::new(Mutex::new(Vec::new())),
        }
    }
    
    pub fn add_server(&self, server: Server) {
        let mut servers = self.servers.lock().unwrap();
        servers.push(server);
    }
    
    pub fn get_next_server(&self) -> Option<Server> {
        let servers = self.servers.lock().unwrap();
        
        let mut best_server = None;
        let mut best_score = f64::MAX;
        
        for server in servers.iter() {
            if server.health_status == HealthStatus::Healthy {
                let score = server.current_connections as f64 / server.weight as f64;
                if score < best_score {
                    best_score = score;
                    best_server = Some(server.clone());
                }
            }
        }
        
        best_server
    }
    
    pub fn increment_connections(&self, server_id: &str) {
        let mut servers = self.servers.lock().unwrap();
        if let Some(server) = servers.iter_mut().find(|s| s.id == server_id) {
            server.current_connections += 1;
        }
    }
    
    pub fn decrement_connections(&self, server_id: &str) {
        let mut servers = self.servers.lock().unwrap();
        if let Some(server) = servers.iter_mut().find(|s| s.id == server_id) {
            server.current_connections = server.current_connections.saturating_sub(1);
        }
    }
}
```

### 2.3 响应时间算法实现 / Response Time Implementation

**基于响应时间的负载均衡：**

```rust
use std::time::{Duration, Instant};

#[derive(Debug, Clone)]
pub struct ServerMetrics {
    server: Server,
    average_response_time: f64,
    response_time_samples: VecDeque<f64>,
    max_samples: usize,
    last_update: Instant,
}

impl ServerMetrics {
    pub fn new(server: Server, max_samples: usize) -> Self {
        ServerMetrics {
            server,
            average_response_time: 0.0,
            response_time_samples: VecDeque::new(),
            max_samples,
            last_update: Instant::now(),
        }
    }
    
    pub fn update_response_time(&mut self, response_time: f64) {
        self.response_time_samples.push_back(response_time);
        
        if self.response_time_samples.len() > self.max_samples {
            self.response_time_samples.pop_front();
        }
        
        // 计算平均响应时间
        let sum: f64 = self.response_time_samples.iter().sum();
        self.average_response_time = sum / self.response_time_samples.len() as f64;
        
        self.last_update = Instant::now();
    }
    
    pub fn get_score(&self) -> f64 {
        // 考虑响应时间和健康状态的综合评分
        let health_multiplier = match self.server.health_status {
            HealthStatus::Healthy => 1.0,
            HealthStatus::Unhealthy => 0.0,
            HealthStatus::Unknown => 0.5,
        };
        
        // 响应时间越短，评分越高
        let response_score = 1.0 / (1.0 + self.average_response_time);
        
        response_score * health_multiplier
    }
}

pub struct ResponseTimeBalancer {
    servers: Arc<Mutex<Vec<ServerMetrics>>>,
    update_interval: Duration,
}

impl ResponseTimeBalancer {
    pub fn new(update_interval: Duration) -> Self {
        ResponseTimeBalancer {
            servers: Arc::new(Mutex::new(Vec::new())),
            update_interval,
        }
    }
    
    pub fn add_server(&self, server: Server) {
        let server_metrics = ServerMetrics::new(server, 100);
        let mut servers = self.servers.lock().unwrap();
        servers.push(server_metrics);
    }
    
    pub fn get_next_server(&self) -> Option<Server> {
        let servers = self.servers.lock().unwrap();
        
        let mut best_server = None;
        let mut best_score = 0.0;
        
        for server_metrics in servers.iter() {
            let score = server_metrics.get_score();
            if score > best_score {
                best_score = score;
                best_server = Some(server_metrics.server.clone());
            }
        }
        
        best_server
    }
    
    pub fn update_server_response_time(&self, server_id: &str, response_time: f64) {
        let mut servers = self.servers.lock().unwrap();
        if let Some(server_metrics) = servers.iter_mut().find(|s| s.server.id == server_id) {
            server_metrics.update_response_time(response_time);
        }
    }
    
    pub fn get_server_metrics(&self, server_id: &str) -> Option<ServerMetrics> {
        let servers = self.servers.lock().unwrap();
        servers.iter()
            .find(|s| s.server.id == server_id)
            .cloned()
    }
}
```

## 3. 健康检查实现 / Health Check Implementation

### 3.1 健康检查机制 / Health Check Mechanism

**健康检查器：**

```rust
use tokio::time::{Duration, interval};
use std::sync::Arc;

pub struct HealthChecker {
    servers: Arc<Mutex<Vec<Server>>>,
    check_interval: Duration,
    timeout: Duration,
    failure_threshold: u32,
    success_threshold: u32,
}

impl HealthChecker {
    pub fn new(check_interval: Duration, timeout: Duration) -> Self {
        HealthChecker {
            servers: Arc::new(Mutex::new(Vec::new())),
            check_interval,
            timeout,
            failure_threshold: 3,
            success_threshold: 2,
        }
    }
    
    pub fn add_server(&self, server: Server) {
        let mut servers = self.servers.lock().unwrap();
        servers.push(server);
    }
    
    pub async fn start_health_check_loop(&self) {
        let mut interval = interval(self.check_interval);
        
        loop {
            interval.tick().await;
            self.perform_health_checks().await;
        }
    }
    
    async fn perform_health_checks(&self) {
        let servers = self.servers.lock().unwrap().clone();
        
        for mut server in servers {
            let health_status = self.check_server_health(&server).await;
            
            // 更新服务器健康状态
            {
                let mut servers = self.servers.lock().unwrap();
                if let Some(server_ref) = servers.iter_mut().find(|s| s.id == server.id) {
                    server_ref.health_status = health_status;
                }
            }
        }
    }
    
    async fn check_server_health(&self, server: &Server) -> HealthStatus {
        let health_url = format!("http://{}:{}/health", server.address, server.port);
        
        match tokio::time::timeout(self.timeout, reqwest::get(&health_url)).await {
            Ok(Ok(response)) => {
                if response.status().is_success() {
                    HealthStatus::Healthy
                } else {
                    HealthStatus::Unhealthy
                }
            }
            _ => HealthStatus::Unhealthy,
        }
    }
    
    pub fn set_failure_threshold(&mut self, threshold: u32) {
        self.failure_threshold = threshold;
    }
    
    pub fn set_success_threshold(&mut self, threshold: u32) {
        self.success_threshold = threshold;
    }
}

// TCP健康检查
pub struct TCPHealthChecker {
    servers: Arc<Mutex<Vec<Server>>>,
    check_interval: Duration,
    timeout: Duration,
}

impl TCPHealthChecker {
    pub fn new(check_interval: Duration, timeout: Duration) -> Self {
        TCPHealthChecker {
            servers: Arc::new(Mutex::new(Vec::new())),
            check_interval,
            timeout,
        }
    }
    
    pub async fn check_tcp_health(&self, server: &Server) -> HealthStatus {
        let addr = format!("{}:{}", server.address, server.port);
        
        match tokio::time::timeout(self.timeout, tokio::net::TcpStream::connect(&addr)).await {
            Ok(Ok(_)) => HealthStatus::Healthy,
            _ => HealthStatus::Unhealthy,
        }
    }
    
    pub async fn start_tcp_health_check_loop(&self) {
        let mut interval = interval(self.check_interval);
        
        loop {
            interval.tick().await;
            self.perform_tcp_health_checks().await;
        }
    }
    
    async fn perform_tcp_health_checks(&self) {
        let servers = self.servers.lock().unwrap().clone();
        
        for server in servers {
            let health_status = self.check_tcp_health(&server).await;
            
            // 更新服务器健康状态
            {
                let mut servers = self.servers.lock().unwrap();
                if let Some(server_ref) = servers.iter_mut().find(|s| s.id == server.id) {
                    server_ref.health_status = health_status;
                }
            }
        }
    }
}
```

### 3.2 会话保持 / Session Persistence

**会话保持实现：**

```rust
use std::collections::HashMap;
use std::time::{Duration, Instant};

#[derive(Debug, Clone)]
pub struct Session {
    session_id: String,
    server_id: String,
    created_at: Instant,
    last_accessed: Instant,
    ttl: Duration,
}

pub struct SessionManager {
    sessions: Arc<Mutex<HashMap<String, Session>>>,
    session_ttl: Duration,
    cleanup_interval: Duration,
}

impl SessionManager {
    pub fn new(session_ttl: Duration, cleanup_interval: Duration) -> Self {
        SessionManager {
            sessions: Arc::new(Mutex::new(HashMap::new())),
            session_ttl,
            cleanup_interval,
        }
    }
    
    pub fn create_session(&self, session_id: String, server_id: String) {
        let session = Session {
            session_id: session_id.clone(),
            server_id,
            created_at: Instant::now(),
            last_accessed: Instant::now(),
            ttl: self.session_ttl,
        };
        
        let mut sessions = self.sessions.lock().unwrap();
        sessions.insert(session_id, session);
    }
    
    pub fn get_server_for_session(&self, session_id: &str) -> Option<String> {
        let mut sessions = self.sessions.lock().unwrap();
        
        if let Some(session) = sessions.get_mut(session_id) {
            // 检查会话是否过期
            if session.last_accessed.elapsed() > session.ttl {
                sessions.remove(session_id);
                return None;
            }
            
            session.last_accessed = Instant::now();
            Some(session.server_id.clone())
        } else {
            None
        }
    }
    
    pub fn remove_session(&self, session_id: &str) {
        let mut sessions = self.sessions.lock().unwrap();
        sessions.remove(session_id);
    }
    
    pub async fn start_cleanup_loop(&self) {
        let mut interval = interval(self.cleanup_interval);
        
        loop {
            interval.tick().await;
            self.cleanup_expired_sessions();
        }
    }
    
    fn cleanup_expired_sessions(&self) {
        let mut sessions = self.sessions.lock().unwrap();
        let now = Instant::now();
        
        sessions.retain(|_, session| {
            now.duration_since(session.last_accessed) <= session.ttl
        });
    }
}

// 基于Cookie的会话保持
pub struct CookieBasedSessionManager {
    session_manager: SessionManager,
    cookie_name: String,
}

impl CookieBasedSessionManager {
    pub fn new(session_ttl: Duration, cookie_name: String) -> Self {
        CookieBasedSessionManager {
            session_manager: SessionManager::new(session_ttl, Duration::from_secs(300)),
            cookie_name,
        }
    }
    
    pub fn extract_session_id(&self, cookies: &str) -> Option<String> {
        for cookie in cookies.split(';') {
            let parts: Vec<&str> = cookie.trim().split('=').collect();
            if parts.len() == 2 && parts[0] == self.cookie_name {
                return Some(parts[1].to_string());
            }
        }
        None
    }
    
    pub fn get_server_for_request(&self, cookies: &str) -> Option<String> {
        if let Some(session_id) = self.extract_session_id(cookies) {
            self.session_manager.get_server_for_session(&session_id)
        } else {
            None
        }
    }
    
    pub fn create_session_cookie(&self, session_id: &str, server_id: &str) -> String {
        self.session_manager.create_session(session_id.to_string(), server_id.to_string());
        format!("{}={}; Path=/; HttpOnly", self.cookie_name, session_id)
    }
}
```

## 4. 负载均衡器实现 / Load Balancer Implementation

### 4.1 完整负载均衡器 / Complete Load Balancer

**负载均衡器核心：**

```rust
use tokio::net::{TcpListener, TcpStream};
use tokio::io::{AsyncReadExt, AsyncWriteExt};

pub struct LoadBalancer {
    balancer: Arc<Mutex<dyn BalancerAlgorithm + Send>>,
    health_checker: Arc<HealthChecker>,
    session_manager: Arc<SessionManager>,
    listener: TcpListener,
    metrics: Arc<LoadBalancerMetrics>,
}

#[async_trait]
pub trait BalancerAlgorithm {
    fn get_next_server(&self) -> Option<Server>;
    fn update_server_metrics(&self, server_id: &str, metrics: ServerMetrics);
}

pub struct LoadBalancerMetrics {
    total_requests: Arc<Mutex<u64>>,
    successful_requests: Arc<Mutex<u64>>,
    failed_requests: Arc<Mutex<u64>>,
    average_response_time: Arc<Mutex<f64>>,
}

impl LoadBalancer {
    pub async fn new(
        balancer: Box<dyn BalancerAlgorithm + Send>,
        health_checker: HealthChecker,
        session_manager: SessionManager,
        bind_address: String,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        let listener = TcpListener::bind(&bind_address).await?;
        
        Ok(LoadBalancer {
            balancer: Arc::new(Mutex::new(balancer)),
            health_checker: Arc::new(health_checker),
            session_manager: Arc::new(session_manager),
            listener,
            metrics: Arc::new(LoadBalancerMetrics {
                total_requests: Arc::new(Mutex::new(0)),
                successful_requests: Arc::new(Mutex::new(0)),
                failed_requests: Arc::new(Mutex::new(0)),
                average_response_time: Arc::new(Mutex::new(0.0)),
            }),
        })
    }
    
    pub async fn start(&self) {
        println!("Load balancer started");
        
        loop {
            match self.listener.accept().await {
                Ok((client_socket, client_addr)) => {
                    println!("New connection from: {}", client_addr);
                    
                    let balancer = self.balancer.clone();
                    let health_checker = self.health_checker.clone();
                    let session_manager = self.session_manager.clone();
                    let metrics = self.metrics.clone();
                    
                    tokio::spawn(async move {
                        Self::handle_connection(
                            client_socket,
                            balancer,
                            health_checker,
                            session_manager,
                            metrics,
                        ).await;
                    });
                }
                Err(e) => {
                    eprintln!("Failed to accept connection: {}", e);
                }
            }
        }
    }
    
    async fn handle_connection(
        mut client_socket: TcpStream,
        balancer: Arc<Mutex<dyn BalancerAlgorithm + Send>>,
        health_checker: Arc<HealthChecker>,
        session_manager: Arc<SessionManager>,
        metrics: Arc<LoadBalancerMetrics>,
    ) {
        let start_time = Instant::now();
        
        // 读取客户端请求
        let mut buffer = [0; 1024];
        let n = match client_socket.read(&mut buffer).await {
            Ok(n) if n == 0 => return,
            Ok(n) => n,
            Err(_) => return,
        };
        
        let request = String::from_utf8_lossy(&buffer[..n]);
        
        // 检查会话保持
        let server_id = if let Some(cookie) = extract_cookie(&request, "session") {
            session_manager.get_server_for_session(&cookie)
        } else {
            None
        };
        
        // 选择服务器
        let selected_server = if let Some(id) = server_id {
            // 使用会话保持的服务器
            health_checker.get_server_by_id(&id)
        } else {
            // 使用负载均衡算法选择服务器
            balancer.lock().unwrap().get_next_server()
        };
        
        if let Some(server) = selected_server {
            // 转发请求到后端服务器
            match Self::forward_request(&request, &server).await {
                Ok(response) => {
                    // 更新成功请求计数
                    {
                        let mut total = metrics.total_requests.lock().unwrap();
                        let mut successful = metrics.successful_requests.lock().unwrap();
                        *total += 1;
                        *successful += 1;
                    }
                    
                    // 更新响应时间
                    let response_time = start_time.elapsed().as_millis() as f64;
                    {
                        let mut avg_time = metrics.average_response_time.lock().unwrap();
                        *avg_time = (*avg_time + response_time) / 2.0;
                    }
                    
                    // 发送响应给客户端
                    if let Err(e) = client_socket.write_all(response.as_bytes()).await {
                        eprintln!("Failed to write response: {}", e);
                    }
                }
                Err(e) => {
                    eprintln!("Failed to forward request: {}", e);
                    
                    // 更新失败请求计数
                    {
                        let mut total = metrics.total_requests.lock().unwrap();
                        let mut failed = metrics.failed_requests.lock().unwrap();
                        *total += 1;
                        *failed += 1;
                    }
                }
            }
        } else {
            // 没有可用的服务器
            let error_response = "HTTP/1.1 503 Service Unavailable\r\n\r\n";
            if let Err(e) = client_socket.write_all(error_response.as_bytes()).await {
                eprintln!("Failed to write error response: {}", e);
            }
        }
    }
    
    async fn forward_request(request: &str, server: &Server) -> Result<String, Box<dyn std::error::Error>> {
        let server_addr = format!("{}:{}", server.address, server.port);
        let mut server_socket = TcpStream::connect(&server_addr).await?;
        
        // 发送请求到后端服务器
        server_socket.write_all(request.as_bytes()).await?;
        
        // 读取响应
        let mut response = Vec::new();
        let mut buffer = [0; 1024];
        
        loop {
            let n = server_socket.read(&mut buffer).await?;
            if n == 0 {
                break;
            }
            response.extend_from_slice(&buffer[..n]);
        }
        
        Ok(String::from_utf8_lossy(&response).to_string())
    }
}

fn extract_cookie(request: &str, cookie_name: &str) -> Option<String> {
    for line in request.lines() {
        if line.starts_with("Cookie: ") {
            let cookie_value = &line[8..];
            for cookie in cookie_value.split(';') {
                let parts: Vec<&str> = cookie.trim().split('=').collect();
                if parts.len() == 2 && parts[0] == cookie_name {
                    return Some(parts[1].to_string());
                }
            }
        }
    }
    None
}
```

## 5. 批判性分析 / Critical Analysis

### 5.1 负载均衡的优势 / Advantages of Load Balancing

- **高可用性**：单点故障不影响整体服务  
  High availability: single point failure doesn't affect overall service
- **可扩展性**：支持水平扩展和动态扩容  
  Scalability: supports horizontal scaling and dynamic expansion
- **性能优化**：通过负载分布提高系统性能  
  Performance optimization: improves system performance through load distribution

### 5.2 局限性分析 / Limitation Analysis

- **算法复杂性**：复杂算法增加系统开销  
  Algorithm complexity: complex algorithms increase system overhead
- **会话一致性**：分布式环境下的会话管理挑战  
  Session consistency: session management challenges in distributed environments
- **健康检查开销**：频繁健康检查消耗资源  
  Health check overhead: frequent health checks consume resources

### 5.3 工程权衡 / Engineering Trade-offs

- **算法复杂度 vs 性能**：复杂算法 vs 高性能  
  Algorithm complexity vs performance: complex algorithms vs high performance
- **一致性 vs 可用性**：强一致性 vs 高可用性  
  Consistency vs availability: strong consistency vs high availability
- **实时性 vs 准确性**：实时响应 vs 准确负载分布  
  Real-time vs accuracy: real-time response vs accurate load distribution

---

> 本文件为负载均衡的系统化重构，采用严格的形式化定义、数学证明和Rust代码实现，确保内容的学术规范性和工程实用性。
> This file provides systematic refactoring of load balancing, using strict formal definitions, mathematical proofs, and Rust code implementation, ensuring academic standards and engineering practicality.
