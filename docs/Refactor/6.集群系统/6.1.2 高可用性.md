# 6.1.2 集群高可用性 / Cluster High Availability


<!-- TOC START -->

- [6.1.2 集群高可用性 / Cluster High Availability](#612-集群高可用性-cluster-high-availability)
  - [6.1.2.1 高可用性架构 / High Availability Architecture](#6121-高可用性架构-high-availability-architecture)
    - [6.1.2.1.1 可用性设计原则 / Availability Design Principles](#61211-可用性设计原则-availability-design-principles)
    - [6.1.2.1.2 故障检测机制 / Fault Detection Mechanisms](#61212-故障检测机制-fault-detection-mechanisms)
  - [6.1.2.2 自动故障恢复 / Automatic Fault Recovery](#6122-自动故障恢复-automatic-fault-recovery)
    - [6.1.2.2.1 故障恢复策略 / Fault Recovery Strategies](#61221-故障恢复策略-fault-recovery-strategies)
  - [6.1.2.3 负载均衡与流量管理 / Load Balancing and Traffic Management](#6123-负载均衡与流量管理-load-balancing-and-traffic-management)
    - [6.1.2.3.1 智能负载均衡器 / Intelligent Load Balancer](#61231-智能负载均衡器-intelligent-load-balancer)
  - [总结 / Summary](#总结-summary)

<!-- TOC END -->

## 6.1.2.1 高可用性架构 / High Availability Architecture

### 6.1.2.1.1 可用性设计原则 / Availability Design Principles

**高可用性架构模式：**

```text
多层高可用架构 (Multi-Tier HA Architecture)
    ├── 应用层高可用 (Application Layer HA)
    │   ├── 无状态服务设计 (Stateless Service Design)
    │   ├── 服务负载均衡 (Service Load Balancing)
    │   ├── 断路器模式 (Circuit Breaker Pattern)
    │   └── 优雅降级 (Graceful Degradation)
    │
    ├── 集群层高可用 (Cluster Layer HA)
    │   ├── 节点冗余 (Node Redundancy)
    │   ├── 故障检测 (Failure Detection)
    │   ├── 自动故障转移 (Automatic Failover)
    │   └── 脑裂预防 (Split-Brain Prevention)
    │
    ├── 网络层高可用 (Network Layer HA)
    │   ├── 网络冗余 (Network Redundancy)
    │   ├── 链路聚合 (Link Aggregation)
    │   ├── 多路径路由 (Multi-Path Routing)
    │   └── 网络隔离 (Network Isolation)
    │
    └── 存储层高可用 (Storage Layer HA)
        ├── 数据复制 (Data Replication)
        ├── 分布式存储 (Distributed Storage)
        ├── 快照备份 (Snapshot Backup)
        └── 灾难恢复 (Disaster Recovery)
```

**SLA可用性等级定义：**

```rust
use std::time::{Duration, SystemTime, UNIX_EPOCH};
use std::collections::HashMap;
use serde::{Serialize, Deserialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AvailabilityTier {
    Basic,      // 99% - 8.76小时/年停机
    Standard,   // 99.9% - 52.56分钟/年停机
    Enhanced,   // 99.99% - 5.26分钟/年停机
    Premium,    // 99.999% - 31.5秒/年停机
    Mission,    // 99.9999% - 3.15秒/年停机
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SLARequirement {
    pub service_name: String,
    pub availability_tier: AvailabilityTier,
    pub rto: Duration,          // Recovery Time Objective
    pub rpo: Duration,          // Recovery Point Objective
    pub mttr: Duration,         // Mean Time To Recovery
    pub mtbf: Duration,         // Mean Time Between Failures
    pub planned_downtime: Duration, // 计划内停机时间
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AvailabilityMetrics {
    pub uptime: Duration,
    pub downtime: Duration,
    pub total_time: Duration,
    pub availability_percentage: f64,
    pub failure_count: u32,
    pub recovery_count: u32,
    pub average_recovery_time: Duration,
}

#[derive(Debug)]
pub struct HighAvailabilityManager {
    services: HashMap<String, ServiceStatus>,
    sla_requirements: HashMap<String, SLARequirement>,
    metrics: HashMap<String, AvailabilityMetrics>,
    fault_detectors: Vec<Box<dyn FaultDetector + Send + Sync>>,
    recovery_strategies: HashMap<String, Box<dyn RecoveryStrategy + Send + Sync>>,
}

#[derive(Debug, Clone)]
pub struct ServiceStatus {
    pub service_name: String,
    pub is_healthy: bool,
    pub last_health_check: SystemTime,
    pub failure_start_time: Option<SystemTime>,
    pub recovery_in_progress: bool,
    pub current_load: f64,
    pub error_rate: f64,
    pub response_time: Duration,
}

pub trait FaultDetector: Send + Sync {
    fn detect_fault(&self, service: &ServiceStatus) -> Option<FaultType>;
    fn get_confidence(&self) -> f64; // 0.0 to 1.0
}

pub trait RecoveryStrategy: Send + Sync {
    fn recover(&self, service_name: &str, fault_type: &FaultType) -> Result<(), RecoveryError>;
    fn get_recovery_time_estimate(&self, fault_type: &FaultType) -> Duration;
}

#[derive(Debug, Clone)]
pub enum FaultType {
    ServiceUnresponsive,
    HighLatency,
    HighErrorRate,
    ResourceExhaustion,
    NetworkPartition,
    HardwareFailure,
    DependencyFailure,
}

#[derive(Debug)]
pub enum RecoveryError {
    InsufficientResources,
    RecoveryTimeout,
    DependencyUnavailable,
    ConfigurationError,
}

impl HighAvailabilityManager {
    pub fn new() -> Self {
        Self {
            services: HashMap::new(),
            sla_requirements: HashMap::new(),
            metrics: HashMap::new(),
            fault_detectors: Vec::new(),
            recovery_strategies: HashMap::new(),
        }
    }
    
    pub fn register_service(&mut self, service_name: String, sla: SLARequirement) {
        let status = ServiceStatus {
            service_name: service_name.clone(),
            is_healthy: true,
            last_health_check: SystemTime::now(),
            failure_start_time: None,
            recovery_in_progress: false,
            current_load: 0.0,
            error_rate: 0.0,
            response_time: Duration::from_millis(0),
        };
        
        let metrics = AvailabilityMetrics {
            uptime: Duration::from_secs(0),
            downtime: Duration::from_secs(0),
            total_time: Duration::from_secs(0),
            availability_percentage: 100.0,
            failure_count: 0,
            recovery_count: 0,
            average_recovery_time: Duration::from_secs(0),
        };
        
        self.services.insert(service_name.clone(), status);
        self.sla_requirements.insert(service_name.clone(), sla);
        self.metrics.insert(service_name, metrics);
    }
    
    pub fn add_fault_detector(&mut self, detector: Box<dyn FaultDetector + Send + Sync>) {
        self.fault_detectors.push(detector);
    }
    
    pub fn add_recovery_strategy(&mut self, service_name: String, strategy: Box<dyn RecoveryStrategy + Send + Sync>) {
        self.recovery_strategies.insert(service_name, strategy);
    }
    
    pub async fn monitor_services(&mut self) -> Vec<(String, FaultType)> {
        let mut detected_faults = Vec::new();
        
        for (service_name, status) in &mut self.services {
            // 运行故障检测器
            for detector in &self.fault_detectors {
                if let Some(fault) = detector.detect_fault(status) {
                    detected_faults.push((service_name.clone(), fault.clone()));
                    
                    // 触发故障恢复
                    if let Err(e) = self.handle_fault(service_name, &fault).await {
                        eprintln!("Recovery failed for {}: {:?}", service_name, e);
                    }
                }
            }
            
            // 更新指标
            self.update_metrics(service_name, status);
        }
        
        detected_faults
    }
    
    async fn handle_fault(&mut self, service_name: &str, fault_type: &FaultType) -> Result<(), RecoveryError> {
        if let Some(status) = self.services.get_mut(service_name) {
            // 标记服务为不健康
            if status.is_healthy {
                status.is_healthy = false;
                status.failure_start_time = Some(SystemTime::now());
                
                // 更新故障计数
                if let Some(metrics) = self.metrics.get_mut(service_name) {
                    metrics.failure_count += 1;
                }
            }
            
            status.recovery_in_progress = true;
        }
        
        // 执行恢复策略
        if let Some(strategy) = self.recovery_strategies.get(service_name) {
            strategy.recover(service_name, fault_type)?;
            
            // 恢复成功，更新状态
            if let Some(status) = self.services.get_mut(service_name) {
                let recovery_time = status.failure_start_time
                    .map(|start| SystemTime::now().duration_since(start).unwrap_or(Duration::from_secs(0)))
                    .unwrap_or(Duration::from_secs(0));
                
                status.is_healthy = true;
                status.recovery_in_progress = false;
                status.failure_start_time = None;
                
                // 更新恢复指标
                if let Some(metrics) = self.metrics.get_mut(service_name) {
                    metrics.recovery_count += 1;
                    let total_recovery_time = metrics.average_recovery_time * (metrics.recovery_count - 1) as u32 + recovery_time;
                    metrics.average_recovery_time = total_recovery_time / metrics.recovery_count as u32;
                }
            }
        }
        
        Ok(())
    }
    
    fn update_metrics(&mut self, service_name: &str, status: &ServiceStatus) {
        if let Some(metrics) = self.metrics.get_mut(service_name) {
            let now = SystemTime::now();
            let elapsed = now.duration_since(status.last_health_check)
                .unwrap_or(Duration::from_secs(1));
            
            metrics.total_time += elapsed;
            
            if status.is_healthy {
                metrics.uptime += elapsed;
            } else {
                metrics.downtime += elapsed;
            }
            
            metrics.availability_percentage = 
                (metrics.uptime.as_secs_f64() / metrics.total_time.as_secs_f64()) * 100.0;
        }
    }
    
    pub fn get_sla_compliance(&self, service_name: &str) -> Result<SLACompliance, &'static str> {
        let sla = self.sla_requirements.get(service_name)
            .ok_or("SLA requirement not found")?;
        let metrics = self.metrics.get(service_name)
            .ok_or("Metrics not found")?;
        
        let required_availability = match sla.availability_tier {
            AvailabilityTier::Basic => 99.0,
            AvailabilityTier::Standard => 99.9,
            AvailabilityTier::Enhanced => 99.99,
            AvailabilityTier::Premium => 99.999,
            AvailabilityTier::Mission => 99.9999,
        };
        
        let compliance = SLACompliance {
            service_name: service_name.to_string(),
            required_availability,
            actual_availability: metrics.availability_percentage,
            is_compliant: metrics.availability_percentage >= required_availability,
            rto_compliant: metrics.average_recovery_time <= sla.rto,
            availability_gap: required_availability - metrics.availability_percentage,
            total_violations: if metrics.average_recovery_time > sla.rto { 1 } else { 0 },
        };
        
        Ok(compliance)
    }
    
    pub fn generate_availability_report(&self) -> String {
        let mut report = String::new();
        report.push_str("# High Availability Report\n\n");
        
        report.push_str("## Service Availability Summary\n\n");
        report.push_str("| Service | Availability | Uptime | Downtime | Failures | MTTR |\n");
        report.push_str("|---------|--------------|--------|----------|----------|------|\n");
        
        for (service_name, metrics) in &self.metrics {
            report.push_str(&format!(
                "| {} | {:.4}% | {}s | {}s | {} | {}s |\n",
                service_name,
                metrics.availability_percentage,
                metrics.uptime.as_secs(),
                metrics.downtime.as_secs(),
                metrics.failure_count,
                metrics.average_recovery_time.as_secs()
            ));
        }
        
        report.push_str("\n## SLA Compliance\n\n");
        for service_name in self.sla_requirements.keys() {
            if let Ok(compliance) = self.get_sla_compliance(service_name) {
                let status = if compliance.is_compliant { "✅ COMPLIANT" } else { "❌ NON-COMPLIANT" };
                report.push_str(&format!(
                    "- **{}**: {} (Required: {:.4}%, Actual: {:.4}%)\n",
                    service_name, status, compliance.required_availability, compliance.actual_availability
                ));
            }
        }
        
        report
    }
}

#[derive(Debug, Clone)]
pub struct SLACompliance {
    pub service_name: String,
    pub required_availability: f64,
    pub actual_availability: f64,
    pub is_compliant: bool,
    pub rto_compliant: bool,
    pub availability_gap: f64,
    pub total_violations: u32,
}
```

### 6.1.2.1.2 故障检测机制 / Fault Detection Mechanisms

**多层故障检测实现：**

```rust
use std::time::{Duration, SystemTime, UNIX_EPOCH};
use std::collections::{HashMap, VecDeque};
use tokio::time::{interval, timeout};

// 心跳检测器
pub struct HeartbeatDetector {
    heartbeat_interval: Duration,
    timeout_threshold: Duration,
    missed_heartbeat_threshold: u32,
    last_heartbeats: HashMap<String, SystemTime>,
    missed_counts: HashMap<String, u32>,
}

impl HeartbeatDetector {
    pub fn new(interval: Duration, timeout: Duration, threshold: u32) -> Self {
        Self {
            heartbeat_interval: interval,
            timeout_threshold: timeout,
            missed_heartbeat_threshold: threshold,
            last_heartbeats: HashMap::new(),
            missed_counts: HashMap::new(),
        }
    }
    
    pub fn record_heartbeat(&mut self, node_id: &str) {
        self.last_heartbeats.insert(node_id.to_string(), SystemTime::now());
        self.missed_counts.insert(node_id.to_string(), 0);
    }
    
    pub fn check_heartbeats(&mut self) -> Vec<String> {
        let now = SystemTime::now();
        let mut failed_nodes = Vec::new();
        
        for (node_id, last_heartbeat) in &self.last_heartbeats {
            if let Ok(elapsed) = now.duration_since(*last_heartbeat) {
                if elapsed > self.timeout_threshold {
                    let missed_count = self.missed_counts.entry(node_id.clone()).or_insert(0);
                    *missed_count += 1;
                    
                    if *missed_count >= self.missed_heartbeat_threshold {
                        failed_nodes.push(node_id.clone());
                    }
                }
            }
        }
        
        failed_nodes
    }
}

impl FaultDetector for HeartbeatDetector {
    fn detect_fault(&self, service: &ServiceStatus) -> Option<FaultType> {
        let now = SystemTime::now();
        if let Ok(elapsed) = now.duration_since(service.last_health_check) {
            if elapsed > self.timeout_threshold {
                return Some(FaultType::ServiceUnresponsive);
            }
        }
        None
    }
    
    fn get_confidence(&self) -> f64 {
        0.9 // 心跳检测有很高的置信度
    }
}

// 性能监控检测器
pub struct PerformanceDetector {
    latency_threshold: Duration,
    error_rate_threshold: f64,
    cpu_threshold: f64,
    memory_threshold: f64,
    performance_history: HashMap<String, VecDeque<PerformanceMetric>>,
}

#[derive(Debug, Clone)]
pub struct PerformanceMetric {
    pub timestamp: SystemTime,
    pub latency: Duration,
    pub error_rate: f64,
    pub cpu_usage: f64,
    pub memory_usage: f64,
    pub request_count: u64,
}

impl PerformanceDetector {
    pub fn new() -> Self {
        Self {
            latency_threshold: Duration::from_millis(5000),
            error_rate_threshold: 0.05, // 5%
            cpu_threshold: 0.8,         // 80%
            memory_threshold: 0.9,      // 90%
            performance_history: HashMap::new(),
        }
    }
    
    pub fn record_metric(&mut self, service_name: &str, metric: PerformanceMetric) {
        let history = self.performance_history.entry(service_name.to_string())
            .or_insert_with(VecDeque::new);
        
        history.push_back(metric);
        
        // 保持最近100个记录
        if history.len() > 100 {
            history.pop_front();
        }
    }
    
    pub fn analyze_trends(&self, service_name: &str) -> Option<TrendAnalysis> {
        let history = self.performance_history.get(service_name)?;
        
        if history.len() < 10 {
            return None; // 数据不足
        }
        
        let recent = &history.iter().rev().take(10).collect::<Vec<_>>();
        let baseline = &history.iter().take(history.len() - 10).collect::<Vec<_>>();
        
        let recent_avg_latency = recent.iter()
            .map(|m| m.latency.as_millis() as f64)
            .sum::<f64>() / recent.len() as f64;
        
        let baseline_avg_latency = baseline.iter()
            .map(|m| m.latency.as_millis() as f64)
            .sum::<f64>() / baseline.len() as f64;
        
        let recent_avg_error_rate = recent.iter()
            .map(|m| m.error_rate)
            .sum::<f64>() / recent.len() as f64;
        
        let baseline_avg_error_rate = baseline.iter()
            .map(|m| m.error_rate)
            .sum::<f64>() / baseline.len() as f64;
        
        Some(TrendAnalysis {
            latency_trend: (recent_avg_latency - baseline_avg_latency) / baseline_avg_latency,
            error_rate_trend: (recent_avg_error_rate - baseline_avg_error_rate) / baseline_avg_error_rate.max(0.001),
            is_degrading: recent_avg_latency > baseline_avg_latency * 1.5 || 
                          recent_avg_error_rate > baseline_avg_error_rate * 2.0,
        })
    }
}

#[derive(Debug, Clone)]
pub struct TrendAnalysis {
    pub latency_trend: f64,     // 百分比变化
    pub error_rate_trend: f64,  // 百分比变化
    pub is_degrading: bool,
}

impl FaultDetector for PerformanceDetector {
    fn detect_fault(&self, service: &ServiceStatus) -> Option<FaultType> {
        if service.response_time > self.latency_threshold {
            return Some(FaultType::HighLatency);
        }
        
        if service.error_rate > self.error_rate_threshold {
            return Some(FaultType::HighErrorRate);
        }
        
        // 基于趋势分析检测性能退化
        if let Some(trend) = self.analyze_trends(&service.service_name) {
            if trend.is_degrading {
                return Some(FaultType::ResourceExhaustion);
            }
        }
        
        None
    }
    
    fn get_confidence(&self) -> f64 {
        0.8 // 性能检测有较高置信度
    }
}

// 依赖健康检测器
pub struct DependencyHealthDetector {
    dependencies: HashMap<String, Vec<DependencyCheck>>,
    check_interval: Duration,
}

#[derive(Debug, Clone)]
pub struct DependencyCheck {
    pub name: String,
    pub endpoint: String,
    pub timeout: Duration,
    pub last_check: Option<SystemTime>,
    pub is_healthy: bool,
    pub consecutive_failures: u32,
}

impl DependencyHealthDetector {
    pub fn new(check_interval: Duration) -> Self {
        Self {
            dependencies: HashMap::new(),
            check_interval,
        }
    }
    
    pub fn add_dependency(&mut self, service_name: &str, dependency: DependencyCheck) {
        self.dependencies.entry(service_name.to_string())
            .or_insert_with(Vec::new)
            .push(dependency);
    }
    
    pub async fn check_dependencies(&mut self, service_name: &str) -> Vec<String> {
        let mut failed_dependencies = Vec::new();
        
        if let Some(deps) = self.dependencies.get_mut(service_name) {
            for dep in deps {
                if let Some(last_check) = dep.last_check {
                    if SystemTime::now().duration_since(last_check).unwrap_or(Duration::from_secs(0)) < self.check_interval {
                        continue; // 还没到检查时间
                    }
                }
                
                let is_healthy = self.perform_health_check(dep).await;
                dep.last_check = Some(SystemTime::now());
                
                if !is_healthy {
                    dep.consecutive_failures += 1;
                    dep.is_healthy = false;
                    
                    if dep.consecutive_failures >= 3 {
                        failed_dependencies.push(dep.name.clone());
                    }
                } else {
                    dep.consecutive_failures = 0;
                    dep.is_healthy = true;
                }
            }
        }
        
        failed_dependencies
    }
    
    async fn perform_health_check(&self, dependency: &DependencyCheck) -> bool {
        // 简化的健康检查实现
        // 实际应该发送HTTP请求或执行其他检查
        match timeout(dependency.timeout, self.simulate_health_check(&dependency.endpoint)).await {
            Ok(Ok(healthy)) => healthy,
            _ => false, // 超时或错误
        }
    }
    
    async fn simulate_health_check(&self, endpoint: &str) -> Result<bool, &'static str> {
        // 模拟健康检查
        tokio::time::sleep(Duration::from_millis(100)).await;
        
        // 简化的模拟逻辑
        Ok(!endpoint.contains("unhealthy"))
    }
}

impl FaultDetector for DependencyHealthDetector {
    fn detect_fault(&self, service: &ServiceStatus) -> Option<FaultType> {
        if let Some(deps) = self.dependencies.get(&service.service_name) {
            for dep in deps {
                if !dep.is_healthy && dep.consecutive_failures >= 3 {
                    return Some(FaultType::DependencyFailure);
                }
            }
        }
        None
    }
    
    fn get_confidence(&self) -> f64 {
        0.85 // 依赖检测有较高置信度
    }
}

// 异常模式检测器 (基于机器学习)
pub struct AnomalyPatternDetector {
    normal_patterns: HashMap<String, Vec<f64>>,
    anomaly_threshold: f64,
    window_size: usize,
    feature_weights: Vec<f64>,
}

impl AnomalyPatternDetector {
    pub fn new() -> Self {
        Self {
            normal_patterns: HashMap::new(),
            anomaly_threshold: 2.5, // 标准差阈值
            window_size: 50,
            feature_weights: vec![0.3, 0.3, 0.2, 0.2], // latency, error_rate, cpu, memory
        }
    }
    
    pub fn train_baseline(&mut self, service_name: &str, training_data: &[PerformanceMetric]) {
        let features = self.extract_features(training_data);
        self.normal_patterns.insert(service_name.to_string(), features);
    }
    
    pub fn detect_anomaly(&self, service_name: &str, current_metrics: &[PerformanceMetric]) -> Option<f64> {
        let normal_pattern = self.normal_patterns.get(service_name)?;
        let current_features = self.extract_features(current_metrics);
        
        if current_features.len() != normal_pattern.len() {
            return None;
        }
        
        // 计算马氏距离
        let distance = self.calculate_mahalanobis_distance(&current_features, normal_pattern);
        
        if distance > self.anomaly_threshold {
            Some(distance)
        } else {
            None
        }
    }
    
    fn extract_features(&self, metrics: &[PerformanceMetric]) -> Vec<f64> {
        if metrics.is_empty() {
            return Vec::new();
        }
        
        let avg_latency = metrics.iter()
            .map(|m| m.latency.as_millis() as f64)
            .sum::<f64>() / metrics.len() as f64;
        
        let avg_error_rate = metrics.iter()
            .map(|m| m.error_rate)
            .sum::<f64>() / metrics.len() as f64;
        
        let avg_cpu = metrics.iter()
            .map(|m| m.cpu_usage)
            .sum::<f64>() / metrics.len() as f64;
        
        let avg_memory = metrics.iter()
            .map(|m| m.memory_usage)
            .sum::<f64>() / metrics.len() as f64;
        
        vec![avg_latency, avg_error_rate, avg_cpu, avg_memory]
    }
    
    fn calculate_mahalanobis_distance(&self, current: &[f64], baseline: &[f64]) -> f64 {
        // 简化的马氏距离计算
        let mut distance = 0.0;
        
        for (i, (&curr, &base)) in current.iter().zip(baseline.iter()).enumerate() {
            let normalized_diff = if base > 0.0 { (curr - base) / base } else { curr - base };
            distance += self.feature_weights[i] * normalized_diff.powi(2);
        }
        
        distance.sqrt()
    }
}

impl FaultDetector for AnomalyPatternDetector {
    fn detect_fault(&self, service: &ServiceStatus) -> Option<FaultType> {
        // 需要历史数据来进行异常检测
        // 这里简化为基于当前状态的简单检测
        if service.error_rate > 0.1 && service.response_time > Duration::from_secs(10) {
            Some(FaultType::ResourceExhaustion)
        } else {
            None
        }
    }
    
    fn get_confidence(&self) -> f64 {
        0.7 // 异常检测置信度中等
    }
}

// 组合故障检测器
pub struct CompositeDetector {
    detectors: Vec<Box<dyn FaultDetector + Send + Sync>>,
    voting_threshold: f64, // 投票阈值
}

impl CompositeDetector {
    pub fn new(voting_threshold: f64) -> Self {
        Self {
            detectors: Vec::new(),
            voting_threshold,
        }
    }
    
    pub fn add_detector(&mut self, detector: Box<dyn FaultDetector + Send + Sync>) {
        self.detectors.push(detector);
    }
}

impl FaultDetector for CompositeDetector {
    fn detect_fault(&self, service: &ServiceStatus) -> Option<FaultType> {
        let mut fault_votes: HashMap<FaultType, f64> = HashMap::new();
        
        for detector in &self.detectors {
            if let Some(fault_type) = detector.detect_fault(service) {
                let confidence = detector.get_confidence();
                *fault_votes.entry(fault_type).or_insert(0.0) += confidence;
            }
        }
        
        // 找到票数最高的故障类型
        fault_votes.into_iter()
            .filter(|(_, votes)| *votes >= self.voting_threshold)
            .max_by(|a, b| a.1.partial_cmp(&b.1).unwrap())
            .map(|(fault_type, _)| fault_type)
    }
    
    fn get_confidence(&self) -> f64 {
        0.95 // 组合检测器有最高置信度
    }
}
```

## 6.1.2.2 自动故障恢复 / Automatic Fault Recovery

### 6.1.2.2.1 故障恢复策略 / Fault Recovery Strategies

**自适应恢复机制实现：**

```rust
use std::sync::Arc;
use tokio::sync::Mutex;
use std::collections::HashMap;

// 重启恢复策略
pub struct RestartRecoveryStrategy {
    max_restart_attempts: u32,
    restart_delay: Duration,
    restart_counts: Arc<Mutex<HashMap<String, u32>>>,
}

impl RestartRecoveryStrategy {
    pub fn new(max_attempts: u32, delay: Duration) -> Self {
        Self {
            max_restart_attempts: max_attempts,
            restart_delay: delay,
            restart_counts: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    async fn restart_service(&self, service_name: &str) -> Result<(), RecoveryError> {
        let mut counts = self.restart_counts.lock().await;
        let count = counts.entry(service_name.to_string()).or_insert(0);
        
        if *count >= self.max_restart_attempts {
            return Err(RecoveryError::RecoveryTimeout);
        }
        
        *count += 1;
        
        // 实际重启逻辑
        println!("Restarting service: {} (attempt {})", service_name, count);
        tokio::time::sleep(self.restart_delay).await;
        
        // 模拟重启成功
        if *count <= 2 { // 前两次重启成功
            *count = 0; // 重置计数
            Ok(())
        } else {
            Err(RecoveryError::RecoveryTimeout)
        }
    }
}

impl RecoveryStrategy for RestartRecoveryStrategy {
    fn recover(&self, service_name: &str, fault_type: &FaultType) -> Result<(), RecoveryError> {
        match fault_type {
            FaultType::ServiceUnresponsive | FaultType::HighErrorRate => {
                // 异步重启 (简化为同步)
                println!("Scheduling restart for service: {}", service_name);
                Ok(())
            }
            _ => Err(RecoveryError::ConfigurationError),
        }
    }
    
    fn get_recovery_time_estimate(&self, _fault_type: &FaultType) -> Duration {
        self.restart_delay + Duration::from_secs(30) // 重启时间 + 启动时间
    }
}

// 故障转移恢复策略
pub struct FailoverRecoveryStrategy {
    backup_instances: HashMap<String, Vec<String>>, // service -> backup_nodes
    load_balancer: Arc<LoadBalancer>,
}

impl FailoverRecoveryStrategy {
    pub fn new(load_balancer: Arc<LoadBalancer>) -> Self {
        Self {
            backup_instances: HashMap::new(),
            load_balancer,
        }
    }
    
    pub fn add_backup_instance(&mut self, service_name: String, backup_node: String) {
        self.backup_instances.entry(service_name)
            .or_insert_with(Vec::new)
            .push(backup_node);
    }
    
    async fn failover_to_backup(&self, service_name: &str) -> Result<(), RecoveryError> {
        if let Some(backups) = self.backup_instances.get(service_name) {
            for backup in backups {
                if self.load_balancer.is_node_healthy(backup).await {
                    self.load_balancer.redirect_traffic(service_name, backup).await?;
                    return Ok(());
                }
            }
            Err(RecoveryError::InsufficientResources)
        } else {
            Err(RecoveryError::ConfigurationError)
        }
    }
}

impl RecoveryStrategy for FailoverRecoveryStrategy {
    fn recover(&self, service_name: &str, fault_type: &FaultType) -> Result<(), RecoveryError> {
        match fault_type {
            FaultType::ServiceUnresponsive | FaultType::HardwareFailure => {
                println!("Scheduling failover for service: {}", service_name);
                Ok(())
            }
            _ => Err(RecoveryError::ConfigurationError),
        }
    }
    
    fn get_recovery_time_estimate(&self, _fault_type: &FaultType) -> Duration {
        Duration::from_secs(10) // 故障转移通常很快
    }
}

// 扩容恢复策略
pub struct ScaleUpRecoveryStrategy {
    auto_scaler: Arc<AutoScaler>,
    max_instances: u32,
}

impl ScaleUpRecoveryStrategy {
    pub fn new(auto_scaler: Arc<AutoScaler>, max_instances: u32) -> Self {
        Self {
            auto_scaler,
            max_instances,
        }
    }
}

impl RecoveryStrategy for ScaleUpRecoveryStrategy {
    fn recover(&self, service_name: &str, fault_type: &FaultType) -> Result<(), RecoveryError> {
        match fault_type {
            FaultType::HighLatency | FaultType::ResourceExhaustion => {
                println!("Scheduling scale-up for service: {}", service_name);
                Ok(())
            }
            _ => Err(RecoveryError::ConfigurationError),
        }
    }
    
    fn get_recovery_time_estimate(&self, _fault_type: &FaultType) -> Duration {
        Duration::from_secs(120) // 启动新实例需要时间
    }
}

// 电路熔断恢复策略
pub struct CircuitBreakerRecoveryStrategy {
    circuit_breakers: Arc<Mutex<HashMap<String, CircuitBreaker>>>,
}

#[derive(Debug, Clone)]
pub enum CircuitState {
    Closed,   // 正常状态
    Open,     // 熔断状态
    HalfOpen, // 半开状态
}

#[derive(Debug, Clone)]
pub struct CircuitBreaker {
    pub state: CircuitState,
    pub failure_count: u32,
    pub failure_threshold: u32,
    pub timeout: Duration,
    pub last_failure_time: Option<SystemTime>,
    pub half_open_max_calls: u32,
    pub half_open_success_count: u32,
}

impl CircuitBreaker {
    pub fn new(failure_threshold: u32, timeout: Duration) -> Self {
        Self {
            state: CircuitState::Closed,
            failure_count: 0,
            failure_threshold,
            timeout,
            last_failure_time: None,
            half_open_max_calls: 3,
            half_open_success_count: 0,
        }
    }
    
    pub fn record_success(&mut self) {
        match self.state {
            CircuitState::Closed => {
                self.failure_count = 0;
            }
            CircuitState::HalfOpen => {
                self.half_open_success_count += 1;
                if self.half_open_success_count >= self.half_open_max_calls {
                    self.state = CircuitState::Closed;
                    self.failure_count = 0;
                    self.half_open_success_count = 0;
                }
            }
            CircuitState::Open => {} // 不应该在Open状态记录成功
        }
    }
    
    pub fn record_failure(&mut self) {
        self.failure_count += 1;
        self.last_failure_time = Some(SystemTime::now());
        
        match self.state {
            CircuitState::Closed => {
                if self.failure_count >= self.failure_threshold {
                    self.state = CircuitState::Open;
                }
            }
            CircuitState::HalfOpen => {
                self.state = CircuitState::Open;
                self.half_open_success_count = 0;
            }
            CircuitState::Open => {} // 已经是Open状态
        }
    }
    
    pub fn can_execute(&mut self) -> bool {
        match self.state {
            CircuitState::Closed => true,
            CircuitState::Open => {
                if let Some(last_failure) = self.last_failure_time {
                    if SystemTime::now().duration_since(last_failure).unwrap_or(Duration::from_secs(0)) >= self.timeout {
                        self.state = CircuitState::HalfOpen;
                        self.half_open_success_count = 0;
                        true
                    } else {
                        false
                    }
                } else {
                    false
                }
            }
            CircuitState::HalfOpen => true,
        }
    }
}

impl CircuitBreakerRecoveryStrategy {
    pub fn new() -> Self {
        Self {
            circuit_breakers: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    pub async fn add_circuit_breaker(&self, service_name: String, circuit_breaker: CircuitBreaker) {
        let mut breakers = self.circuit_breakers.lock().await;
        breakers.insert(service_name, circuit_breaker);
    }
    
    pub async fn trip_circuit(&self, service_name: &str) -> Result<(), RecoveryError> {
        let mut breakers = self.circuit_breakers.lock().await;
        if let Some(breaker) = breakers.get_mut(service_name) {
            breaker.record_failure();
            println!("Circuit breaker tripped for service: {}", service_name);
            Ok(())
        } else {
            Err(RecoveryError::ConfigurationError)
        }
    }
}

impl RecoveryStrategy for CircuitBreakerRecoveryStrategy {
    fn recover(&self, service_name: &str, fault_type: &FaultType) -> Result<(), RecoveryError> {
        match fault_type {
            FaultType::HighErrorRate | FaultType::DependencyFailure => {
                println!("Activating circuit breaker for service: {}", service_name);
                Ok(())
            }
            _ => Err(RecoveryError::ConfigurationError),
        }
    }
    
    fn get_recovery_time_estimate(&self, _fault_type: &FaultType) -> Duration {
        Duration::from_secs(1) // 电路熔断几乎是即时的
    }
}

// 自适应恢复协调器
pub struct AdaptiveRecoveryCoordinator {
    recovery_strategies: HashMap<String, Vec<Box<dyn RecoveryStrategy + Send + Sync>>>,
    recovery_history: HashMap<String, Vec<RecoveryAttempt>>,
    strategy_effectiveness: HashMap<String, f64>,
}

#[derive(Debug, Clone)]
pub struct RecoveryAttempt {
    pub strategy_name: String,
    pub fault_type: FaultType,
    pub start_time: SystemTime,
    pub end_time: Option<SystemTime>,
    pub success: bool,
    pub recovery_time: Option<Duration>,
}

impl AdaptiveRecoveryCoordinator {
    pub fn new() -> Self {
        Self {
            recovery_strategies: HashMap::new(),
            recovery_history: HashMap::new(),
            strategy_effectiveness: HashMap::new(),
        }
    }
    
    pub fn register_strategy(&mut self, 
                           service_name: String, 
                           strategy_name: String,
                           strategy: Box<dyn RecoveryStrategy + Send + Sync>) {
        self.recovery_strategies.entry(service_name)
            .or_insert_with(Vec::new)
            .push(strategy);
        
        self.strategy_effectiveness.insert(strategy_name, 0.5); // 初始效果50%
    }
    
    pub async fn coordinate_recovery(&mut self, 
                                   service_name: &str, 
                                   fault_type: &FaultType) -> Result<(), RecoveryError> {
        // 选择最适合的恢复策略
        let strategy_name = self.select_best_strategy(service_name, fault_type)?;
        
        // 记录恢复尝试开始
        let attempt = RecoveryAttempt {
            strategy_name: strategy_name.clone(),
            fault_type: fault_type.clone(),
            start_time: SystemTime::now(),
            end_time: None,
            success: false,
            recovery_time: None,
        };
        
        self.recovery_history.entry(service_name.to_string())
            .or_insert_with(Vec::new)
            .push(attempt.clone());
        
        // 执行恢复策略
        let result = self.execute_recovery_strategy(service_name, &strategy_name, fault_type).await;
        
        // 更新恢复记录和策略效果
        self.update_recovery_result(service_name, &strategy_name, result.is_ok()).await;
        
        result
    }
    
    fn select_best_strategy(&self, service_name: &str, fault_type: &FaultType) -> Result<String, RecoveryError> {
        // 基于历史效果选择策略
        let mut best_strategy = None;
        let mut best_effectiveness = 0.0;
        
        if let Some(strategies) = self.recovery_strategies.get(service_name) {
            for (i, _) in strategies.iter().enumerate() {
                let strategy_name = format!("strategy_{}", i);
                let effectiveness = self.strategy_effectiveness.get(&strategy_name).unwrap_or(&0.5);
                
                if *effectiveness > best_effectiveness {
                    best_effectiveness = *effectiveness;
                    best_strategy = Some(strategy_name);
                }
            }
        }
        
        best_strategy.ok_or(RecoveryError::ConfigurationError)
    }
    
    async fn execute_recovery_strategy(&self, 
                                     service_name: &str, 
                                     strategy_name: &str, 
                                     fault_type: &FaultType) -> Result<(), RecoveryError> {
        if let Some(strategies) = self.recovery_strategies.get(service_name) {
            // 简化：根据策略名称选择策略
            let strategy_index = strategy_name.chars()
                .filter(|c| c.is_ascii_digit())
                .collect::<String>()
                .parse::<usize>()
                .unwrap_or(0);
            
            if let Some(strategy) = strategies.get(strategy_index) {
                strategy.recover(service_name, fault_type)
            } else {
                Err(RecoveryError::ConfigurationError)
            }
        } else {
            Err(RecoveryError::ConfigurationError)
        }
    }
    
    async fn update_recovery_result(&mut self, 
                                  service_name: &str, 
                                  strategy_name: &str, 
                                  success: bool) {
        // 更新策略效果
        let effectiveness = self.strategy_effectiveness.entry(strategy_name.to_string()).or_insert(0.5);
        
        if success {
            *effectiveness = (*effectiveness * 0.9 + 1.0 * 0.1).min(1.0); // 加权平均，成功提高效果
        } else {
            *effectiveness = (*effectiveness * 0.9 + 0.0 * 0.1).max(0.1); // 失败降低效果
        }
        
        // 更新恢复历史
        if let Some(history) = self.recovery_history.get_mut(service_name) {
            if let Some(last_attempt) = history.last_mut() {
                last_attempt.end_time = Some(SystemTime::now());
                last_attempt.success = success;
                last_attempt.recovery_time = last_attempt.end_time.and_then(|end| 
                    end.duration_since(last_attempt.start_time).ok());
            }
        }
    }
    
    pub fn get_strategy_effectiveness_report(&self) -> String {
        let mut report = String::new();
        report.push_str("# Recovery Strategy Effectiveness Report\n\n");
        
        report.push_str("| Strategy | Effectiveness | Total Attempts | Success Rate |\n");
        report.push_str("|----------|---------------|----------------|---------------|\n");
        
        for (strategy_name, effectiveness) in &self.strategy_effectiveness {
            let total_attempts = self.recovery_history.values()
                .flat_map(|history| history.iter())
                .filter(|attempt| attempt.strategy_name == *strategy_name)
                .count();
            
            let successful_attempts = self.recovery_history.values()
                .flat_map(|history| history.iter())
                .filter(|attempt| attempt.strategy_name == *strategy_name && attempt.success)
                .count();
            
            let success_rate = if total_attempts > 0 {
                (successful_attempts as f64 / total_attempts as f64) * 100.0
            } else {
                0.0
            };
            
            report.push_str(&format!(
                "| {} | {:.2} | {} | {:.1}% |\n",
                strategy_name, effectiveness, total_attempts, success_rate
            ));
        }
        
        report
    }
}

// 负载均衡器和自动扩容器的简化实现
pub struct LoadBalancer {
    healthy_nodes: Arc<Mutex<HashMap<String, Vec<String>>>>,
}

impl LoadBalancer {
    pub fn new() -> Self {
        Self {
            healthy_nodes: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    pub async fn is_node_healthy(&self, node_id: &str) -> bool {
        let nodes = self.healthy_nodes.lock().await;
        nodes.values().any(|service_nodes| service_nodes.contains(&node_id.to_string()))
    }
    
    pub async fn redirect_traffic(&self, service_name: &str, target_node: &str) -> Result<(), RecoveryError> {
        println!("Redirecting traffic for {} to {}", service_name, target_node);
        Ok(())
    }
}

pub struct AutoScaler {
    current_instances: Arc<Mutex<HashMap<String, u32>>>,
}

impl AutoScaler {
    pub fn new() -> Self {
        Self {
            current_instances: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    pub async fn scale_up(&self, service_name: &str, target_instances: u32) -> Result<(), RecoveryError> {
        let mut instances = self.current_instances.lock().await;
        let current = instances.entry(service_name.to_string()).or_insert(1);
        
        if target_instances > *current {
            *current = target_instances;
            println!("Scaling up {} to {} instances", service_name, target_instances);
            Ok(())
        } else {
            Err(RecoveryError::ConfigurationError)
        }
    }
}
```

## 6.1.2.3 负载均衡与流量管理 / Load Balancing and Traffic Management

### 6.1.2.3.1 智能负载均衡器 / Intelligent Load Balancer

**自适应负载均衡实现：**

```rust
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use std::time::{Duration, SystemTime, UNIX_EPOCH};

#[derive(Debug, Clone)]
pub enum LoadBalancingAlgorithm {
    RoundRobin,
    WeightedRoundRobin,
    LeastConnections,
    WeightedLeastConnections,
    ResourceBased,
    ResponseTimeBased,
    ConsistentHashing,
    PowerOfTwoChoices,
}

#[derive(Debug, Clone)]
pub struct BackendNode {
    pub node_id: String,
    pub address: String,
    pub port: u16,
    pub weight: u32,
    pub current_connections: u32,
    pub max_connections: u32,
    pub response_time: Duration,
    pub error_rate: f64,
    pub cpu_usage: f64,
    pub memory_usage: f64,
    pub is_healthy: bool,
    pub last_health_check: SystemTime,
}

#[derive(Debug, Clone)]
pub struct LoadBalancingPolicy {
    pub algorithm: LoadBalancingAlgorithm,
    pub health_check_interval: Duration,
    pub failure_threshold: u32,
    pub recovery_threshold: u32,
    pub max_retries: u32,
    pub circuit_breaker_enabled: bool,
    pub session_affinity: bool,
}

#[derive(Debug)]
pub struct IntelligentLoadBalancer {
    nodes: Arc<RwLock<HashMap<String, BackendNode>>>,
    policies: HashMap<String, LoadBalancingPolicy>,
    request_history: Arc<RwLock<VecDeque<RequestRecord>>>,
    session_table: Arc<RwLock<HashMap<String, String>>>, // session_id -> node_id
    consistent_hash_ring: Arc<RwLock<ConsistentHashRing>>,
    round_robin_counters: Arc<RwLock<HashMap<String, usize>>>,
}

#[derive(Debug, Clone)]
pub struct RequestRecord {
    pub timestamp: SystemTime,
    pub service_name: String,
    pub node_id: String,
    pub response_time: Duration,
    pub success: bool,
    pub retry_count: u32,
}

#[derive(Debug, Clone)]
pub struct ConsistentHashRing {
    ring: std::collections::BTreeMap<u64, String>,
    virtual_nodes: u32,
}

impl ConsistentHashRing {
    pub fn new(virtual_nodes: u32) -> Self {
        Self {
            ring: std::collections::BTreeMap::new(),
            virtual_nodes,
        }
    }
    
    pub fn add_node(&mut self, node_id: &str) {
        for i in 0..self.virtual_nodes {
            let key = self.hash(&format!("{}:{}", node_id, i));
            self.ring.insert(key, node_id.to_string());
        }
    }
    
    pub fn remove_node(&mut self, node_id: &str) {
        for i in 0..self.virtual_nodes {
            let key = self.hash(&format!("{}:{}", node_id, i));
            self.ring.remove(&key);
        }
    }
    
    pub fn get_node(&self, key: &str) -> Option<String> {
        if self.ring.is_empty() {
            return None;
        }
        
        let hash_key = self.hash(key);
        
        // 找到第一个大于等于hash_key的节点
        if let Some((_, node_id)) = self.ring.range(hash_key..).next() {
            Some(node_id.clone())
        } else {
            // 如果没有找到，返回环上的第一个节点
            self.ring.values().next().cloned()
        }
    }
    
    fn hash(&self, input: &str) -> u64 {
        // 简化的哈希函数 (实际应使用SHA1或其他哈希算法)
        let mut hash = 0u64;
        for byte in input.bytes() {
            hash = hash.wrapping_mul(31).wrapping_add(byte as u64);
        }
        hash
    }
}

impl IntelligentLoadBalancer {
    pub fn new() -> Self {
        Self {
            nodes: Arc::new(RwLock::new(HashMap::new())),
            policies: HashMap::new(),
            request_history: Arc::new(RwLock::new(VecDeque::new())),
            session_table: Arc::new(RwLock::new(HashMap::new())),
            consistent_hash_ring: Arc::new(RwLock::new(ConsistentHashRing::new(150))),
            round_robin_counters: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    pub async fn add_backend_node(&self, node: BackendNode) {
        let mut nodes = self.nodes.write().await;
        let mut hash_ring = self.consistent_hash_ring.write().await;
        
        hash_ring.add_node(&node.node_id);
        nodes.insert(node.node_id.clone(), node);
    }
    
    pub async fn remove_backend_node(&self, node_id: &str) {
        let mut nodes = self.nodes.write().await;
        let mut hash_ring = self.consistent_hash_ring.write().await;
        
        hash_ring.remove_node(node_id);
        nodes.remove(node_id);
    }
    
    pub fn set_policy(&mut self, service_name: String, policy: LoadBalancingPolicy) {
        self.policies.insert(service_name, policy);
    }
    
    pub async fn select_backend(&self, 
                               service_name: &str, 
                               session_id: Option<&str>, 
                               request_key: Option<&str>) -> Option<String> {
        let policy = self.policies.get(service_name)?;
        let nodes = self.nodes.read().await;
        
        // 过滤健康的节点
        let healthy_nodes: Vec<_> = nodes.values()
            .filter(|node| node.is_healthy)
            .collect();
        
        if healthy_nodes.is_empty() {
            return None;
        }
        
        // 会话亲和性检查
        if policy.session_affinity {
            if let Some(session) = session_id {
                let session_table = self.session_table.read().await;
                if let Some(node_id) = session_table.get(session) {
                    if nodes.get(node_id).map_or(false, |n| n.is_healthy) {
                        return Some(node_id.clone());
                    }
                }
            }
        }
        
        // 根据算法选择节点
        let selected_node = match policy.algorithm {
            LoadBalancingAlgorithm::RoundRobin => {
                self.round_robin_selection(service_name, &healthy_nodes).await
            }
            LoadBalancingAlgorithm::WeightedRoundRobin => {
                self.weighted_round_robin_selection(&healthy_nodes).await
            }
            LoadBalancingAlgorithm::LeastConnections => {
                self.least_connections_selection(&healthy_nodes).await
            }
            LoadBalancingAlgorithm::WeightedLeastConnections => {
                self.weighted_least_connections_selection(&healthy_nodes).await
            }
            LoadBalancingAlgorithm::ResourceBased => {
                self.resource_based_selection(&healthy_nodes).await
            }
            LoadBalancingAlgorithm::ResponseTimeBased => {
                self.response_time_based_selection(&healthy_nodes).await
            }
            LoadBalancingAlgorithm::ConsistentHashing => {
                self.consistent_hash_selection(request_key.unwrap_or("")).await
            }
            LoadBalancingAlgorithm::PowerOfTwoChoices => {
                self.power_of_two_choices_selection(&healthy_nodes).await
            }
        };
        
        // 更新会话表
        if let (Some(session), Some(node_id)) = (session_id, &selected_node) {
            if policy.session_affinity {
                let mut session_table = self.session_table.write().await;
                session_table.insert(session.to_string(), node_id.clone());
            }
        }
        
        selected_node
    }
    
    async fn round_robin_selection(&self, service_name: &str, nodes: &[&BackendNode]) -> Option<String> {
        let mut counters = self.round_robin_counters.write().await;
        let counter = counters.entry(service_name.to_string()).or_insert(0);
        
        let selected = nodes.get(*counter % nodes.len())?;
        *counter += 1;
        
        Some(selected.node_id.clone())
    }
    
    async fn weighted_round_robin_selection(&self, nodes: &[&BackendNode]) -> Option<String> {
        // 根据权重构建选择池
        let mut weighted_pool = Vec::new();
        for node in nodes {
            for _ in 0..node.weight {
                weighted_pool.push(node);
            }
        }
        
        if weighted_pool.is_empty() {
            return None;
        }
        
        // 简化的随机选择
        let now = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_nanos() as usize;
        let selected = weighted_pool[now % weighted_pool.len()];
        Some(selected.node_id.clone())
    }
    
    async fn least_connections_selection(&self, nodes: &[&BackendNode]) -> Option<String> {
        nodes.iter()
            .min_by_key(|node| node.current_connections)
            .map(|node| node.node_id.clone())
    }
    
    async fn weighted_least_connections_selection(&self, nodes: &[&BackendNode]) -> Option<String> {
        nodes.iter()
            .min_by(|a, b| {
                let a_ratio = a.current_connections as f64 / a.weight as f64;
                let b_ratio = b.current_connections as f64 / b.weight as f64;
                a_ratio.partial_cmp(&b_ratio).unwrap_or(std::cmp::Ordering::Equal)
            })
            .map(|node| node.node_id.clone())
    }
    
    async fn resource_based_selection(&self, nodes: &[&BackendNode]) -> Option<String> {
        // 基于CPU和内存使用率选择负载最低的节点
        nodes.iter()
            .min_by(|a, b| {
                let a_load = a.cpu_usage * 0.7 + a.memory_usage * 0.3;
                let b_load = b.cpu_usage * 0.7 + b.memory_usage * 0.3;
                a_load.partial_cmp(&b_load).unwrap_or(std::cmp::Ordering::Equal)
            })
            .map(|node| node.node_id.clone())
    }
    
    async fn response_time_based_selection(&self, nodes: &[&BackendNode]) -> Option<String> {
        nodes.iter()
            .min_by_key(|node| node.response_time)
            .map(|node| node.node_id.clone())
    }
    
    async fn consistent_hash_selection(&self, request_key: &str) -> Option<String> {
        let hash_ring = self.consistent_hash_ring.read().await;
        hash_ring.get_node(request_key)
    }
    
    async fn power_of_two_choices_selection(&self, nodes: &[&BackendNode]) -> Option<String> {
        if nodes.len() < 2 {
            return nodes.first().map(|n| n.node_id.clone());
        }
        
        // 随机选择两个节点
        let now = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_nanos() as usize;
        let idx1 = now % nodes.len();
        let idx2 = (now / nodes.len()) % nodes.len();
        
        let node1 = nodes[idx1];
        let node2 = nodes[idx2];
        
        // 选择连接数较少的节点
        if node1.current_connections <= node2.current_connections {
            Some(node1.node_id.clone())
        } else {
            Some(node2.node_id.clone())
        }
    }
    
    pub async fn record_request(&self, record: RequestRecord) {
        let mut history = self.request_history.write().await;
        history.push_back(record);
        
        // 保持历史记录大小
        if history.len() > 10000 {
            history.pop_front();
        }
        
        // 更新节点统计
        self.update_node_statistics().await;
    }
    
    async fn update_node_statistics(&self) {
        let history = self.request_history.read().await;
        let mut nodes = self.nodes.write().await;
        
        // 计算最近时间窗口内的统计信息
        let window_start = SystemTime::now() - Duration::from_secs(300); // 5分钟窗口
        
        for (node_id, node) in nodes.iter_mut() {
            let node_requests: Vec<_> = history.iter()
                .filter(|r| r.node_id == *node_id && r.timestamp >= window_start)
                .collect();
            
            if !node_requests.is_empty() {
                // 计算平均响应时间
                let total_response_time: Duration = node_requests.iter()
                    .map(|r| r.response_time)
                    .sum();
                node.response_time = total_response_time / node_requests.len() as u32;
                
                // 计算错误率
                let error_count = node_requests.iter()
                    .filter(|r| !r.success)
                    .count();
                node.error_rate = error_count as f64 / node_requests.len() as f64;
            }
        }
    }
    
    pub async fn health_check(&self, service_name: &str) {
        if let Some(policy) = self.policies.get(service_name) {
            let mut nodes = self.nodes.write().await;
            
            for node in nodes.values_mut() {
                let elapsed = SystemTime::now()
                    .duration_since(node.last_health_check)
                    .unwrap_or(Duration::from_secs(0));
                
                if elapsed >= policy.health_check_interval {
                    let is_healthy = self.perform_health_check(node).await;
                    node.is_healthy = is_healthy;
                    node.last_health_check = SystemTime::now();
                    
                    if !is_healthy {
                        // 从一致性哈希环中移除
                        let mut hash_ring = self.consistent_hash_ring.write().await;
                        hash_ring.remove_node(&node.node_id);
                    } else {
                        // 添加回一致性哈希环
                        let mut hash_ring = self.consistent_hash_ring.write().await;
                        hash_ring.add_node(&node.node_id);
                    }
                }
            }
        }
    }
    
    async fn perform_health_check(&self, node: &BackendNode) -> bool {
        // 简化的健康检查实现
        // 实际应该发送HTTP请求或TCP连接测试
        
        // 模拟健康检查
        if node.error_rate > 0.5 || node.cpu_usage > 0.95 {
            false
        } else {
            // 模拟网络检查
            !node.address.contains("unreachable")
        }
    }
    
    pub async fn get_load_balancing_statistics(&self) -> LoadBalancingStatistics {
        let nodes = self.nodes.read().await;
        let history = self.request_history.read().await;
        
        let total_requests = history.len() as u64;
        let successful_requests = history.iter().filter(|r| r.success).count() as u64;
        let failed_requests = total_requests - successful_requests;
        
        let avg_response_time = if !history.is_empty() {
            history.iter()
                .map(|r| r.response_time.as_millis() as u64)
                .sum::<u64>() / history.len() as u64
        } else {
            0
        };
        
        let healthy_nodes = nodes.values().filter(|n| n.is_healthy).count() as u32;
        let total_nodes = nodes.len() as u32;
        
        LoadBalancingStatistics {
            total_requests,
            successful_requests,
            failed_requests,
            avg_response_time_ms: avg_response_time,
            healthy_nodes,
            total_nodes,
            success_rate: if total_requests > 0 { 
                successful_requests as f64 / total_requests as f64 
            } else { 
                0.0 
            },
        }
    }
}

#[derive(Debug, Clone)]
pub struct LoadBalancingStatistics {
    pub total_requests: u64,
    pub successful_requests: u64,
    pub failed_requests: u64,
    pub avg_response_time_ms: u64,
    pub healthy_nodes: u32,
    pub total_nodes: u32,
    pub success_rate: f64,
}
```

## 总结 / Summary

本文档全面介绍了集群高可用性的关键技术，包括：

1. **高可用性架构**：多层高可用设计和SLA要求定义
2. **故障检测机制**：心跳检测、性能监控、依赖健康检查等多层检测
3. **自动故障恢复**：重启、故障转移、扩容、熔断等恢复策略
4. **负载均衡与流量管理**：智能负载均衡和自适应流量分发

通过这些技术的综合应用，可以构建高度可靠的集群系统，确保服务的持续可用性和优异的性能表现。关键是要根据具体的业务需求和SLA要求，选择合适的策略组合，并持续监控和优化系统性能。
