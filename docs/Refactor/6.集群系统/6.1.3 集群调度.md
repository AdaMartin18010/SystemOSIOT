# 6.1.3 集群调度 / Cluster Scheduling

## 6.1.3.1 调度理论基础 / Scheduling Theory Fundamentals

### 6.1.3.1.1 调度算法分类 / Scheduling Algorithm Classification

**集群调度算法分类：**

```text
集群调度算法 (Cluster Scheduling Algorithms)
    ├── 作业级调度 (Job-Level Scheduling)
    │   ├── FIFO (先进先出)
    │   ├── 公平调度 (Fair Scheduling)
    │   ├── 容量调度 (Capacity Scheduling)
    │   └── 优先级调度 (Priority Scheduling)
    │
    ├── 任务级调度 (Task-Level Scheduling)
    │   ├── 数据本地性调度
    │   ├── 资源感知调度
    │   ├── 延迟调度 (Delay Scheduling)
    │   └── 推测执行调度
    │
    ├── 资源级调度 (Resource-Level Scheduling)
    │   ├── CPU 调度
    │   ├── 内存调度
    │   ├── 网络带宽调度
    │   └── 存储I/O调度
    │
    └── 多层级调度 (Multi-Level Scheduling)
        ├── 两级调度
        ├── 层次化调度
        ├── 混合调度策略
        └── 自适应调度
```

**集群调度器核心实现：**

```rust
use std::collections::{HashMap, VecDeque, BTreeMap};
use std::time::{Duration, SystemTime, UNIX_EPOCH};
use std::sync::{Arc, Mutex};
use tokio::sync::RwLock;
use serde::{Serialize, Deserialize};

// 调度器接口
pub trait ClusterScheduler: Send + Sync {
    fn schedule_job(&mut self, job: &SchedulingJob) -> Result<SchedulingDecision, SchedulingError>;
    fn update_cluster_state(&mut self, state: &ClusterState);
    fn get_scheduler_metrics(&self) -> SchedulerMetrics;
    fn configure(&mut self, config: SchedulerConfig);
}

// 调度决策
#[derive(Debug, Clone)]
pub struct SchedulingDecision {
    pub job_id: String,
    pub task_assignments: Vec<TaskAssignment>,
    pub resource_allocations: Vec<ResourceAllocation>,
    pub scheduling_delay: Duration,
    pub priority_boost: bool,
}

#[derive(Debug, Clone)]
pub struct TaskAssignment {
    pub task_id: String,
    pub node_id: String,
    pub container_id: String,
    pub start_time: SystemTime,
    pub estimated_duration: Duration,
    pub data_locality_score: f64,
}

#[derive(Debug, Clone)]
pub struct ResourceAllocation {
    pub resource_type: ResourceType,
    pub allocated_amount: f64,
    pub node_id: String,
    pub allocation_id: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ResourceType {
    CPU,
    Memory,
    Storage,
    NetworkBandwidth,
    GPU,
    CustomResource(String),
}

// 调度作业定义
#[derive(Debug, Clone)]
pub struct SchedulingJob {
    pub job_id: String,
    pub user_id: String,
    pub queue_name: String,
    pub job_type: JobType,
    pub priority: JobPriority,
    pub resource_requirements: JobResourceRequirements,
    pub constraints: Vec<SchedulingConstraint>,
    pub deadline: Option<SystemTime>,
    pub submission_time: SystemTime,
    pub estimated_runtime: Option<Duration>,
}

#[derive(Debug, Clone)]
pub enum JobType {
    Batch,
    Interactive,
    Streaming,
    Service,
    ML_Training,
    ML_Inference,
}

#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord)]
pub enum JobPriority {
    Critical = 1,
    High = 2,
    Normal = 3,
    Low = 4,
    Best_Effort = 5,
}

#[derive(Debug, Clone)]
pub struct JobResourceRequirements {
    pub cpu_cores: f64,
    pub memory_gb: f64,
    pub storage_gb: f64,
    pub network_bandwidth_mbps: f64,
    pub gpu_count: u32,
    pub custom_resources: HashMap<String, f64>,
    pub max_runtime: Option<Duration>,
}

#[derive(Debug, Clone)]
pub enum SchedulingConstraint {
    NodeAffinity(NodeSelector),
    PodAffinity(PodSelector),
    AntiAffinity(AntiAffinityRule),
    ResourceLimit(ResourceLimitConstraint),
    DataLocality(DataLocalityConstraint),
    Deadline(SystemTime),
}

// 集群状态
#[derive(Debug, Clone)]
pub struct ClusterState {
    pub nodes: HashMap<String, NodeState>,
    pub running_jobs: HashMap<String, RunningJob>,
    pub resource_utilization: ClusterResourceUtilization,
    pub queue_states: HashMap<String, QueueState>,
    pub timestamp: SystemTime,
}

#[derive(Debug, Clone)]
pub struct NodeState {
    pub node_id: String,
    pub total_resources: NodeResources,
    pub available_resources: NodeResources,
    pub allocated_resources: NodeResources,
    pub running_tasks: Vec<String>,
    pub node_labels: HashMap<String, String>,
    pub health_status: NodeHealthStatus,
    pub last_heartbeat: SystemTime,
}

#[derive(Debug, Clone)]
pub struct NodeResources {
    pub cpu_cores: f64,
    pub memory_gb: f64,
    pub storage_gb: f64,
    pub network_bandwidth_mbps: f64,
    pub gpu_count: u32,
    pub custom_resources: HashMap<String, f64>,
}

#[derive(Debug, Clone)]
pub enum NodeHealthStatus {
    Healthy,
    Warning,
    Critical,
    Unreachable,
    Draining,
}

// 公平调度器实现
#[derive(Debug)]
pub struct FairScheduler {
    queues: HashMap<String, FairQueue>,
    global_config: FairSchedulerConfig,
    scheduling_history: VecDeque<SchedulingEvent>,
    resource_calculator: ResourceCalculator,
    preemption_manager: PreemptionManager,
}

#[derive(Debug)]
pub struct FairQueue {
    pub queue_name: String,
    pub weight: f64,
    pub min_resources: NodeResources,
    pub max_resources: NodeResources,
    pub running_jobs: Vec<String>,
    pub pending_jobs: VecDeque<SchedulingJob>,
    pub fair_share: NodeResources,
    pub usage: NodeResources,
    pub scheduling_policy: QueueSchedulingPolicy,
}

#[derive(Debug, Clone)]
pub enum QueueSchedulingPolicy {
    FIFO,
    Fair,
    DRF, // Dominant Resource Fairness
}

#[derive(Debug)]
pub struct FairSchedulerConfig {
    pub preemption_enabled: bool,
    pub preemption_threshold: f64,
    pub locality_delay_node_local: Duration,
    pub locality_delay_rack_local: Duration,
    pub continuous_scheduling_enabled: bool,
    pub assign_multiple: bool,
    pub max_assign: u32,
}

impl FairScheduler {
    pub fn new(config: FairSchedulerConfig) -> Self {
        Self {
            queues: HashMap::new(),
            global_config: config,
            scheduling_history: VecDeque::new(),
            resource_calculator: ResourceCalculator::new(),
            preemption_manager: PreemptionManager::new(),
        }
    }
    
    pub fn add_queue(&mut self, queue: FairQueue) {
        self.queues.insert(queue.queue_name.clone(), queue);
    }
    
    fn calculate_fair_shares(&mut self, total_resources: &NodeResources) {
        let total_weight: f64 = self.queues.values().map(|q| q.weight).sum();
        
        for queue in self.queues.values_mut() {
            let share_ratio = queue.weight / total_weight;
            queue.fair_share = NodeResources {
                cpu_cores: total_resources.cpu_cores * share_ratio,
                memory_gb: total_resources.memory_gb * share_ratio,
                storage_gb: total_resources.storage_gb * share_ratio,
                network_bandwidth_mbps: total_resources.network_bandwidth_mbps * share_ratio,
                gpu_count: (total_resources.gpu_count as f64 * share_ratio) as u32,
                custom_resources: total_resources.custom_resources.iter()
                    .map(|(k, v)| (k.clone(), v * share_ratio))
                    .collect(),
            };
        }
    }
    
    fn select_queue_to_schedule(&self) -> Option<&str> {
        let mut most_starved_queue = None;
        let mut max_starvation = f64::MIN;
        
        for (queue_name, queue) in &self.queues {
            if queue.pending_jobs.is_empty() {
                continue;
            }
            
            let starvation = self.calculate_queue_starvation(queue);
            if starvation > max_starvation {
                max_starvation = starvation;
                most_starved_queue = Some(queue_name.as_str());
            }
        }
        
        most_starved_queue
    }
    
    fn calculate_queue_starvation(&self, queue: &FairQueue) -> f64 {
        // 使用Dominant Resource Fairness (DRF) 算法
        let cpu_usage_ratio = queue.usage.cpu_cores / queue.fair_share.cpu_cores.max(0.001);
        let memory_usage_ratio = queue.usage.memory_gb / queue.fair_share.memory_gb.max(0.001);
        let storage_usage_ratio = queue.usage.storage_gb / queue.fair_share.storage_gb.max(0.001);
        
        // 计算主导资源使用率
        let dominant_usage = cpu_usage_ratio.max(memory_usage_ratio).max(storage_usage_ratio);
        
        // 饥饿程度 = 1 - 主导资源使用率
        1.0 - dominant_usage
    }
    
    fn select_job_from_queue(&self, queue: &FairQueue) -> Option<&SchedulingJob> {
        match queue.scheduling_policy {
            QueueSchedulingPolicy::FIFO => queue.pending_jobs.front(),
            QueueSchedulingPolicy::Fair => self.select_fair_job_from_queue(queue),
            QueueSchedulingPolicy::DRF => self.select_drf_job_from_queue(queue),
        }
    }
    
    fn select_fair_job_from_queue(&self, queue: &FairQueue) -> Option<&SchedulingJob> {
        // 选择等待时间最长的作业
        queue.pending_jobs.iter()
            .min_by_key(|job| job.submission_time)
    }
    
    fn select_drf_job_from_queue(&self, queue: &FairQueue) -> Option<&SchedulingJob> {
        // 使用DRF算法选择作业
        queue.pending_jobs.iter()
            .min_by(|a, b| {
                let a_dominant = self.calculate_dominant_resource_demand(&a.resource_requirements);
                let b_dominant = self.calculate_dominant_resource_demand(&b.resource_requirements);
                a_dominant.partial_cmp(&b_dominant).unwrap()
            })
    }
    
    fn calculate_dominant_resource_demand(&self, requirements: &JobResourceRequirements) -> f64 {
        // 简化的主导资源计算
        let cpu_demand = requirements.cpu_cores / 100.0; // 假设集群总共100核
        let memory_demand = requirements.memory_gb / 1000.0; // 假设集群总共1000GB内存
        
        cpu_demand.max(memory_demand)
    }
    
    fn select_node_for_job(&self, job: &SchedulingJob, cluster_state: &ClusterState) -> Option<String> {
        let mut best_node = None;
        let mut best_score = f64::MIN;
        
        for (node_id, node_state) in &cluster_state.nodes {
            if node_state.health_status != NodeHealthStatus::Healthy {
                continue;
            }
            
            if !self.node_satisfies_constraints(node_state, job) {
                continue;
            }
            
            if !self.node_has_sufficient_resources(node_state, &job.resource_requirements) {
                continue;
            }
            
            let score = self.calculate_node_score(node_state, job);
            if score > best_score {
                best_score = score;
                best_node = Some(node_id.clone());
            }
        }
        
        best_node
    }
    
    fn node_satisfies_constraints(&self, node: &NodeState, job: &SchedulingJob) -> bool {
        for constraint in &job.constraints {
            match constraint {
                SchedulingConstraint::NodeAffinity(selector) => {
                    if !self.node_matches_selector(node, selector) {
                        return false;
                    }
                }
                SchedulingConstraint::ResourceLimit(limit) => {
                    if !self.node_satisfies_resource_limit(node, limit) {
                        return false;
                    }
                }
                _ => {} // 其他约束类型的处理
            }
        }
        true
    }
    
    fn node_has_sufficient_resources(&self, node: &NodeState, requirements: &JobResourceRequirements) -> bool {
        node.available_resources.cpu_cores >= requirements.cpu_cores &&
        node.available_resources.memory_gb >= requirements.memory_gb &&
        node.available_resources.storage_gb >= requirements.storage_gb &&
        node.available_resources.gpu_count >= requirements.gpu_count
    }
    
    fn calculate_node_score(&self, node: &NodeState, job: &SchedulingJob) -> f64 {
        let mut score = 0.0;
        
        // 资源利用率评分 (避免资源浪费)
        let cpu_utilization = node.allocated_resources.cpu_cores / node.total_resources.cpu_cores;
        let memory_utilization = node.allocated_resources.memory_gb / node.total_resources.memory_gb;
        let avg_utilization = (cpu_utilization + memory_utilization) / 2.0;
        score += (1.0 - avg_utilization) * 0.3; // 30%权重
        
        // 数据本地性评分
        let locality_score = self.calculate_data_locality_score(node, job);
        score += locality_score * 0.4; // 40%权重
        
        // 负载均衡评分
        let load_balance_score = 1.0 - (node.running_tasks.len() as f64 / 10.0).min(1.0);
        score += load_balance_score * 0.2; // 20%权重
        
        // 节点健康状况评分
        let health_score = match node.health_status {
            NodeHealthStatus::Healthy => 1.0,
            NodeHealthStatus::Warning => 0.7,
            NodeHealthStatus::Critical => 0.3,
            _ => 0.0,
        };
        score += health_score * 0.1; // 10%权重
        
        score
    }
    
    fn calculate_data_locality_score(&self, node: &NodeState, job: &SchedulingJob) -> f64 {
        // 简化的数据本地性计算
        // 实际实现需要检查输入数据在该节点的分布
        
        for constraint in &job.constraints {
            if let SchedulingConstraint::DataLocality(locality_constraint) = constraint {
                // 检查数据本地性
                return if self.has_local_data(node, locality_constraint) {
                    1.0 // 本地数据
                } else if self.has_rack_local_data(node, locality_constraint) {
                    0.7 // 机架本地
                } else {
                    0.3 // 远程数据
                };
            }
        }
        
        0.5 // 默认评分
    }
    
    fn has_local_data(&self, node: &NodeState, constraint: &DataLocalityConstraint) -> bool {
        // 检查节点是否有本地数据
        constraint.preferred_locations.contains(&node.node_id)
    }
    
    fn has_rack_local_data(&self, node: &NodeState, constraint: &DataLocalityConstraint) -> bool {
        // 检查机架级数据本地性
        if let Some(rack_id) = node.node_labels.get("rack") {
            return constraint.rack_local_locations.contains(rack_id);
        }
        false
    }
    
    fn node_matches_selector(&self, node: &NodeState, selector: &NodeSelector) -> bool {
        for (key, value) in &selector.match_labels {
            if node.node_labels.get(key) != Some(value) {
                return false;
            }
        }
        true
    }
    
    fn node_satisfies_resource_limit(&self, node: &NodeState, limit: &ResourceLimitConstraint) -> bool {
        match limit.resource_type {
            ResourceType::CPU => node.available_resources.cpu_cores >= limit.min_amount,
            ResourceType::Memory => node.available_resources.memory_gb >= limit.min_amount,
            ResourceType::Storage => node.available_resources.storage_gb >= limit.min_amount,
            ResourceType::GPU => node.available_resources.gpu_count >= limit.min_amount as u32,
            _ => true,
        }
    }
}

impl ClusterScheduler for FairScheduler {
    fn schedule_job(&mut self, job: &SchedulingJob) -> Result<SchedulingDecision, SchedulingError> {
        let start_time = SystemTime::now();
        
        // 1. 将作业加入相应队列
        if let Some(queue) = self.queues.get_mut(&job.queue_name) {
            queue.pending_jobs.push_back(job.clone());
        } else {
            return Err(SchedulingError::QueueNotFound);
        }
        
        // 2. 重新计算公平份额
        let total_resources = self.calculate_total_cluster_resources();
        self.calculate_fair_shares(&total_resources);
        
        // 3. 选择要调度的队列
        let selected_queue = self.select_queue_to_schedule()
            .ok_or(SchedulingError::NoJobsToSchedule)?;
        
        // 4. 从队列中选择作业
        let selected_job = {
            let queue = self.queues.get(selected_queue).unwrap();
            self.select_job_from_queue(queue)
                .ok_or(SchedulingError::NoJobsToSchedule)?
                .clone()
        };
        
        // 5. 选择执行节点
        let cluster_state = self.get_current_cluster_state();
        let selected_node = self.select_node_for_job(&selected_job, &cluster_state)
            .ok_or(SchedulingError::NoSuitableNode)?;
        
        // 6. 创建调度决策
        let scheduling_delay = SystemTime::now().duration_since(start_time).unwrap();
        
        let decision = SchedulingDecision {
            job_id: selected_job.job_id.clone(),
            task_assignments: vec![TaskAssignment {
                task_id: format!("{}_task_0", selected_job.job_id),
                node_id: selected_node.clone(),
                container_id: format!("container_{}", SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_millis()),
                start_time: SystemTime::now(),
                estimated_duration: selected_job.estimated_runtime.unwrap_or(Duration::from_secs(3600)),
                data_locality_score: 0.8,
            }],
            resource_allocations: vec![ResourceAllocation {
                resource_type: ResourceType::CPU,
                allocated_amount: selected_job.resource_requirements.cpu_cores,
                node_id: selected_node,
                allocation_id: format!("alloc_{}", SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_millis()),
            }],
            scheduling_delay,
            priority_boost: false,
        };
        
        // 7. 记录调度事件
        self.record_scheduling_event(&selected_job, &decision);
        
        // 8. 从队列中移除已调度的作业
        if let Some(queue) = self.queues.get_mut(selected_queue) {
            queue.pending_jobs.retain(|j| j.job_id != selected_job.job_id);
            queue.running_jobs.push(selected_job.job_id.clone());
        }
        
        Ok(decision)
    }
    
    fn update_cluster_state(&mut self, state: &ClusterState) {
        // 更新队列的资源使用情况
        for (queue_name, queue) in &mut self.queues {
            queue.usage = self.calculate_queue_resource_usage(queue_name, state);
        }
        
        // 检查是否需要抢占
        if self.global_config.preemption_enabled {
            self.check_preemption_opportunities(state);
        }
    }
    
    fn get_scheduler_metrics(&self) -> SchedulerMetrics {
        SchedulerMetrics {
            total_jobs_scheduled: self.scheduling_history.len() as u64,
            average_scheduling_delay: self.calculate_average_scheduling_delay(),
            queue_utilizations: self.calculate_queue_utilizations(),
            fairness_index: self.calculate_fairness_index(),
            preemptions_count: self.preemption_manager.get_total_preemptions(),
        }
    }
    
    fn configure(&mut self, config: SchedulerConfig) {
        // 更新调度器配置
        match config {
            SchedulerConfig::FairScheduler(fair_config) => {
                self.global_config = fair_config;
            }
            _ => {
                eprintln!("Invalid configuration for FairScheduler");
            }
        }
    }
}

// 辅助方法实现
impl FairScheduler {
    fn calculate_total_cluster_resources(&self) -> NodeResources {
        // 简化实现 - 实际需要从集群状态获取
        NodeResources {
            cpu_cores: 1000.0,
            memory_gb: 10000.0,
            storage_gb: 100000.0,
            network_bandwidth_mbps: 10000.0,
            gpu_count: 100,
            custom_resources: HashMap::new(),
        }
    }
    
    fn get_current_cluster_state(&self) -> ClusterState {
        // 简化实现 - 实际需要从集群管理器获取
        ClusterState {
            nodes: HashMap::new(),
            running_jobs: HashMap::new(),
            resource_utilization: ClusterResourceUtilization::default(),
            queue_states: HashMap::new(),
            timestamp: SystemTime::now(),
        }
    }
    
    fn record_scheduling_event(&mut self, job: &SchedulingJob, decision: &SchedulingDecision) {
        let event = SchedulingEvent {
            timestamp: SystemTime::now(),
            job_id: job.job_id.clone(),
            queue_name: job.queue_name.clone(),
            scheduling_delay: decision.scheduling_delay,
            selected_node: decision.task_assignments.first().map(|t| t.node_id.clone()),
        };
        
        self.scheduling_history.push_back(event);
        
        // 保持历史记录大小
        if self.scheduling_history.len() > 10000 {
            self.scheduling_history.pop_front();
        }
    }
    
    fn calculate_queue_resource_usage(&self, queue_name: &str, state: &ClusterState) -> NodeResources {
        // 计算队列的实际资源使用量
        NodeResources {
            cpu_cores: 0.0,
            memory_gb: 0.0,
            storage_gb: 0.0,
            network_bandwidth_mbps: 0.0,
            gpu_count: 0,
            custom_resources: HashMap::new(),
        }
    }
    
    fn check_preemption_opportunities(&mut self, state: &ClusterState) {
        if !self.global_config.preemption_enabled {
            return;
        }
        
        // 检查是否有饥饿的队列需要抢占资源
        for (queue_name, queue) in &self.queues {
            let starvation = self.calculate_queue_starvation(queue);
            if starvation > self.global_config.preemption_threshold {
                self.preemption_manager.consider_preemption_for_queue(queue_name, state);
            }
        }
    }
    
    fn calculate_average_scheduling_delay(&self) -> Duration {
        if self.scheduling_history.is_empty() {
            return Duration::from_secs(0);
        }
        
        let total_delay: Duration = self.scheduling_history.iter()
            .map(|event| event.scheduling_delay)
            .sum();
        
        total_delay / self.scheduling_history.len() as u32
    }
    
    fn calculate_queue_utilizations(&self) -> HashMap<String, f64> {
        self.queues.iter()
            .map(|(name, queue)| {
                let utilization = if queue.fair_share.cpu_cores > 0.0 {
                    queue.usage.cpu_cores / queue.fair_share.cpu_cores
                } else {
                    0.0
                };
                (name.clone(), utilization)
            })
            .collect()
    }
    
    fn calculate_fairness_index(&self) -> f64 {
        // 使用Jain's fairness index
        let utilizations: Vec<f64> = self.queues.values()
            .map(|queue| {
                if queue.fair_share.cpu_cores > 0.0 {
                    queue.usage.cpu_cores / queue.fair_share.cpu_cores
                } else {
                    0.0
                }
            })
            .collect();
        
        if utilizations.is_empty() {
            return 1.0;
        }
        
        let sum: f64 = utilizations.iter().sum();
        let sum_squared: f64 = utilizations.iter().map(|x| x * x).sum();
        let n = utilizations.len() as f64;
        
        if sum_squared == 0.0 {
            1.0
        } else {
            (sum * sum) / (n * sum_squared)
        }
    }
}

// 相关数据结构定义
#[derive(Debug, Clone)]
pub struct NodeSelector {
    pub match_labels: HashMap<String, String>,
}

#[derive(Debug, Clone)]
pub struct PodSelector {
    pub match_labels: HashMap<String, String>,
}

#[derive(Debug, Clone)]
pub struct AntiAffinityRule {
    pub target_labels: HashMap<String, String>,
    pub topology_key: String,
}

#[derive(Debug, Clone)]
pub struct ResourceLimitConstraint {
    pub resource_type: ResourceType,
    pub min_amount: f64,
    pub max_amount: Option<f64>,
}

#[derive(Debug, Clone)]
pub struct DataLocalityConstraint {
    pub preferred_locations: Vec<String>,
    pub rack_local_locations: Vec<String>,
    pub locality_preference: LocalityPreference,
}

#[derive(Debug, Clone)]
pub enum LocalityPreference {
    NodeLocal,
    RackLocal,
    AnyNode,
}

#[derive(Debug, Clone)]
pub struct RunningJob {
    pub job_id: String,
    pub start_time: SystemTime,
    pub allocated_resources: NodeResources,
    pub assigned_nodes: Vec<String>,
}

#[derive(Debug, Clone, Default)]
pub struct ClusterResourceUtilization {
    pub cpu_utilization: f64,
    pub memory_utilization: f64,
    pub storage_utilization: f64,
    pub network_utilization: f64,
}

#[derive(Debug, Clone)]
pub struct QueueState {
    pub queue_name: String,
    pub pending_jobs: u32,
    pub running_jobs: u32,
    pub completed_jobs: u32,
    pub resource_usage: NodeResources,
}

#[derive(Debug, Clone)]
pub struct SchedulingEvent {
    pub timestamp: SystemTime,
    pub job_id: String,
    pub queue_name: String,
    pub scheduling_delay: Duration,
    pub selected_node: Option<String>,
}

#[derive(Debug, Clone)]
pub struct SchedulerMetrics {
    pub total_jobs_scheduled: u64,
    pub average_scheduling_delay: Duration,
    pub queue_utilizations: HashMap<String, f64>,
    pub fairness_index: f64,
    pub preemptions_count: u64,
}

#[derive(Debug)]
pub enum SchedulerConfig {
    FairScheduler(FairSchedulerConfig),
    CapacityScheduler(CapacitySchedulerConfig),
    PriorityScheduler(PrioritySchedulerConfig),
}

#[derive(Debug)]
pub struct CapacitySchedulerConfig {
    pub queue_capacities: HashMap<String, f64>,
    pub max_capacity_limit: f64,
    pub user_limit: f64,
}

#[derive(Debug)]
pub struct PrioritySchedulerConfig {
    pub priority_levels: Vec<JobPriority>,
    pub aging_enabled: bool,
    pub aging_interval: Duration,
}

#[derive(Debug)]
pub enum SchedulingError {
    QueueNotFound,
    NoJobsToSchedule,
    NoSuitableNode,
    ResourceInsufficient,
    ConstraintViolation,
    InternalError,
}

// 抢占管理器
#[derive(Debug)]
pub struct PreemptionManager {
    preemption_history: VecDeque<PreemptionEvent>,
    total_preemptions: u64,
}

#[derive(Debug, Clone)]
pub struct PreemptionEvent {
    pub timestamp: SystemTime,
    pub preempted_job_id: String,
    pub preempting_job_id: String,
    pub node_id: String,
    pub resource_reclaimed: NodeResources,
}

impl PreemptionManager {
    pub fn new() -> Self {
        Self {
            preemption_history: VecDeque::new(),
            total_preemptions: 0,
        }
    }
    
    pub fn consider_preemption_for_queue(&mut self, queue_name: &str, cluster_state: &ClusterState) {
        // 抢占逻辑实现
        println!("Considering preemption for queue: {}", queue_name);
        // 实际实现会分析可抢占的作业并执行抢占
    }
    
    pub fn get_total_preemptions(&self) -> u64 {
        self.total_preemptions
    }
}

// 资源计算器
#[derive(Debug)]
pub struct ResourceCalculator;

impl ResourceCalculator {
    pub fn new() -> Self {
        Self
    }
    
    pub fn calculate_dominant_resource(&self, resources: &NodeResources, total: &NodeResources) -> f64 {
        let cpu_ratio = resources.cpu_cores / total.cpu_cores.max(0.001);
        let memory_ratio = resources.memory_gb / total.memory_gb.max(0.001);
        let storage_ratio = resources.storage_gb / total.storage_gb.max(0.001);
        
        cpu_ratio.max(memory_ratio).max(storage_ratio)
    }
}
