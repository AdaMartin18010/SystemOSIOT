# 2.1.1 进程管理 / Process Management

## 1. 进程基础理论 / Process Basic Theory

### 1.1 进程形式化定义 / Process Formal Definition

**进程数学定义：**

- $Process = (PID, State, Resources, Context, Priority)$  
  Process consists of PID, state, resources, context, and priority
- $PID \in \mathbb{N}$：进程标识符 / Process identifier
- $State \in \{Ready, Running, Blocked, Terminated\}$：进程状态 / Process state
- $Resources = \{CPU_{Time}, Memory_{Space}, I/O_{Devices}\}$：资源集合 / Resource set
- $Context = \{Registers, Stack, Heap, Code_{Segment}\}$：上下文信息 / Context information
- $Priority \in \mathbb{N}$：进程优先级 / Process priority

**进程状态转换：**

- $State_{Transition}: State \times Event \rightarrow State$：状态转换函数  
  State transition function: maps current state and event to new state
- $Event = \{Create, Schedule, Block, Wake, Terminate\}$：事件集合 / Event set

**进程状态机：**

```text
创建 → 就绪 → 运行 → 阻塞
  ↑      ↑      ↓      ↓
终止 ← 终止 ← 就绪 ← 唤醒
```

### 1.2 进程调度理论 / Process Scheduling Theory

**调度算法分类：**

- **抢占式调度**：$Preemptive_{Scheduling} = \{Algorithm | Running_{Process} \text{ can be interrupted}\}$  
  Preemptive scheduling: running process can be interrupted
- **非抢占式调度**：$Non_{Preemptive}_{Scheduling} = \{Algorithm | Running_{Process} \text{ cannot be interrupted}\}$  
  Non-preemptive scheduling: running process cannot be interrupted

**调度性能指标：**

- **周转时间**：$Turnaround_{Time} = Completion_{Time} - Arrival_{Time}$  
  Turnaround time: completion time minus arrival time
- **等待时间**：$Waiting_{Time} = Turnaround_{Time} - Burst_{Time}$  
  Waiting time: turnaround time minus burst time
- **响应时间**：$Response_{Time} = First_{CPU}_{Time} - Arrival_{Time}$  
  Response time: first CPU time minus arrival time

## 2. 进程调度算法 / Process Scheduling Algorithms

### 2.1 先来先服务（FCFS）/ First Come First Served

**算法原理：**

- 按照进程到达的顺序进行调度  
  Schedule processes in order of arrival
- 非抢占式算法 / Non-preemptive algorithm

**数学分析：**

- 平均等待时间：$Average_{Waiting}_{Time} = \frac{\sum_{i=1}^{n} Waiting_{Time_i}}{n}$  
  Average waiting time: sum of waiting times divided by number of processes
- 平均周转时间：$Average_{Turnaround}_{Time} = \frac{\sum_{i=1}^{n} Turnaround_{Time_i}}{n}$  
  Average turnaround time: sum of turnaround times divided by number of processes

**Rust实现：**

```rust
use std::collections::VecDeque;

#[derive(Debug, Clone)]
pub struct Process {
    pid: u32,
    arrival_time: u32,
    burst_time: u32,
    priority: u32,
    state: ProcessState,
}

#[derive(Debug, Clone, PartialEq)]
pub enum ProcessState {
    Ready,
    Running,
    Blocked,
    Terminated,
}

pub struct FCFSScheduler {
    ready_queue: VecDeque<Process>,
    current_process: Option<Process>,
    current_time: u32,
}

impl FCFSScheduler {
    pub fn new() -> Self {
        FCFSScheduler {
            ready_queue: VecDeque::new(),
            current_process: None,
            current_time: 0,
        }
    }
    
    pub fn add_process(&mut self, process: Process) {
        self.ready_queue.push_back(process);
    }
    
    pub fn schedule(&mut self) -> Option<Process> {
        // 如果当前没有运行进程，从就绪队列取一个
        if self.current_process.is_none() && !self.ready_queue.is_empty() {
            self.current_process = self.ready_queue.pop_front();
        }
        
        // 更新当前进程状态
        if let Some(ref mut process) = self.current_process {
            if process.state == ProcessState::Running {
                process.burst_time -= 1;
                
                if process.burst_time == 0 {
                    process.state = ProcessState::Terminated;
                    let completed_process = self.current_process.take();
                    return completed_process;
                }
            } else {
                process.state = ProcessState::Running;
            }
        }
        
        None
    }
    
    pub fn get_average_waiting_time(&self, processes: &[Process]) -> f64 {
        let mut total_waiting_time = 0;
        let mut current_time = 0;
        
        for process in processes {
            if current_time < process.arrival_time {
                current_time = process.arrival_time;
            }
            total_waiting_time += current_time - process.arrival_time;
            current_time += process.burst_time;
        }
        
        total_waiting_time as f64 / processes.len() as f64
    }
    
    pub fn get_average_turnaround_time(&self, processes: &[Process]) -> f64 {
        let mut total_turnaround_time = 0;
        let mut current_time = 0;
        
        for process in processes {
            if current_time < process.arrival_time {
                current_time = process.arrival_time;
            }
            total_turnaround_time += current_time + process.burst_time - process.arrival_time;
            current_time += process.burst_time;
        }
        
        total_turnaround_time as f64 / processes.len() as f64
    }
}
```

### 2.2 最短作业优先（SJF）/ Shortest Job First

**算法原理：**

- 选择执行时间最短的进程优先执行  
  Select process with shortest burst time first
- 非抢占式算法 / Non-preemptive algorithm

**数学分析：**

- 最优性证明：SJF算法在所有非抢占式算法中平均等待时间最短  
  Optimality proof: SJF has minimum average waiting time among all non-preemptive algorithms
- 证明思路：
  - 假设存在更优的调度序列
  - 通过交换相邻进程可以证明SJF最优
  - 使用数学归纳法完成证明

**Rust实现：**

```rust
use std::collections::BinaryHeap;
use std::cmp::Ordering;

#[derive(Debug, Clone)]
pub struct SJFProcess {
    pid: u32,
    arrival_time: u32,
    burst_time: u32,
    remaining_time: u32,
}

impl PartialEq for SJFProcess {
    fn eq(&self, other: &Self) -> bool {
        self.remaining_time == other.remaining_time
    }
}

impl Eq for SJFProcess {}

impl PartialOrd for SJFProcess {
    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
        Some(self.cmp(other))
    }
}

impl Ord for SJFProcess {
    fn cmp(&self, other: &Self) -> Ordering {
        // 最小堆，所以反转比较
        other.remaining_time.cmp(&self.remaining_time)
    }
}

pub struct SJFScheduler {
    ready_queue: BinaryHeap<SJFProcess>,
    current_time: u32,
    completed_processes: Vec<SJFProcess>,
}

impl SJFScheduler {
    pub fn new() -> Self {
        SJFScheduler {
            ready_queue: BinaryHeap::new(),
            current_time: 0,
            completed_processes: Vec::new(),
        }
    }
    
    pub fn add_process(&mut self, process: SJFProcess) {
        self.ready_queue.push(process);
    }
    
    pub fn schedule(&mut self) -> Option<SJFProcess> {
        if let Some(mut current_process) = self.ready_queue.pop() {
            current_process.remaining_time -= 1;
            self.current_time += 1;
            
            if current_process.remaining_time == 0 {
                self.completed_processes.push(current_process.clone());
                return Some(current_process);
            } else {
                self.ready_queue.push(current_process);
            }
        }
        
        None
    }
    
    pub fn get_optimal_schedule(&self, processes: &[SJFProcess]) -> Vec<SJFProcess> {
        let mut sorted_processes = processes.to_vec();
        sorted_processes.sort_by_key(|p| (p.arrival_time, p.burst_time));
        
        let mut schedule = Vec::new();
        let mut current_time = 0;
        let mut available_processes = Vec::new();
        
        for process in sorted_processes {
            if process.arrival_time <= current_time {
                available_processes.push(process);
            } else {
                // 处理当前可用的进程
                available_processes.sort_by_key(|p| p.burst_time);
                if let Some(selected) = available_processes.remove(0) {
                    schedule.push(selected.clone());
                    current_time += selected.burst_time;
                }
                available_processes.push(process);
            }
        }
        
        // 处理剩余的进程
        available_processes.sort_by_key(|p| p.burst_time);
        schedule.extend(available_processes);
        
        schedule
    }
}
```

### 2.3 优先级调度 / Priority Scheduling

**算法原理：**

- 根据进程优先级进行调度  
  Schedule processes based on priority
- 可以是抢占式或非抢占式 / Can be preemptive or non-preemptive

**优先级反转问题：**

- **问题定义**：低优先级进程持有高优先级进程需要的资源  
  Problem definition: low priority process holds resource needed by high priority process
- **解决方案**：优先级继承协议 / Solution: priority inheritance protocol

**Rust实现：**

```rust
use std::collections::BinaryHeap;

#[derive(Debug, Clone)]
pub struct PriorityProcess {
    pid: u32,
    priority: u32, // 数字越小优先级越高
    arrival_time: u32,
    burst_time: u32,
    remaining_time: u32,
}

impl PartialEq for PriorityProcess {
    fn eq(&self, other: &Self) -> bool {
        self.priority == other.priority
    }
}

impl Eq for PriorityProcess {}

impl PartialOrd for PriorityProcess {
    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
        Some(self.cmp(other))
    }
}

impl Ord for PriorityProcess {
    fn cmp(&self, other: &Self) -> Ordering {
        // 优先级越高（数字越小）越优先
        self.priority.cmp(&other.priority)
    }
}

pub struct PriorityScheduler {
    ready_queue: BinaryHeap<PriorityProcess>,
    current_process: Option<PriorityProcess>,
    current_time: u32,
    is_preemptive: bool,
}

impl PriorityScheduler {
    pub fn new(is_preemptive: bool) -> Self {
        PriorityScheduler {
            ready_queue: BinaryHeap::new(),
            current_process: None,
            current_time: 0,
            is_preemptive,
        }
    }
    
    pub fn add_process(&mut self, process: PriorityProcess) {
        if self.is_preemptive {
            // 抢占式：检查是否需要抢占当前进程
            if let Some(ref current) = self.current_process {
                if process.priority < current.priority {
                    // 抢占当前进程
                    let mut preempted = self.current_process.take().unwrap();
                    preempted.state = ProcessState::Ready;
                    self.ready_queue.push(preempted);
                    self.current_process = Some(process);
                    return;
                }
            }
        }
        
        self.ready_queue.push(process);
    }
    
    pub fn schedule(&mut self) -> Option<PriorityProcess> {
        if self.current_process.is_none() && !self.ready_queue.is_empty() {
            self.current_process = self.ready_queue.pop();
        }
        
        if let Some(ref mut process) = self.current_process {
            process.remaining_time -= 1;
            self.current_time += 1;
            
            if process.remaining_time == 0 {
                let completed = self.current_process.take();
                return completed;
            }
        }
        
        None
    }
}
```

### 2.4 轮转调度（RR）/ Round Robin

**算法原理：**

- 为每个进程分配固定的时间片  
  Allocate fixed time slice to each process
- 时间片用完后，进程回到就绪队列末尾  
  When time slice expires, process goes to end of ready queue

**数学分析：**

- 时间片选择：$Time_{Slice} = \min\{Max_{Time}, Average_{Burst}_{Time}\}$  
  Time slice selection: minimum of maximum time and average burst time
- 响应时间：$Response_{Time} \leq (n-1) \times Time_{Slice}$  
  Response time: less than or equal to (n-1) times time slice

**Rust实现：**

```rust
use std::collections::VecDeque;

#[derive(Debug, Clone)]
pub struct RRProcess {
    pid: u32,
    arrival_time: u32,
    burst_time: u32,
    remaining_time: u32,
    time_slice: u32,
}

pub struct RRScheduler {
    ready_queue: VecDeque<RRProcess>,
    current_process: Option<RRProcess>,
    current_time: u32,
    time_slice: u32,
    time_slice_counter: u32,
}

impl RRScheduler {
    pub fn new(time_slice: u32) -> Self {
        RRScheduler {
            ready_queue: VecDeque::new(),
            current_process: None,
            current_time: 0,
            time_slice,
            time_slice_counter: 0,
        }
    }
    
    pub fn add_process(&mut self, process: RRProcess) {
        self.ready_queue.push_back(process);
    }
    
    pub fn schedule(&mut self) -> Option<RRProcess> {
        // 如果当前进程完成或时间片用完，切换到下一个进程
        if let Some(ref mut current) = self.current_process {
            current.remaining_time -= 1;
            self.time_slice_counter += 1;
            self.current_time += 1;
            
            if current.remaining_time == 0 {
                // 进程完成
                let completed = self.current_process.take();
                self.time_slice_counter = 0;
                return completed;
            } else if self.time_slice_counter >= self.time_slice {
                // 时间片用完，进程回到队列末尾
                let mut preempted = self.current_process.take().unwrap();
                self.ready_queue.push_back(preempted);
                self.time_slice_counter = 0;
            }
        }
        
        // 选择下一个进程
        if self.current_process.is_none() && !self.ready_queue.is_empty() {
            self.current_process = self.ready_queue.pop_front();
            self.time_slice_counter = 0;
        }
        
        None
    }
    
    pub fn get_optimal_time_slice(&self, processes: &[RRProcess]) -> u32 {
        let total_burst_time: u32 = processes.iter().map(|p| p.burst_time).sum();
        let avg_burst_time = total_burst_time / processes.len() as u32;
        
        // 时间片通常设置为平均突发时间的80%
        (avg_burst_time as f64 * 0.8) as u32
    }
}
```

## 3. 进程同步 / Process Synchronization

### 3.1 临界区问题 / Critical Section Problem

**临界区定义：**

- $Critical_{Section} = \{Code_{Segment} | Shared_{Resource}_{Access}\}$  
  Critical section: code segment that accesses shared resources
- $Mutual_{Exclusion} = \{\forall i,j \neq i, Process_i \text{ and } Process_j \text{ cannot be in critical section simultaneously}\}$  
  Mutual exclusion: no two processes can be in critical section simultaneously

**Peterson算法：**

```rust
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::Arc;

pub struct PetersonLock {
    flag: [Arc<AtomicBool>; 2],
    turn: Arc<AtomicBool>,
}

impl PetersonLock {
    pub fn new() -> Self {
        PetersonLock {
            flag: [Arc::new(AtomicBool::new(false)), Arc::new(AtomicBool::new(false))],
            turn: Arc::new(AtomicBool::new(false)),
        }
    }
    
    pub fn lock(&self, id: usize) {
        let other = 1 - id;
        
        // 设置标志
        self.flag[id].store(true, Ordering::SeqCst);
        // 设置轮次
        self.turn.store(other == 1, Ordering::SeqCst);
        
        // 等待
        while self.flag[other].load(Ordering::SeqCst) && 
              self.turn.load(Ordering::SeqCst) == (other == 1) {
            // 忙等待
        }
    }
    
    pub fn unlock(&self, id: usize) {
        self.flag[id].store(false, Ordering::SeqCst);
    }
}

// 使用示例
pub struct SharedResource {
    data: i32,
    lock: PetersonLock,
}

impl SharedResource {
    pub fn new() -> Self {
        SharedResource {
            data: 0,
            lock: PetersonLock::new(),
        }
    }
    
    pub fn increment(&self, process_id: usize) {
        self.lock.lock(process_id);
        
        // 临界区
        self.data += 1;
        
        self.lock.unlock(process_id);
    }
    
    pub fn get_data(&self) -> i32 {
        self.data
    }
}
```

### 3.2 信号量 / Semaphores

**信号量定义：**

- $Semaphore = (Value, Queue)$：信号量由值和等待队列组成  
  Semaphore consists of value and waiting queue
- $Value \in \mathbb{N}$：信号量的值 / Semaphore value
- $Queue = \{Process_1, Process_2, ..., Process_n\}$：等待队列 / Waiting queue

**P操作和V操作：**

- $P(Semaphore)$：如果$Value > 0$，则$Value = Value - 1$；否则进程阻塞  
  P operation: if value > 0, decrement value; otherwise block process
- $V(Semaphore)$：$Value = Value + 1$，唤醒一个等待进程  
  V operation: increment value and wake up one waiting process

**Rust实现：**

```rust
use std::sync::{Arc, Mutex, Condvar};

pub struct Semaphore {
    value: Arc<Mutex<i32>>,
    condition: Arc<Condvar>,
}

impl Semaphore {
    pub fn new(initial_value: i32) -> Self {
        Semaphore {
            value: Arc::new(Mutex::new(initial_value)),
            condition: Arc::new(Condvar::new()),
        }
    }
    
    pub fn wait(&self) {
        let mut value = self.value.lock().unwrap();
        
        while *value <= 0 {
            value = self.condition.wait(value).unwrap();
        }
        
        *value -= 1;
    }
    
    pub fn signal(&self) {
        let mut value = self.value.lock().unwrap();
        *value += 1;
        self.condition.notify_one();
    }
}

// 生产者-消费者问题
pub struct ProducerConsumer {
    buffer: Arc<Mutex<Vec<i32>>>,
    empty: Semaphore,
    full: Semaphore,
    mutex: Arc<Mutex<()>>,
}

impl ProducerConsumer {
    pub fn new(buffer_size: usize) -> Self {
        ProducerConsumer {
            buffer: Arc::new(Mutex::new(Vec::new())),
            empty: Semaphore::new(buffer_size as i32),
            full: Semaphore::new(0),
            mutex: Arc::new(Mutex::new(())),
        }
    }
    
    pub fn produce(&self, item: i32) {
        self.empty.wait();
        
        let _lock = self.mutex.lock().unwrap();
        self.buffer.lock().unwrap().push(item);
        
        self.full.signal();
    }
    
    pub fn consume(&self) -> Option<i32> {
        self.full.wait();
        
        let _lock = self.mutex.lock().unwrap();
        let item = self.buffer.lock().unwrap().pop();
        
        self.empty.signal();
        item
    }
}
```

## 4. 死锁理论 / Deadlock Theory

### 4.1 死锁条件 / Deadlock Conditions

**死锁的四个必要条件：**

1. **互斥条件**：$Mutual_{Exclusion} = \{\forall Resource, \text{at most one process can use it}\}$  
   Mutual exclusion: at most one process can use a resource
2. **占有和等待**：$Hold_{and}_{Wait} = \{\exists Process, \text{holds resources and waits for more}\}$  
   Hold and wait: process holds resources and waits for more
3. **不可抢占**：$No_{Preemption} = \{\forall Resource, \text{cannot be forcibly taken}\}$  
   No preemption: resources cannot be forcibly taken
4. **循环等待**：$Circular_{Wait} = \{\exists Process_{Chain}, P_1 \rightarrow P_2 \rightarrow ... \rightarrow P_n \rightarrow P_1\}$  
   Circular wait: circular chain of waiting processes

### 4.2 死锁检测 / Deadlock Detection

**资源分配图：**

- $Resource_{Allocation}_{Graph} = (Processes, Resources, Edges)$  
  Resource allocation graph: processes, resources, and edges
- $Edges = \{Request_{Edges}, Assignment_{Edges}\}$：请求边和分配边  
  Edges: request edges and assignment edges

**死锁检测算法：**

```rust
use std::collections::{HashMap, HashSet};

#[derive(Debug, Clone)]
pub struct ResourceAllocationGraph {
    processes: Vec<usize>,
    resources: Vec<usize>,
    request_edges: HashMap<(usize, usize), usize>, // (process, resource) -> amount
    assignment_edges: HashMap<(usize, usize), usize>, // (resource, process) -> amount
}

impl ResourceAllocationGraph {
    pub fn new(num_processes: usize, num_resources: usize) -> Self {
        ResourceAllocationGraph {
            processes: (0..num_processes).collect(),
            resources: (0..num_resources).collect(),
            request_edges: HashMap::new(),
            assignment_edges: HashMap::new(),
        }
    }
    
    pub fn add_request(&mut self, process: usize, resource: usize, amount: usize) {
        self.request_edges.insert((process, resource), amount);
    }
    
    pub fn add_assignment(&mut self, resource: usize, process: usize, amount: usize) {
        self.assignment_edges.insert((resource, process), amount);
    }
    
    pub fn detect_deadlock(&self) -> Option<Vec<usize>> {
        let mut visited = HashSet::new();
        let mut recursion_stack = HashSet::new();
        
        for &process in &self.processes {
            if !visited.contains(&process) {
                if self.has_cycle(process, &mut visited, &mut recursion_stack) {
                    return Some(self.get_cycle_processes());
                }
            }
        }
        
        None
    }
    
    fn has_cycle(&self, process: usize, visited: &mut HashSet<usize>, 
                 recursion_stack: &mut HashSet<usize>) -> bool {
        visited.insert(process);
        recursion_stack.insert(process);
        
        // 检查所有请求的资源
        for &resource in &self.resources {
            if let Some(&amount) = self.request_edges.get(&(process, resource)) {
                if amount > 0 {
                    // 检查是否有进程持有该资源
                    for &other_process in &self.processes {
                        if let Some(&held_amount) = self.assignment_edges.get(&(resource, other_process)) {
                            if held_amount > 0 {
                                if !visited.contains(&other_process) {
                                    if self.has_cycle(other_process, visited, recursion_stack) {
                                        return true;
                                    }
                                } else if recursion_stack.contains(&other_process) {
                                    return true;
                                }
                            }
                        }
                    }
                }
            }
        }
        
        recursion_stack.remove(&process);
        false
    }
    
    fn get_cycle_processes(&self) -> Vec<usize> {
        // 简化实现，返回所有进程
        self.processes.clone()
    }
}
```

### 4.3 死锁预防 / Deadlock Prevention

**银行家算法：**

```rust
#[derive(Debug, Clone)]
pub struct BankerAlgorithm {
    available: Vec<usize>,
    maximum: Vec<Vec<usize>>,
    allocation: Vec<Vec<usize>>,
    need: Vec<Vec<usize>>,
}

impl BankerAlgorithm {
    pub fn new(num_processes: usize, num_resources: usize) -> Self {
        BankerAlgorithm {
            available: vec![0; num_resources],
            maximum: vec![vec![0; num_resources]; num_processes],
            allocation: vec![vec![0; num_resources]; num_processes],
            need: vec![vec![0; num_resources]; num_processes],
        }
    }
    
    pub fn set_available(&mut self, available: Vec<usize>) {
        self.available = available;
    }
    
    pub fn set_maximum(&mut self, process: usize, maximum: Vec<usize>) {
        self.maximum[process] = maximum;
        self.update_need(process);
    }
    
    pub fn set_allocation(&mut self, process: usize, allocation: Vec<usize>) {
        self.allocation[process] = allocation;
        self.update_need(process);
    }
    
    fn update_need(&mut self, process: usize) {
        for i in 0..self.available.len() {
            self.need[process][i] = self.maximum[process][i] - self.allocation[process][i];
        }
    }
    
    pub fn is_safe(&self) -> bool {
        let mut work = self.available.clone();
        let mut finish = vec![false; self.maximum.len()];
        
        loop {
            let mut found = false;
            
            for i in 0..self.maximum.len() {
                if !finish[i] && self.can_allocate(i, &work) {
                    // 分配资源给进程i
                    for j in 0..work.len() {
                        work[j] += self.allocation[i][j];
                    }
                    finish[i] = true;
                    found = true;
                }
            }
            
            if !found {
                break;
            }
        }
        
        finish.iter().all(|&x| x)
    }
    
    fn can_allocate(&self, process: usize, work: &[usize]) -> bool {
        for i in 0..work.len() {
            if self.need[process][i] > work[i] {
                return false;
            }
        }
        true
    }
    
    pub fn request_resources(&mut self, process: usize, request: Vec<usize>) -> bool {
        // 检查请求是否超过需求
        for i in 0..request.len() {
            if request[i] > self.need[process][i] {
                return false;
            }
        }
        
        // 检查是否有足够资源
        for i in 0..request.len() {
            if request[i] > self.available[i] {
                return false;
            }
        }
        
        // 尝试分配
        for i in 0..request.len() {
            self.available[i] -= request[i];
            self.allocation[process][i] += request[i];
            self.need[process][i] -= request[i];
        }
        
        // 检查是否安全
        if self.is_safe() {
            true
        } else {
            // 回滚分配
            for i in 0..request.len() {
                self.available[i] += request[i];
                self.allocation[process][i] -= request[i];
                self.need[process][i] += request[i];
            }
            false
        }
    }
}
```

## 5. 批判性分析 / Critical Analysis

### 5.1 进程管理的优势 / Advantages of Process Management

- **资源隔离**：进程间资源隔离，提高系统稳定性  
  Resource isolation: process isolation improves system stability
- **并发执行**：支持多进程并发执行，提高系统效率  
  Concurrent execution: supports multi-process concurrent execution
- **故障隔离**：单个进程故障不影响其他进程  
  Fault isolation: single process failure doesn't affect others

### 5.2 局限性分析 / Limitation Analysis

- **上下文切换开销**：进程切换需要保存和恢复上下文  
  Context switch overhead: process switching requires saving and restoring context
- **内存开销**：每个进程需要独立的内存空间  
  Memory overhead: each process needs independent memory space
- **通信复杂性**：进程间通信相对复杂  
  Communication complexity: inter-process communication is relatively complex

### 5.3 工程权衡 / Engineering Trade-offs

- **进程粒度 vs 系统开销**：细粒度进程 vs 系统开销  
  Process granularity vs system overhead
- **调度公平性 vs 系统效率**：公平调度 vs 系统效率  
  Scheduling fairness vs system efficiency
- **实时性 vs 通用性**：实时调度 vs 通用调度  
  Real-time vs generality

---

> 本文件为进程管理的系统化重构，采用严格的形式化定义、数学证明和Rust代码实现，确保内容的学术规范性和工程实用性。
> This file provides systematic refactoring of process management, using strict formal definitions, mathematical proofs, and Rust code implementation, ensuring academic standards and engineering practicality.
